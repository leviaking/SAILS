2019 Log.

Most recently, I was mainly doing GS work. There's a log for that: 2018_log_GS.txt. It's really short, so I'm just going to paste the whole of it below.

##### Begin 2018_log_GS.txt #####

2018-07-20.

The GS files are fully parsed; see /Users/leviking/Documents/dissertation/SAILS/gold_standards/finalcsvs

Now I must begin the scoring process. I need to do leave one out testing on each of the gold standards.

2018-07-23.
Currently working on the files OLDprep_conll_for_tfidf.sh and OLDprep_conll_for_tfidf.py; I will rename these without the "OLD" when they are fully updated.
DONE.

2018-07-24.
Today I'm starting work on the leave-one-out (LOO) GS experiments. I need to drag out the tf-idf scripts from last time and prepare them for LOO.

Oh, this time, I'm only going to use the tf-idf cosine ("TC") approach; this was the clear winner in the pilot study. See the table pasted below. So it'll be TC and the Brown Corpus for sure, and I think I should look at xdh, ldh, and probably xdx.
For the term (depstring) forms,
Approach	form		Ref Corpus		
0.51577 TC xdh 0.51810 Brown 0.51534
0.50780 FC ldh 0.51677 WSJ 0.50798
0.50755 TA lxh 0.51350
0.49464 FA xdx 0.49901
			ldx 0.49352

I need these scripts: lk_tfidf.sh, lk_tfidf.py
These have been copied to the scripts folder as "OLDlk_tfidf*"; I'll remove the "OLD" when they are completed.
I'm not quite ready to implement LOO. These scripts simply compare dependencies in the test file with dependencies in the reference corpus and record the resulting tf-idf score for each term. I need to modify them to work on the csv files that now contain all the depstrings.


2018-08-14.
I've almost finished OLDlk_tfidf.py. It's basically working, but I need to uncomment the remaining deptypes in a couple of places in the main program; relatedly, I need to fix it so that I'm not working with empty lists after I run through the first deptype (I think this has something to do with the csvreader and/or the way the loop is managed).

2019-04-15.
I'm trying to get my mojo back. First I need to nail down exactly where the data processing left off. I believe this is what remains, in broad strokes:
* Leave One Out (LOO): Get average scores? See below.
* To the existing GS files, add columns where we include a weight factor for the weight of each dependency (for each dependency, weight is 1/n, for n = total number of deps in response)
* Repeat full process for non-GS response files
* Modify pipeline to operate on csv

It's been a while but it looks like LOO for the GS files has been executed -- each response has a vector similarity score (cosine).
I think the question is-- what to do with those... I believe they should be averaged per file (test item), and probably per other settings *across* all test items; those settings would be depstring format: ldh, xdh, xdx; speakers: all_cns (all crowdsourced NS), all_fns (all familiar NS), all_ns; annotations: perfects, almosts, core_yes; response ordering: firsts, seconds.

I need to examine the script score_GS_LOO_TC.py; this appears to be the beginnings of a script to perform the above paragraph, I think. If not, figure out what it is and what to do with it...


2019-04-17.
TODO: I believe I need to return to this script; investigate and implement as necessary:
/Users/leviking/Documents/dissertation/SAILS/scripts/lk_tfidf_LOO_TC_weighted.py

For now:
OK, I'm getting myself back on track re Spearman's correlation and how we want to use it.
We want to rank responses by annotation: scores range 0-1, where each "1" annotation carries a 0.2 score.
We want to rank responses by TC (tf-idf cosine).
Then when we get Spearman scores, that score applies to a given Gold Standard. So Spearman is used to evaluate the GSs, not the responses. It gives us a measure of how well using TC on a GS can align with using our manual annotations (which one would not generally have "in the wild").

##### End 2018_log_GS.txt #####

2019-05-05. Sunday.

My immediate goal is to finish the data work. At the very least, the processing/pre-processing:
1. Finish GS CSVs:
	Weight dependencies
	Regenerate LOO scores (cosine) on weighted dependency responses
	
2. Process all NNS data (CSVs):
	Parse all responses (in CSV)
	Generate "depstrings" (ldh, xdh, xdx)
	
3. Generate cosine scores for all NNS responses (vs. each GS)

4. Get Spearman scores for each GS

5. Based on Spearman results, choose best GS format and threshold, apply to held out test data.

From the header of lk_tfidf_LOO_TC_weighted.py, here's an explanation of weighted dependencies:

##2018-08-23. This is a modified version of lk_tfidf_LOO_TC.py. It does a similar analysis, but uses a slightly smarter approach to the GS. The original version simply dumps all dependencies in the GS into one "bag", so that all dependencies (tokens) carry an equal weight. In this version, however, all *responses* are equal, and each dependency (token) in a GS response of length n dependencies (tokens) receives a weight of 1/n. My thinking is that this should minimize the weight of extraneous, uninformative dependencies in the GS. Compare these two hypothetical GS responses:
##The smiling man is planting a baby tree in the green grass on a sunny day.
##The man is planting a tree.
##So the impact of dependencies like "amod-green-grass" should be lowered, while the impact of "obj-tree-plant" should be boosted (relatively speaking). 

Crucially, note that the weighting applies to the GS only. We're measuring the similarity of 1 response to a collection of responses; This means that we'd simply be weighting each dependency in the test response by dividing by 1 (the number of responses it appears in), so sure -- in theory weighting applies here, but it all comes out in the wash. So we're going to skip implementing it for the test responses.

2019-05-11. Saturday.
I've now combed through lk_tfidf_LOO_TC_weighted.py entirely and reworked some things. It's not weighted at all now -- it's really just a clean implementation of the unweighted version, which I'm pretty sure was not doing what I thought it was doing. But I haven't used any scores from it for anything yet, so it's not a problem. I'm going to save this as lk_tfidf_LOO_TC.py and make the necessary minor changes within the script. [DONE]
Then I need to return to lk_tfidf_LOO_TC_weighted.py and actually implement weighting here.
DONE! Weighting is implemented.
Now I need to run the shell script in order to process all the files.

2019-05-19. Sunday.
I ran the GS LOO scripts (assigning a "TC" to each GS response) for both non-weighted and weighted dependencies for all GSs for items 1-10 (targeted and untargeted, ldh, xdh, xdx). It takes about 2 hours to do this for a single item, either in weighted or non-weighted setting, so it's about 4 hours total for one item. So with 20 items remaining, this will take about 80 more hours. I may look into running this on an IU cluster to speed it up.
I also finished a script ((score_GS_LOO_TC-weighted.py)) to score the GSs for a single item (i.e., 01T unweighted), so we can see which GS correlates TC scores and annotation scores the highest. I'm realizing now that it would be better if we include weighted and unweighted dependency approaches in the table of GS scores that this outputs; this would allow us to compare weighed and unweighted GSs.

Goals today:
1. Find the status of NNS responses: Are they parsed, etc.? tf-idf scored?
	a. If necessary: process responses into depstrings; get tf-idf scores
2. Implement spearman scoring for NNS responses
	a. This part is fast: we can apply weights to annotation on the fly here. Let's generate score spreadsheets based on a few different weighting patterns (features C, A, G, I, V):
		1. flat: .2 for all annotation features
		2. core only: 1.0 for core
		3. Core Heavy: .6, .1, .1, .1, .1
		4. Ideal: .5, .2, .05, .15, .1 ## representing my intuition on the importance of each feature in an ICALL context
		I think that's probably enough...
		
OK, here's some pseudocode to serve as an outline for the processing of the test responses:
# pseudocode (see also sails_NNS_stanford_penn_to_conll.sh)
## for masterannofile in corpusdir:
  ## run python script, which does:
    ## generate "rawcsv" of NNS only, with fields: ResponseID, Response, C, A, G, I, V
  ####### BELOW: Repurpose /scripts/gscsv_to_lemmatized_conll.sh for these steps
  ## from rawcsv, get text file of responses;
  ## with stanford coreNLP, lemmatize responses;
      #### can we cache here to avoid duplicate processing? (we must load lemmatizer for each response)
  ## with stanford parser, parse text to penn, store in csv w fields: ReponseID, penn parse
  ## from penncsv, get text file of penn;
  ## with stanford parser, parse this to conll, store in csv w fields: ResponseID, conll parse
  ####### ABOVE: Repurpose /scripts/gscsv_to_lemmatized_conll.sh for these steps
  ## run lemmatize_conll.py to get lemma_conll files
  ## add lemmatized parses to rawcsv to create finalcsv; see add_parses_to_gscsv.py; repurpose (simplify) this script for test reponses
  ## run prep_conll_for_tfidf.py: this generates the *depstrings.csv, which is finalcsv + ldh, xdh, xdx
#####
## Then we run lk_tfidf.py (must create this by modifying lk_tfidf_LOO_TC.py) on each test item csv
  ## Note that this must score each test response in relation to each GS;
      #### can we cache here to avoid duplicate processing?
## Then we need to score the GSs according to Spearman correlations

2019/06/02. Sunday.
I'm back on the case. Let's work through the above goals.

EOD. Today I finished response_csv_to_txt.py; This takes a single "master_anno" csv file from the SAILS/corpus/ directory. It finds all the NNS responses and writes the response sentence to a .txt file in SAILS/responses/txt/, and writes the ResponseID, Response, C, A, G, I, V to a row of a corresponding csv in SAILS/responses/rawcsvs/, e.g. I0T_NNS.csv.
This will be run via a shell script -- it takes the relative path+filename to the master_anno file as an argument.

2019/06/16. Sunday.
OK, it looks like these steps remain:
(run all files through response_csv_to_txt.py -- probably use a shell, call it response_csv_to_txt.sh)
run text files through stanford CoreNLP lemmatizer
run text files through stanford parser --> penn
run penn parses through stanford parser --> conll
merge lemmatized sentence with dependency parse ( --> lemma_conll)

OK, I now have script text_to_lemmatized_conll.sh
While testing, I temporarily moved the responses folder to "responses-REAL-protected06-16"; I was testing the "responses" folder with only toy files. Now I have copied the responses back; I will start running the script again before I go to bed.

2019/06/22. Saturday.
I successfully ran the script on all the NNS responses. The directories are now back in order; the backup has been removed.

Today I need to:
For each item:
	from lemma_conll file, add parse into the rawcsvs file --> result is "finalcsvs/*.csv"
	DONE; used script add_parses_to_response_csv.py
	from finalcsvs file, create *_depstrings.csv file, with these columns:
		ResponseID	Response	Core	Answer	Gramm	Interp	Verif	parse	ldh	xdh	xdx
	DONE; used script prep_conll_for_tfidf.sh ; each item now has a *_depstrings.csv file in responses/depstrings/


Extend the depstrings.csv file with the TC scores from each gold standard+depstring format.
	This should look similar to the GS LOO files.

Future notes:
Generating the above essentially means I'll have all the tf-idf scoring complete.
Some of it will be held out as test data, but there's really no harm in generating all the scores up front (I don't have to examine it thoroughly). Beyond this, I believe the remaining work will entail writing a couple of python scripts to pull scores from these csvs and group them according to depstring format; intransitive / trans / ditrans; targeted / untargeted; 

**TODO: For simplicity, let's combine the GS LOO_TC weighted and unweighted files -- we don't need two for each item.



2019/10/26. Saturday.
Back on the horse, ready to slay this beast once and for all.
Here are the questions I need to answer from the data:
("UNSUPERVISED" ~ annotation is not necessary for forming these GSs)
1. [2 values] Targeted vs Untargeted; I DO NOT want to "cross-pollinate" these -- i.e., let's compare them directly, but I don't want to spend time examinaing how well Untargeted GS covers Targeted responses, etc.
2. [3] dependency forms: ldh vs xdh vs xdx
3. [2] Weighted dependencies vs Unweighted dependencies
4. [3] Intransitive vs Transitive vs Ditransitive
*5. [2] First responses only vs Firsts + Seconds (sampling/cross-validation to normalize for size?)
*6. [2] Familar vs Crowdsourced (sampling/cross-validation to normalize for size)
("SUPERVISED" ~ annotation is necessary for forming these GSs)
*7. [3] Perfect (5/5) annotations vs Almosts (or ALL) vs CoreYes (sampling/cross-validation to normalize for size)

Running the above in exhaustive combination would result in:
2*3*2*3*2*3*2=432 experiments
It would also result in data sparsity, so we want to avoid that.
Instead, let's focus on only one variable at a time to find the optimal value for that variable.
Given that this isn't a huge data set, I think I should use half the items as a dev set.
I'll use the dev set to investigate the questions above.

Let me expand on the above:
1. Targeted vs Untargeted: How does this variable affect scoring? (i.e., Targeted items are scored more accurately)
2. dependency forms: Which form is scored more accurately?
3. Weighted dependencies vs unweighted: Does weighting deps improve scoring accuracy?
4. Intransitive vs Transitive vs Ditransitive: Are sentence types easier/harder to score accurately?
5. First responses vs Firsts + Seconds: Is it better to have, e.g., 100 respondents give one response or 50 respondents give 2 responses?
6. Familiar vs Crowdsourced: Are familar responses better than crowdsourced responses? To what extent?
7. Perfect vs CoreYes vs All: How well does each perform at scoring?

## BEGIN: IGNORE ##
I'm convinced now that we don't need to mess around with Spearman or Pearson or any complicated ranking/scoring methods like that.
I think we can simply find the threshholds, apply them as a cut-off, and get precision, recall and f-scores. We definitely want to do this for CoreEvent, but we might also do this for Perfect and Almost responses.

I need to email Markus:
Hi Markus,

I hope Fall is treating you well! I know this is out of the blue but I *am* trying to make progress on my dissertation.
When we met in July, I had most of the GSs and responses scored for dependency tfidf and stashed such that the experiments would just be a matter of querying those scores in particular combinations. I got pretty stumped on how to evaluate the results -- we talked about Spearman, etc. I'm now convinced that's too complicated. I have leave-one-out (LOO) scores where each response in the GS is scored against the remainder of the GS (as a reminder, those are tf-idf cosine ("TC") scores; the response is converted to a vector of tf-idf scores for each dependency, the same is done for the GS as a whole, then we get the cosine of the two vectors). Instead of comparing TC score rankings with annotation based rankings to determine some kind of score, I think that for simplicity's sake, I want to establish a threshold from the LOO scores, then use that to determine a precision/recall/f-score, where I'd see how many "Core Event = yes" responses fall above or below the threshold. I think it would also be wise to get such a score for "perfect" responses (where all 5 feature annotations are "yes").

I'm spending some time looking at LOO scores to establish thresholds today. I don't think we simply want to take the GS response with the worst TC score....
## END: IGNORE

2019/10/28. Monday.
I've been thinking a lot about the above and how the scoring would happen. I don't think we can simply use precision/recall/f-scores.
I'd been struggling with the idea of applying Spearman because I haven't had a plan for what the ideal ranking would be. The automatic ranking would be the TC score ranking, but the "oracle" ranking or GS ranking (not to be confused with the GS response sets) has been vague for a long time... I assumed it would use the annotations somehow, but I wasn't sure how. Now I've got it -- the GS ranking will be based on a weighted ranking of the feature annotations, with those weights derived empirically... The problem I had before was that it's just plain difficult to label a response as "good" or "bad" -- after all, that's how we ended up with the 5 features in the first place. But, it's much less difficult to read two responses and choose which is better (or decide they are equal)! If we make enough of these decisions, we can see the rate at which each annotation occurs with the best of the two responses. In practice, I think the judgements would have to be "A is better" "B is better" "Unsure/no difference". I think we have about 14,000 responses. If we have 10% of those paired up (so, 1,400 responses in 700 pairings) and we make decisions on all those, I think we'll have a good enough sample for deriving weights.
So my immediate goal is to assemble such a sample of response pairings. We don't need to worry about separating dev set and test set here. We do want to ensure that for each pairing, the annotation vectors are NOT identical. If the annotation vectors ARE identical, we learn nothing from the decision. So, I need to write a python script that can read through each master anno file, pull out the lines that have responses (many are blank), and put together a list of pairings with non-identical annotations comprising 10% of the total number of responses.
Hmm. I think we'd also prefer to maximize the variety of annotation vector pairings, if possible. I assume certain vectors are more common than others, and so certain pairings are probably also more common. But if we can still reach the desired sample size without running out of interesting vector pairings, we should do that.
So, in practice, I think that for each csv, we'll want to put each response line into a dictionary where the keys are the unique vectors that occur among those responses. From there, let's use a randomizer to choose pairings based on annotation vectors. We can keep a master list of the annotations we've already made and skip over them if they come up again. If we run out of options we then back off to select them (no skip). When we get to 10%, we write them to a single output file and we move on to the next input file. Let's give each a pairing number, like I01T-01a vs I01T-01b, etc.
Later we'll retool the feature annotation tool to read from the pairings file and create an identical output file with new columns for the pairing decisions. For now, this will be "Better, "Worse", "Same". (These will be used for deriving weights later, but I'm not sure if I'll need to consider some kind of normalization to account for the baseline frequency of each feature.... I think it will probably come out in the wash.)
So right now, I'm going to find a previous script to copy over and repurpose for the pairing sampler script.

11/30. Saturday.
I've pulled the sample pairs (1500 pairs), created a rough annotation interface, and scored the pairs. Then I did the math to get the feature weights:
Core:	0.364958887
Answer: 0.092979127
Gramm: 0.055660974
Inerp: 0.223908918
Verif: 0.262492094

So these weights can be applied to any annotated response, and thus we can rank any set of annotated responses this way.
Most experiments here will involve comparing the ("GS") ranking generated by a given GS against the manual ranking. (I need to settle on terms for discussing these different datasets and rankings...)
Today I'd like to see if I can run one experiment. I'm choosing weighted vs unweighted dependencies. It will be good to do this one first -- we'll establish which is better, then we'll only use that one going forward.

2019/12/1. Sunday.
Today we're implementing Spearman for the comparison of TC rankings using weighted and unweighted dependencies. We'll be comparing them against the gold standard ranking derived from the weighted annotations.


2019/12/2. Monday.
Chatted with Markus:
* repeat weighted dependency experiment with smaller GS -- it may be that noise washes out at higher volumes but not with a smaller GS.
* write up recent work before moving on


2019/12/07. Sunday.
OK, I've run the weighting dependency experiment again, this time using the "all_fns" GSs -- these are only ~30-40 responses each, vs ~140-150 responses in the "all_ns" GSs.

One thing I'm unsure of... Spearman ranges from -1 to 1; most of my values are negative, but a handful are positive; when averaging these, should I take the simple sum or the sum of the absolute values? This makes a slight difference in which shows a stronger correlation -- weighted or unweighted dependencies.

For all_ns GSs:
Avg of sums:					w < uw (i.e., unweighted has a stronger correlation)
Avg of abs val sums:	w > uw

For all_fns GSs:
Avg of sums:					w < uw
Avg of abs val sums:	w < uw

In all cases, the differences are very small, probably too small to worry about this variable going forward. In all cases, the p values are ranging from .0475 - .0541. 


2020/01/25. Saturday.
AB tests for weighting features:
I'm preparing the data and interface for Annotator 2. I'm going to tweak the interface a little so it can write to a copy of the file already annotated by Annotator 1. A1 did 20 pairs per item; A2 will do a subset of those 20 -- 5 per item. The pairs are numbered like so:
I01T-001-a / I01T-001-b. So right now I'm preparing a new input file containing only every 4th pair -- I01T-004-a/b, I01T-008-a/b, etc., which will give me 5 pairs per item.


2020/07/17. Friday
Implementing n-fold cross-validation.
Also updating code such that dependencies and their tf-idf scores are written to file;
For each item, I need a CSV; header will list each GS dependency;
Next row is for GS tf-idf scores for those dependencies;
Each subsequent row is for one test response and shows tf-idf scores for the dependencies in the header;
At this point, we also need a function to expand the header when new dependencies are seen in the test response;
I don't think we should do this on the fly... i think it will be better to first write a file for each RESPONSE;
This CSV will contain the union of GS and test responses as the header;
Next line is GS tf-idf values for the deps;
Next line is test response tf-idf values for the deps;
End of file;
After we've run each test response for the item, then we have a script that combines each of the three-line CSVs into a single CSV as described above;
(We may want to then delete the three-line CSVs.)

2020/07/18. Saturday.
OK, there are stats about the GS counts in this file:
/Users/leviking/Documents/dissertation/SAILS/gs_response_counts_MD.xlsx
For cross-validation, we need to know how many responses are in each "all_ns" GS so that we can sample appropriately.
The all_ns GS ranges from 134 responses to 190 responses.
The familiar sets range from 17 to 34.
I think I should sample at 15, because this size will also work later when sampling the FNS and CNS sets in that experiment.
I think I should also sample at 50.
So now I'm going to generate the new sampled GSs for cross-validation. This needs to operate on these files:
/Users/leviking/Documents/dissertation/SAILS/gold_standards/depstrings/I01T_all_ns_depstrings.csv (for all 60 items).
The new files will be stored here:
/Users/leviking/Documents/dissertation/SAILS/gold_standards/cross-validation/depstrings/<sample_size>

# # XF recommends using these numbers for comparing Spearman:
# # lower 5, median, 95, average, standard deviation, maybe also min and max
## There should be a python command "describe"? that spits out many of the above measures;
## This analysis would be considered "descriptive statistics"


###
Parameters:
(Item number, but we will always do all items)
**Assuming bootstrapping with replacement
1**. Number of training sets (K)
2**. Number of responses per training set (J)
3. Dependency format [ldh, xdh, xdx]
4. Source: Familiar vs. Survey (crowd sourced)
5. Targeted vs Untargeted
## 6. Annotations: Any, Perfect, CoreYes
7. First Responses vs. First and Second mixed
8. Intransitive, Transitive, Ditransitive
## 9. Normalized / non-normalized (are responses normalized for length so that dependencies in longer sentences weigh less than deps in shorter sentences?)

So a given training set file will be something like:
I01T-K081_100-J15-LDH-FAM-ANY-FIRST-I-NRM.csv ; meaning:
Item01Targeted-Set81/100-15responses-LDH-Familiar-AnyAnnotation-FirstResponsesOnly-Intransitives-Normalized
So we could have for each of these parameters:
30 * 100 * 2 (or 3?) * 3 * 2 * 3 * 2 * 3 * 2 = 30 items * 43200 = 1,296,000

## So exploring the space exhaustively means 1,296,000 models, each of which generates a Spearman rank correlation. Surely this is too much. I propose some simplifications.

1. [3 above] Dependency formats: This should be broken down into 

1. [7 above] Familiar vs Crowd Sourced: Let this be a one-off experiment that scopes across all other parameters (i.e., ignores them). We have very little familiar data vs the crowdsourced data -- 17-~30 familiar responses per item, vs. ~130-~160 crowdsourced. All other experiments should be trained with crowdsourced data only. This experiment is more of a "what if" to give some insights into the value of choosing trusted sources vs crowdsourcing.

2. [9 above] Normalizing for response length. Let's skip this entirely. The numbers I've run so far show this as having very little impact anyway. I totally volunteered this experiment because I thought it was interesting, so knowing it isn't, I'd be fine with eliminating it. All processing would be non-normalized (the simpler approach).

3. [6 above] Annotation based filtering. I was going to argue that since the annotations were applied manually and blindly for all responses without regard to other variables, these should be sufficiently independent of other variables and we could do this as a one-off, too. Then I realized -- we're probably not really learning anything in this experiment -- we're comparing one form of an annotation-based ranking (the binary annotations * the respective weights = the GS ranking) vs another annotation-based ranking (an encoder model where the training data responses is filtered by binary annotations).



