2019 Log.

Most recently, I was mainly doing GS work. There's a log for that: 2018_log_GS.txt. It's really short, so I'm just going to paste the whole of it below.

##### Begin 2018_log_GS.txt #####

2018-07-20.

The GS files are fully parsed; see /Users/leviking/Documents/dissertation/SAILS/gold_standards/finalcsvs

Now I must begin the scoring process. I need to do leave one out testing on each of the gold standards.

2018-07-23.
Currently working on the files OLDprep_conll_for_tfidf.sh and OLDprep_conll_for_tfidf.py; I will rename these without the "OLD" when they are fully updated.
DONE.

2018-07-24.
Today I'm starting work on the leave-one-out (LOO) GS experiments. I need to drag out the tf-idf scripts from last time and prepare them for LOO.

Oh, this time, I'm only going to use the tf-idf cosine ("TC") approach; this was the clear winner in the pilot study. See the table pasted below. So it'll be TC and the Brown Corpus for sure, and I think I should look at xdh, ldh, and probably xdx.
For the term (depstring) forms,
Approach	form		Ref Corpus		
0.51577 TC xdh 0.51810 Brown 0.51534
0.50780 FC ldh 0.51677 WSJ 0.50798
0.50755 TA lxh 0.51350
0.49464 FA xdx 0.49901
			ldx 0.49352

I need these scripts: lk_tfidf.sh, lk_tfidf.py
These have been copied to the scripts folder as "OLDlk_tfidf*"; I'll remove the "OLD" when they are completed.
I'm not quite ready to implement LOO. These scripts simply compare dependencies in the test file with dependencies in the reference corpus and record the resulting tf-idf score for each term. I need to modify them to work on the csv files that now contain all the depstrings.


2018-08-14.
I've almost finished OLDlk_tfidf.py. It's basically working, but I need to uncomment the remaining deptypes in a couple of places in the main program; relatedly, I need to fix it so that I'm not working with empty lists after I run through the first deptype (I think this has something to do with the csvreader and/or the way the loop is managed).

2019-04-15.
I'm trying to get my mojo back. First I need to nail down exactly where the data processing left off. I believe this is what remains, in broad strokes:
* Leave One Out (LOO): Get average scores? See below.
* To the existing GS files, add columns where we include a weight factor for the weight of each dependency (for each dependency, weight is 1/n, for n = total number of deps in response)
* Repeat full process for non-GS response files
* Modify pipeline to operate on csv

It's been a while but it looks like LOO for the GS files has been executed -- each response has a vector similarity score (cosine).
I think the question is-- what to do with those... I believe they should be averaged per file (test item), and probably per other settings *across* all test items; those settings would be depstring format: ldh, xdh, xdx; speakers: all_cns (all crowdsourced NS), all_fns (all familiar NS), all_ns; annotations: perfects, almosts, core_yes; response ordering: firsts, seconds.

I need to examine the script score_GS_LOO_TC.py; this appears to be the beginnings of a script to perform the above paragraph, I think. If not, figure out what it is and what to do with it...


2019-04-17.
TODO: I believe I need to return to this script; investigate and implement as necessary:
/Users/leviking/Documents/dissertation/SAILS/scripts/lk_tfidf_LOO_TC_weighted.py

For now:
OK, I'm getting myself back on track re Spearman's correlation and how we want to use it.
We want to rank responses by annotation: scores range 0-1, where each "1" annotation carries a 0.2 score.
We want to rank responses by TC (tf-idf cosine).
Then when we get Spearman scores, that score applies to a given Gold Standard. So Spearman is used to evaluate the GSs, not the responses. It gives us a measure of how well using TC on a GS can align with using our manual annotations (which one would not generally have "in the wild").

##### End 2018_log_GS.txt #####

2019-05-05. Sunday.

My immediate goal is to finish the data work. At the very least, the processing/pre-processing:
1. Finish GS CSVs:
	Weight dependencies
	Regenerate LOO scores (cosine) on weighted dependency responses
	
2. Process all NNS data (CSVs):
	Parse all responses (in CSV)
	Generate "depstrings" (ldh, xdh, xdx)
	
3. Generate cosine scores for all NNS responses (vs. each GS)

4. Get Spearman scores for each GS

5. Based on Spearman results, choose best GS format and threshold, apply to held out test data.

From the header of lk_tfidf_LOO_TC_weighted.py, here's an explanation of weighted dependencies:

##2018-08-23. This is a modified version of lk_tfidf_LOO_TC.py. It does a similar analysis, but uses a slightly smarter approach to the GS. The original version simply dumps all dependencies in the GS into one "bag", so that all dependencies (tokens) carry an equal weight. In this version, however, all *responses* are equal, and each dependency (token) in a GS response of length n dependencies (tokens) receives a weight of 1/n. My thinking is that this should minimize the weight of extraneous, uninformative dependencies in the GS. Compare these two hypothetical GS responses:
##The smiling man is planting a baby tree in the green grass on a sunny day.
##The man is planting a tree.
##So the impact of dependencies like "amod-green-grass" should be lowered, while the impact of "obj-tree-plant" should be boosted (relatively speaking). 

Crucially, note that the weighting applies to the GS only. We're measuring the similarity of 1 response to a collection of responses; This means that we'd simply be weighting each dependency in the test response by dividing by 1 (the number of responses it appears in), so sure -- in theory weighting applies here, but it all comes out in the wash. So we're going to skip implementing it for the test responses.

2019-05-11. Saturday.
I've now combed through lk_tfidf_LOO_TC_weighted.py entirely and reworked some things. It's not weighted at all now -- it's really just a clean implementation of the unweighted version, which I'm pretty sure was not doing what I thought it was doing. But I haven't used any scores from it for anything yet, so it's not a problem. I'm going to save this as lk_tfidf_LOO_TC.py and make the necessary minor changes within the script. [DONE]
Then I need to return to lk_tfidf_LOO_TC_weighted.py and actually implement weighting here.
DONE! Weighting is implemented.
Now I need to run the shell script in order to process all the files.

2019-05-19. Sunday.
I ran the GS LOO scripts (assigning a "TC" to each GS response) for both non-weighted and weighted dependencies for all GSs for items 1-10 (targeted and untargeted, ldh, xdh, xdx). It takes about 2 hours to do this for a single item, either in weighted or non-weighted setting, so it's about 4 hours total for one item. So with 20 items remaining, this will take about 80 more hours. I may look into running this on an IU cluster to speed it up.
I also finished a script ((score_GS_LOO_TC-weighted.py)) to score the GSs for a single item (i.e., 01T unweighted), so we can see which GS correlates TC scores and annotation scores the highest. I'm realizing now that it would be better if we include weighted and unweighted dependency approaches in the table of GS scores that this outputs; this would allow us to compare weighed and unweighted GSs.

Goals today:
1. Find the status of NNS responses: Are they parsed, etc.? tf-idf scored?
	a. If necessary: process responses into depstrings; get tf-idf scores
2. Implement spearman scoring for NNS responses
	a. This part is fast: we can apply weights to annotation on the fly here. Let's generate score spreadsheets based on a few different weighting patterns (features C, A, G, I, V):
		1. flat: .2 for all annotation features
		2. core only: 1.0 for core
		3. Core Heavy: .6, .1, .1, .1, .1
		4. Ideal: .5, .2, .05, .15, .1 ## representing my intuition on the importance of each feature in an ICALL context
		I think that's probably enough...
		
OK, here's some pseudocode to serve as an outline for the processing of the test responses:
# pseudocode (see also sails_NNS_stanford_penn_to_conll.sh)
## for masterannofile in corpusdir:
  ## run python script, which does:
    ## generate "rawcsv" of NNS only, with fields: ResponseID, Response, C, A, G, I, V
  ####### BELOW: Repurpose /scripts/gscsv_to_lemmatized_conll.sh for these steps
  ## from rawcsv, get text file of responses;
  ## with stanford coreNLP, lemmatize responses;
      #### can we cache here to avoid duplicate processing? (we must load lemmatizer for each response)
  ## with stanford parser, parse text to penn, store in csv w fields: ReponseID, penn parse
  ## from penncsv, get text file of penn;
  ## with stanford parser, parse this to conll, store in csv w fields: ResponseID, conll parse
  ####### ABOVE: Repurpose /scripts/gscsv_to_lemmatized_conll.sh for these steps
  ## run lemmatize_conll.py to get lemma_conll files
  ## add lemmatized parses to rawcsv to create finalcsv; see add_parses_to_gscsv.py; repurpose (simplify) this script for test reponses
  ## run prep_conll_for_tfidf.py: this generates the *depstrings.csv, which is finalcsv + ldh, xdh, xdx
#####
## Then we run lk_tfidf.py (must create this by modifying lk_tfidf_LOO_TC.py) on each test item csv
  ## Note that this must score each test response in relation to each GS;
      #### can we cache here to avoid duplicate processing?
## Then we need to score the GSs according to Spearman correlations

2019/06/02. Sunday.
I'm back on the case. Let's work through the above goals.

EOD. Today I finished response_csv_to_txt.py; This takes a single "master_anno" csv file from the SAILS/corpus/ directory. It finds all the NNS responses and writes the response sentence to a .txt file in SAILS/responses/txt/, and writes the ResponseID, Response, C, A, G, I, V to a row of a corresponding csv in SAILS/responses/rawcsvs/, e.g. I0T_NNS.csv.
This will be run via a shell script -- it takes the relative path+filename to the master_anno file as an argument.

2019/06/16. Sunday.
OK, it looks like these steps remain:
(run all files through response_csv_to_txt.py -- probably use a shell, call it response_csv_to_txt.sh)
run text files through stanford CoreNLP lemmatizer
run text files through stanford parser --> penn
run penn parses through stanford parser --> conll
merge lemmatized sentence with dependency parse ( --> lemma_conll)

OK, I now have script text_to_lemmatized_conll.sh
While testing, I temporarily moved the responses folder to "responses-REAL-protected06-16"; I was testing the "responses" folder with only toy files. Now I have copied the responses back; I will start running the script again before I go to bed.

