2017-11-02
I've had to revisit the Answerhood annotation this week. I realized that in my attempts to automate some of the untargeted annotation based on the targeted annotation, I made some mistakes. I simply reannotated the entire targeted response set under the guidelines for the untargeted responses. Some of this was automated, some was manual. What I should have done instead was this: where appropriate, automatically apply the targeted annotations to any identical responses in the untargeted set, and then manually annotate the remaining responses.

So I now have a set of annotated targeted responses for which it would be appropriate to apply the annotation to any untargeted duplicates. My current task is to write a short script to do the string matching and autoannote the duplicates. I'm calling this applyTargetedAnnoToUntargeted.py.

KitsuneMBP:answerhood_working leviking$ for UF in $(ls *.csv); do cp $UF ${UF/.csv/TOTALLYTEMP.csv} ; rm $UF ; perl -pe 's/\r\n|\n|\r/\r\n/g' ${UF/.csv/TOTALLYTEMP.csv} > $UF ; done


2017-11-06. Meeting with Markus.

Accomplished since last week:
Re-annotated the untargeted Answerhood responses after a mix-up.
Annotated targeted Core Event.
Read Turner (2000)


Discuss Turner (2000) ("Rater Voices").
Developing rating scales: (See also Turner & Upshur (1996))
0. Consensus on testing construct
1. Divide sample into two halves -- UPPER, LOWER.
2. Discuss, revise, consensus
3. Find (binary) characteristics that distinguish upper & lower


November/December timeline:
Complete untargeted Core Event annotation (11/8 Wed)
Re-annotate all "maybe" responses (11/20 Mon)
Write Task/Participants/Data sections (11/24 Fri)

Read Turner & Upshur (1996) [PAYWALLED; find offline?], Chalhoub-Deville (1997)
Establish holistic rating scale [performance-based rubrics, such as empirically derived, binary choice, boundary definition (EBB) scales]
	3 item sample
		Split into Upper & Lower; Examine/discuss (11/10 Fri) [Ask Sabrina to help here]
		Split Upper (into L3 & L4?); Examine/discuss (11/24 Fri)
		Split Lower (into L1 & L2?); Examine/discuss (11/24 Fri)
	Write up holistic rating guidelines (11/30 Fri)
		
Implement holistic rating scale for all responses (12/14 Thurs)
Collect binary and holistic ratings (samples) from Annotator 2 (12/7 Thurs)
Apply *type* annotations to all response *tokens* (12/18 Mon)
Calculate inter-rater agreement (12/20 Wed)



RE: Answerhood "maybe": How should I handle "going to/getting ready to/about to/fixing to" <verb>?

NTS: data cleanup: "the happening is a horse show" (I09U); "the image did not load" (and similar throughout); "how many more examples do you need to see one ends their sentences with a noun? enough already" (I18U);

#####################################################
NOTES FOR 11/21 MEETING:

NTS: Could I use NER to downweight responses that use names/proper nouns? There are very few legitimate appearances of named entities in the responses. [exceptions: "riding English style", "Australian crawl", maybe others]

Could we get away with a simple yes/no holistic scale?? That would be AWESOME!

Can I work some kind of machine learning in here?

MD says: Using GS data, can we predict something about NNS data? Compare to grammatical error detection...
	Responses with 5/5 binary features ("yes") or 0/0 ("no") are clear 1 and 0 in holistic scale. Examine the 4/5 and 1/5 to get an idea of how to approach holistic annotation. [Pin down the construct by looking at the existing binary feature annotations.]


#####################################################

Re: Annotation and holistic modeling and scoring

Markus,

I just had a long talk with Xuefu about my work. I wanted to write it up while it's fresh in my mind. We discussed with two possible approaches for moving forward.

Approach 1:
I would set aside the completed binary feature annotation for now.
I would complete the holistic rating via the EBB process. (Whether this results in a 2 or 4 point scale is up for debate.)
The binary features may or may not appear to be salient in the resulting holistic ratings, which means that their main role in the dissertation would simply be to provide observation and discussion about the data and report on these features in a SLS/Linguistics oriented chapter.
The CL work would proceed more or less as expected, where the dependency-based system predicts holistic scores for responses.

Approach 2:
I would derive holistic scores from the (weighted) feature annotations. (Again, the scale itself is TBD.) This would be done with a small(?) sample and some kind of cross-validation would be used. Part of the argument here is that this kind of feature annotation is expensive, so we are exploring an approach that could be repeated with less expense. Future test/game/etc. developers could do feature annotation for a sample of responses and use that to derive a model or models which could then complete the holistic annotation.
Again, the CL work would be the same -- use the dependency-based system to predict the holistic scores.

Xuefu likes Approach 2. I don't dislike it, but I think I'd need to sell it a little and try to justify it somehow with literature, etc. I would basically need to say, "I examined the data, did some preliminary annotation, and decided that these features have the most bearing on the 'goodness' of a response. [All of which is true, up to this point.] I also used my judgment and intuitions from this process to assign weights to the features in order to derive a holistic score." This probably makes the most use of the work completed so far, where I attempted the holistic annotation, then decided it couldn't be easily done without somehow considering the features we arrived at.
In Xuefu's words, "We're providing a framework for developing an automatic rating system. We're not claiming that relying on these five features is absolutely the best approach, but we're examining the feasability of such a framework (i.e., deriving holistic scores based on a sample of linguistic feature annotations and approximating those scores with an automated system.)"

Prior to my meeting with you today, I was staring at the data and wondering how I can make the holistic judgments. It felt like I was back at square one, before I started the feature annotation. It's difficult and feels arbitrary, and this makes me think it would result in very low inter-annotator agreement. I need to either:
A) have a well-defined testing construct and clear guidelines OR,
B) as EBB handles this, simply divide the data into two halves by quality (which of course still requires a clear testing construct) and then determine what criteria separate the halves, and repeat the dividing process as necessary.
I think Approach 2 here largely avoids those problems.

I'll be thinking about this more in the coming days as I complete the feature annotation and tidy up the data. I'm interested to hear your thoughts too, now that I've better articulated the concerns. But it's Thanksgiving Break, so enjoy yourself and reply whenever it's convenient -- I have plenty to work on in the meantime.

Regards,
Levi

#####################################################

NTS: 11/21: I'm only finding the untargeted re-merged answerhood files. Troubleshoot this and find or generate the targeted files.

11/22: I found the targeted files, but they need to be sorted back to the original order -- use another feature file to do this. I'm writing a quick script to sort one csv according to another. See '.../responses/2039/sort_csvX_with_csvY.py'

OK, this problem is solved. The targeted answerhood files have been sorted with the python script.

#####################################################

Binary Feature Annotation TO DO [11/25]:
[DONE] 1. Finish remaining Verifiability items
1.b. Revisit Answerhood for responses that show immiment action: "about to/going to/getting ready to/etc."
2. Finish remaining Core Event items
	2.b. For items 1-16 (T&U), I need to revisit and ensure that no subject/object in the core event is referred to by a proper name. I updated this in the guidelines and annotated accordingly from 17 onward.
	2.c. There seems to be no clear instruction for how to handle responses that give a subject (or object?) with the incorrect number ("the women is buying a car", etc.) [As of I19U, I have updated this to say that a response that contains a subject or object with the incorrect number (singular/plural) as a part of the core event should be annotated "no" for core event. I'll need to do some clean up in I01-I19T... maybe just some ctrl+f for a few variations on the subjects and objects... ]
3. Finish second pass (annotate 'maybe' responses)
	* Write script to handle second pass annotation; it should preserve the first pass annotations! Because we may want to know later which responses were difficult to annotate.
4. Write script to reapply type annotations to tokens.
5. Clean up! Remove "The image did not load", etc.

#####################################################

NEXT MEETING [for MD meeting, 12/4]:

11/25: So, something just occurred to me, and I think it might be kind of important. If the holistic score is a function of weighted features, then we probably wouldn't simply use one holistic model, or one model per point on the holistic score... Instead, we'd have two models ('yes' & 'no') per feature... so a test response could be compared with each of these to determine its feature scores, and these could be multiplied by the feature weights to arrive at a holistic score. I'm not completely clear on how to make this work, but I think this deserves further consideration.

11/27: I think I've got the holistic scale figured out!
3: Perfect response (All binary features = 1)
2: Passing response (Core Event = 1, but one or more other binary features = 0)
1: Non-passing response (Core Event = 0)

Idea: Consider targeted responses. Some targeted responses are very short, consisting of only a verb phrase (e.g., "eating pizza"). Any such very short response that is annotated as having the core event is extra valuable, because we know there isn't any fluff here -- these dependencies are the essence of the item.* On the other end of the spectrum, we have long, rambling responses, and many of these are also annotated as having the core event. In the pilot study (if you will), we tossed all the gold standard dependencies into the same bag. I'm sure you see where I'm going with this -- I think each dependency in a GS (or whatever kind of dependency-based model we end up with) should be weighted more thoughtfully. A single response should have a weight of 1, and each dependency it that response should have a weight of 1/D, where D is the total number of dependencies in the response.
*(I may even want to think about implementing a check to see if the subject was dropped, and then automatically adding it back in so that the subject dependency is represented in the models. But this is a side thought... )

######################################################

From above:
	2.b. For items 1-16 (T&U), I need to revisit and ensure that no subject/object in the core event is referred to by a proper name. I updated this in the guidelines and annotated accordingly from 17 onward.
	2.c. There seems to be no clear instruction for how to handle responses that give a subject (or object?) with the incorrect number ("the women is buying a car", etc.) [As of I19U, I have updated this to say that a response that contains a subject or object with the incorrect number (singular/plural) as a part of the core event should be annotated "no" for core event. I'll need to do some clean up in I01-I19T... maybe just some ctrl+f for a few variations on the subjects and objects... ]
Regarding all this... I think I need to take a break and reflect on exactly what core event is, what it does for the process and how it relates to the philosophy of wthis project... Do we want to allow for proper names? Wrong pronouns? Plurals that should be singular? Shouldn't these problems be reflected through other features, like Grammaticality? [I think so...] Part of my concern is with the idea of a Gold Standard -- do "The women is taking a picture" and "The girl is using a camara" make it into a GS? I think the answer is 'no', because GS responses should fulfill all five features, and these will fail Grammaticality.

######################################################

12/4 (prior to meeting today):
I need to get a few example items ready... Let's use items 1-3. Double check for any 'maybe' annotations. I'm going to edit the individual feature files before compiling them. Then I'll sort them into the 3 point scale.

######################################################
12/4 Meeting with MD:
Core Event:
cf LFG: coherence, consistency, completeness
Maybe the criteria for Core Event is: it cannot be in conflict with the context. So a named subject (of correct gender) is ok. A pronoun of the incorrect gender is not okay. A subject that is plural but should be singular is okay -- consider lemmas here.
Avoid overlap with verifiability, grammaticality, etc. -- a response should be penalized for a given offense only once, whenever possible.

######################################################

12/5

Not really in the mood to wade back into the core event annotation and clean up. One thing I can do right now is this: The grammaticality responses (types) were combined into one file (containing targeted and untargeted) and then annotated. I need separate files -- 1 for targeted, 1 for untargeted. I need a script to do this.

I think it worked... check after returning from trivia!
Yes, it did work. The separated grammaticality files are now in the 'Gramm' folder, in the same configuration as the other feature files.

######################################################
12/13

I'm back in the core annotation now.

######################################################
12/19

Core event guidelines have been mostly updated. Check against the notes from previous meeting.
Core event annotation is finished, but some early items may need double checking. [WAIT -- A few items were skipped... revisit this.]
I have started a script for compiling all the annotation layers into one spreadsheet, but this is not yet working.
###
Meeting with MD:

We're aiming for Feb or March for a paper submission date for a paper about the dataset. (BEA or ACL)
Look at LREC and LAW proceedings for examples of papers based on dataset (and annotation schemes) releases.

Do over break:
Revisit timeline; what's happening in the spring; try to hit the ground running in January;
Continue working on the final compilation script and try to get the processing pipeline set up to read from the final compiled version of the data.

###

2018-01-09 Meeting with MD:

To do:
By Friday 1/12:
[DONE] Re-examine Core Event annotation for Items 1-18 (targeted and untargeted); Per updated guidelines, review all "no" annotations. Some "no" annotations will need to be changed to "yes".
[DONE] Ask Annotator 2 for some written feedback about the annotation scheme and the process. What's difficult, unclear, etc.?
[DONE/NA] Conference with Annotator 2 to compare annotations. Try to reach consensus; ask Annotator 2 to re-annotate if necessary. 
Compile all annotation layers from all annotators into a single CSV file; (response types).
Apply the annotations from the compiled response types file to the original response tokens file. Be careful as some minor changes were made in condensing tokens to types (punctuation stripped, etc.).
Begin data writeup / BEA paper (submissions due 3/20) or ACL paper (2/22).
Email committee update

Per MD:
Email out a brief update to committee, including this timeline (actually, put completed tasks back in the timeline to show progress).


2018-01-20.

Working with Annotator 2 today.

I have rechecked my "no" annotations for Core Event for items 1-3. [items 4-18 also need to be rechecked for "no" annotations (see notes above) but now the priority is working with items 1-3, because Annotator 2 has also completed items 1-3.]

Items 1-3 Core Event have now been rechecked for "maybe" annotations; these are in the 2039/sequestered_maybe/Core folder, where they are the "secondpassanno" files.

I will now complete the rechecking of items 1-3 for the remaining features. These will also be in 2039/sequestered_maybe/<FEATURE>.

Items 1-3, "maybes" rechecked:
	Core
	Answer
	Gramm
	Interp
	Verif


Answerhood: Guidelines must be updated (Verb Forms section?) to say that non-dynamic sentences that indicate imminent action should also be accepted here. e.g., "The boy is *about to* eat pizza"




### Feedback from Annotator 2
difficult to remember the many criteria for the different features; especially when working on more than one feature in a short time.
Grammaticality: grammar is more subjective and varied than we normally think.
Interpretability: many respondents assume that the reader knows details or even arguments (but we judge this without considering the image); slang is often a toss up and judgement call
Core event: For most items, it was very clear from the image what core event the item was intended to elicit. Slightly less so for items with more than one interpretation -- is the man delivering a package or picking one up?
Verifiability: These were mostly easy to determine, but embellishments sometimes pose a challenge. "Lunch"/"a snack" or "pepperoni"/etc. were common but not verifiable.
Answerhood: Some respondents didn't take the task seriously at all. The items were very answerable, but some people didn't try. It's easy to overlook minor changes to the form of the targeted subject, but it's not a problem if you keep this in mind. There is also some gray area between active verbs, descriptions of states, and states that indicate imminent action.

####
1/22 Meeting with MD:

We discussed agreement scores.
MD: Next step, for writing paper, would be to examine individual cases of disagreement.
Why does Core Event break the general trend of targeted having higher agreement than untargeted?
Calculate kappa scores (this accounts for chance, which is high because this is a binary choice, and because some items are biased toward 'yes' annotations, etc.). There may be better measures for this task; see the paper Markus sent


Hypothetical / probable Gold Standard versions:
All Familar NS
All Crowdsourced NS
All perfect annotations from NS ("Oracle"?)
All "yes" Core Event responses


### Back to cleaning up Core #4-18 "no" annotations.
DONE!
ALL Core annotations are now in line with the most recent updates to the guidelines.


Now it is time to complete the re-annotation of any outstanding "maybe" annotations. (I did an earlier, manual pass at this, but left many undecided...)
I will begin with the Core annotations for items 4-30 (Items 1-3 are completed).
DONE!
The Core folder inside the sequestered_maybe folder now contains "firstpass" versions (containing maybes) and "secondpass" versions, which do not contain maybes.

KitsuneMBP:2039 leviking$
diff sequestered_maybe/Core/firstpassanno_I03T_Core-2039.csv sequestered_no/Core/firstpassanno_I03T_Core-2039.csv 


####
FIRST PASS annotations are now all in the "firstpass" folder and its feature subfolders.

The 'sequestered_no' folder has been deleted. The (Core) files we needed to generate there were renamed and moved to the 'firstpass' folder, as necessary. 

We should now be working only in the 'sequestered_maybe' folder. (Core is complete -- the firstpass files there reflect any rechecked 'no' annotations, and the secondpass files there do not have 'maybe' annotations). The 'sequestered_maybe' folder should be used to generate secondpass annotation files. We will then move those files to a "secondpass" folder, or something like that.



OK, I realized there was a problem/inconsistency with the answerhood annotations. Markus and I decided to accept responses that indicate "imminent action" (The boy is about to eat the pizza) whether they use a progressive verb or not. I had not yet updated the guidelines [DONE now] or the annotations themselves.
DONE -- all the relevant response annotations have been updated re: answerhood & "imminent action". The effected files have been copied back into the "firstpass" folder. In other words, this issue is closed! Move on to the reannotation of "maybe" responses.

###
ANNOTATING REMAINING "MAYBE" RESPONSES:
DONE. Core Event
DONE. Answerhood
DONE. Verifiability
DONE. Grammaticality
DONE. Interpretability

###
Fri Feb  2 16:26:51 EST 2018
The second pass annotation is 100% complete. There are no "maybe" annotations in the secondpass* files.
Now I need to get the type annotations applied back onto the token files, so we can calculate kappa.

#############
KAPPA scores
OK, everything above is resolved (I think). I now have Cohen's Kappa scores for annotation agreement. See the file agreement_stats_output.xlsx.
Now my task is to look at cases of agreement and disagreement and see if I can find any patterns.
Then I should be ready to write the paper/chapter.


####
Re: interp disagreements:
"enjoy" & "like" "celebrate" -- A2 does not find these interpretable
Previous disagreement stemmed from A2 being more strict with regard to gender of subjects and objects (human). We since agreed that gender is an exception to the rule about use of vague subjects/objects like "someone". I'll need to specify this more clearly in the guidelines. If the gender of a particular figure in the image is not really important or relevant to the event, i.e., if picturing a person of any gender in that position is possible, a subject or object that does not specify gender should be okay. The current rule about vague subjects/objects was really to discriminate against responses like "a man is doing something for someone".
In discussion of disagreements in item 3, A2 states that she previously accepted some telic verbs (not sure how to say this, but ~"receptive" verbs?) like "accept" and "receive" without an indirect object ("from the man"). Upon discussion of disagreements, A2 concludes that such verbs cannot be interpretable without a specified indirect object; the telicity of the event demands that the indirect object, who takes a direct role in the event, must be present. A2 notes that this is in contrast to atelic or repeated actions ("delivering a package" or "delivering packages"), because while these are likely to involve an indirect object, the indirect object is not strictly necessary.
[I think the above distinction is a function of thematic roles more than telicity. When the subject has a thematic role of Recipient, the indirect object (agent) must be present. When the subject is the agent, the indirect object may not be necessary, depending more on the required verb arguments (and telicity might have some bearing on this.)]
A2 states that she was too strict in assessing slang; following reconciliation, A2 re-annotated many slang responses as "yes" for interp.
A2 convinced A1 that minor preposition errors should be overlooked here. "the boy is eating for pizza".

Re: Verif disagreements:
A2 concedes that time of day is generally not verifiable (unless explicitly demonstrated in the image). Thus eating something cannot be considered "lunch" without more information.
A2 previously rejected some responses because the role of a person in the image was specified, e.g., "secretary", "receptionist". We consulted and agreed that such labels are acceptable.

Re: Answerhood disagreements:
A2 previously accepted numerous responses that changed the form of the subject. A2 concedes these should be rejected.
A2 seemed to overlook the requirement for answerhood that responses take the progressive form; A2 accepted many simple present responses. A2 agreed these should be rejected.


Re: Core event disagreements:
Some disagreements simply stem from a different interpretation of the image and its core event. In some cases, annotators do not agree on how specific the core event should be, or whether to allow particular slang responses.

Re: Grammaticality diagreements:
In most cases, annotators disagree over whether a minor grammar issue is grounds for rejecting the response.
A2 allowed some responses with bare nouns; A2 concedes that (per guidelines), these should be rejected.
A2 allowed dropped subjects in the untargeted responses. A2 agrees these should be rejected.

###
2/12/2018
I have a few files of disagreement responses where A2 changed some annotations upon discussion. On these disagreement files, I need to see if there are any of my own annotations that I'd like to change. [DONE]
Then I need to apply the new A1 and A2 annotations to the responses and recalculate the agreement/kappa scores. [DONE]
In the meantime, A2 is annotating a new sample of items (following the discussions). I'll need to reannotate this sample as well (blindly, without seeing A2's annotations). Then I'll calculate the agreement scores for the new sample, and these will serve as the "official" agreement scores.

2/13/2018.
Currently assembling the second sample of annotations (items 28-30)


##Note:
We need to calculate agreement scores for another dimension: intransitive/transitive/ditransitive.

2/18/2018.
A2 has annotated the test sample (items 28-30).
To do:
Separate A2's test sample grammaticality annotations (from TU file --> T & U files). [DONE]
Apply A2's (type) annotations to the token master files. [DONE]
Calculate agreement/kappa for the test sample. [DONE]
(Start writing the corpus paper.)


The test agreement scores are in and look about right to me. I need to move these from the "test_sample" folder on the desktop back into the main project folders and tidy up a bit. 

2/19/2018 meeting with MD.

Why did adjudication boost the Interp score so much?

From dev to test, why did some features improve and why did the others decline?
(Why is Core lower now?)

####
Explaining raw and kappa scores by feature:

Core: Core is lower than expected based on dev set adjudicated scores. For all test items, core has a lower chance agreement than other features (less skewed). Core has the lowest rate of "yes" annotations among all features -- this feature seems to be stricter than others.
	The ditransitive ("a man is giving directions to a woman") item drags down the average Core kappa. This was a difficult item to define by core event.
	Key disagreements: A2 accepts "pet"; for "giving directions", A1 sees the individuals as strangers having a brief interaction, A2 more often accepts answers that describe the figures as a pair engaged in some activity ("reading a map") together; A1 requires an "asymmetry" to the transaction -- man is pointing or relaying some directional information, but A2 is more likely to accept responses that less specifically indicate giving directions. Re: "jogging", both annotators show some inconsistency with regard to responses that only indicate "exercising" and not some form of running.

Answer: Highest kappa & raw agreement scores among the 5 features. Test scores are between initial dev scores and adjudicated test scores, as expected. Chance agreement is 72.11%, second highest after Gramm (76.82%).
	Key disagreements: Few disagreements, but most seem to stem from fatigue or error. From both annotators, there is some inconsistent application of the rule that targeted response subject is exactly the same as the subject in the question. A few disagreements involve responses where it is unclear if the the response refers to the specific ongoing action in the image or simply the action in general: e.g., "running is this woman's chosen activity".

Gramm: Scores --> No surprises, really. k=82.6, but raw agreement is much higher. Gramm tends to be more heavily skewed (toward 'yes') than any other feature.
	Disagreements: Fatigue is an issue -- some disagreements stem from an obviously incorrect annotation from one annotator. Inconsistent application of the "no bare nouns" rule. Some responses simply are ambiguous re: gramm.

Interp: A little lower than expected. Lowest kappa scores among the 5 features. Annotators note that this feature is the most difficult to judge consistently.
	Disagreements: For the 3 items in the sample, more than half of the disagreements come from the ditransitive. Many of these disagreements involve ditransitives that omit the indirect object: "he is indicating the way", "the man is giving direction" -- both annotators show some inconsistency here. There seems to be disagreement over the level of allowable inference: A1 allows some general statements like "the woman loves her dog", "jogging is one of her hobbies" and similar -- does this response equate to a visualizable action, or should it be taken as an abstraction? (A2 accepts no such general responses.)

Verif: No surprises re Kappa.
	Disagreements: A small number of errors/fatigue -- inconsistent annotation of "cat" and "pet" responses. Most disagreements, however, stem from judging which inferences are acceptable and which are not: "the man is showing the hiker points of interest on the map" and "the woman is cuddling her dog after coming home"
	

### 03/07/2018
Meeting with MD.

Me: Write Data collection section.

MD: 3.2 Agreement, 1 Intro

LK: 2 (Data collection), 3.1 Annotation Scheme, 4, examples for table 1, Help with citations throughout.



3/13/2018:
Current priority:
Modify agreement script to report NS and NNS separately. (Also, separate Familiar and Crowdsourced NS if possible; this won't likely be reported in current BEA paper, so it can wait.)



####
How am I getting EXACTLY the same number of response tokens for 28, 29 and 30?? (431 for each)

It's legit. All three targeted files have 226 response tokens, and all three untargeted files have 205 response tokens. I used an excel formula on each file to count the non-zero cells in the response column, and that's what I got.

MEETING:
for TTR table:
Try calculating the scores without the random sampling part -- take the whole set.

Today: MD: S 1 & 2, 3, abstract

I: move tables, change examples where appropriate; write discussion; recalculate TTR for entire sets (not samples) and if similar, report this and simplify the relevant paragraphs; add Joel citation in S4 per MD's marginpar.


####
For Friday, 3/23:
Finish participant_counter.py




Prepare for April 5 video interview with Google:

Review my own past publications (process, delegation of tasks, my role, results, etc.)

Review linguistics:
	POS tagging
	Constituency parsing
	Dependency parsing
	Semantic roles / theta roles
	What is:
		LFG
		HPSG
		CCG
	Jurafsky & Martin: Speech sections (Ch. 7 - 10)
	
Review *speech* specific linguistics:
	IPA
	Phonetics
	Phonology
	Disfluency & other speech features
	
Review tools for speech processing:
	transcription
	ELAN
	forced alignment
	PRAAT


###
04/09/2018
To do:
*blog
*job search
*apply for Wells Fargo job
*revise BEA paper
*BEGIN expanding paper into chapters


BEA Revisions:
MD: I think we can now more directly (& non-anonymously) compare to our previous work/resource, as well the ETS work.  Also, Detmar Meurers' group did some work on task-based corpora, and I feel like we're approaching that a bit.



New Timeline:
Expand BEA paper into:
	Lit Review (May 17)
	Data Collection (April 27)
		Task
		Participants
		Responses
	Annotation (May 10)
		Process
		Agreement
		Final corpus description

Experiments:
	Format and preprocess (parse & lemmatize, etc.) responses (April 30)
	Prepare GS (April 30)
	Derive GS thresholds? (May 17)
	Score responses (June 22)
	Analyze results (June 29)
	
Writing:
	Preprocessing (May 17)
	GS and thresholds (May 24)
	Process for scoring (July 20)
	Results (August 3)
	Intro (August 10)
	Conclusions (August 17)

**Present at BEA (June 5)
Complete BEA poster (May 30)



###
May 7

I want to revise my scripts to work exclusively with the csv format I'm currently using, rather than having to save each response in several different files for different formats, etc.

## May 8
Still trying to merge the demographic columns to a single "template". See "corpus_demographic_merge.py"; so far, this script just identifies problem cases, it doesn't fix anything. Some RespondentIDs appear with non-identical demographic info. They are all minor differences: sometimes '' vs '0', sometimes inconsistent use of whitespace, and sometimes e.g., 'USA 34 ' vs 'USA 34 years'



####
May 10

Current goal is to get the distribution of distances between single GS responses and the full GS; I want to do a cross-validation on each response in the GS to see what kind of distances occur. This will help establish thresholds for acceptability when we test NNS responses.

I need to get the pipeline revised and reconstructed in my current environment. I'm currently migrating scripts and files from the SAILS2014 folder.

####
May 11 (continued)


####
May 14. Meeting with Markus.

I want to ask Markus about this idea:
RE: GS.

GS Experiment 1:
Let's compile three versions:
1. Perfect annotations (5/5)
2. #1 + 4/5 annotations, but Core Event *must* be "1"
3. Everything else; i.e., Core Event = "0" AND/OR 3/5 or less.

MD:
1. perfect
2. 4/5 including core event
3. any with core=1

MD: let's try to optimize the GS using experment 

GS Experiment 2:
1. All NSs
2. All FNSs
3. All CNSs
*Experiment should look at different sample sizes for the GS

GS Experiment 3:
1. 1st responses
2. 2nd responses
3. Combined

MD: One possibility: Maybe later, just think about for now:
Try using all NS responses, but weighted by the annotation score: 5/5 = 1, 4/5 = .8; so dependencies are weighted in the GS.

Let's do something like use a dev set of 3 items per intrans/trans/ditrans; use leave-one-out testing (or whatever kind of annotation based experiments we decided on above) to establish thresholds (re: distance from the GS), then use remaining items as test set and see how well these thresholds generalize to new items. To be clear, we'd want to establish thresholds separately for intransitive, transitive, ditransitive.


empties:  46347
responses:  13533
NNS responses:  4290
NS responses:  9243
NSC responses:  7960
NSF responses:  1283
first responses:  4634
second responses:  4609
NSC firsts:  3992
NSC seconds:  3968
NSF firsts:  642
NSF seconds:  641

intrans=['01', '04', '07', '10', '13', '18', '20', '24', '27', '30']
trans=['02', '06', '09', '12', '15', '16', '19', '22', '25', '29']
ditrans=['03', '05', '08', '11', '14', '17', '21', '23', '26', '28']

These are the type to token ratios obtained from response_explorer.py. This represents all NS responses (and no NNS responses). These are broken down by targeted vs untargeted, verb type, and first vs second response. The 3 numbers for each are: type, token, TTR


6/21:
CORPUS NOTE:
I found that the corpus file for I19U has some problems. Specifically, the Core Event column is empty! Everything else looks normal... I checked to make sure the annotation columns weren't simply shifted to the right by one, with the last one cut off, and that is definitely not what happened. The remaining annotation columns made perfect sense. So I simply quickly reannotated the Core column directly in place in the main sails folder (the one that I sync with GitHub). I have also updated the GitHub repo with the corrected file.


7/2: Meeting with MD:
OK, for now, we've decided to proceed with GS experiments like so:
We will do leave one out (LOO) tests on the various GSs, without downsampling. For now, we are going to rely only on the GS responses -- no NNS responses. The immediate goal is to implement this with the complete set of NS responses. We take a naive approach -- the five binary annotations are converted to one annotation score consisting of one of the following: 0.0, 0.2, 0.4, 0.6, 0.8, 1.0. We then get system output scores for each response in the GS (via LOO). To assess the GS, we will look at some statistical measure (we don't know exactly what that will be yet) that indicates the degree of correlation between the annotation scores and the system scores. 
This should work for the GSs where the annotation isn't considered, but seems inappropriate for those sets that are already filtered by annotation (the "perfect" GS, etc.).
I wonder if we may be better off working with a development set of NNS responses for each GS -- a set of annotated responses that cover as wide a range of annotations as possible (some 5/5 responses, some 4/5, some 3/5, etc). I think we can consider this a little later and just proceed as discussed for now.

##

OK, I think what I need to get now is a parsed corpus, organized in some way that I can easily query it as necessary for GS & processing stuff. I imagine this will be some kind of dictionary (csv) with ResponseID as key and the CONLL parse as value.
1 first line
1 second line
1 third line
1 fourth line

2 first line
2 second line
2 third line
2 fourth line


##
7/09.
I've been working on getting the gold standards parsed & lemmatized. There's a snag in my process, however. I've found a couple of instances where the parser (during the first step -- constituency parsing) and the lemmatizer both incorrectly combine two separate responses. In I02T_all_cns.*, for example, there's a response:
The b
(That's the whole response.) My csv to text script changes this to:
The b.
And from there, the parser tools combine this with the next sentence:
1	the	_	DT	DT	_	2	det	_	_
2	b.	_	NN	NN	_	0	root	_	_
3	get	_	VBG	VBG	_	4	dep	_	_
4	ready	_	JJ	JJ	_	2	partmod	_	_
5	to	_	TO	TO	_	6	aux	_	_
6	take	_	VB	VB	_	4	xcomp	_	_
7	a	_	DT	DT	_	8	det	_	_
8	bite	_	NN	NN	_	6	dobj	_	_
9	of	_	IN	IN	_	0	erased	_	_
10	pizza	_	NN	NN	_	8	prep_of	_	_
11	.	_	.	.	_	2	punct	_	_

I have to find a way to force the parsers to handle these as two separate sentences.
I also think I'll still have a problem with responses that have internal periods or are otherwise segmented into multiple sentences, e.g.:
The happening is a horse show. (You have a typo in this question, perhaps.)
(from I09U_master_anno.csv)

I changed the shell such that it handles each line as a single sentence. However, by my calculation, it will take about 50 hours to run on my laptop. This is not reasonable. The process is really redundant, which wasn't a problem until I had to reload the lemmatizer before each sentence -- this is a major time suck. Because the "all_ns" GS file contains all ns responses, and the other GS files are subsets of this, there's no need to parse these same responses over and over when they appear in different files. It would be more efficient and much faster to simply process the "all_ns" files for each item, then copy the parse info over to the individual subset files ("all_cns", "firsts", etc.) using the ResponseID.

Okay, I ran I01-I10 (20 items) "all_ns" through the pipeline, but numerous files are not processing correctly. In many cases, lots of sentences are missing from the final lemma_conll file. I'm in no frame of mind to sort this out tonight, so I'll get to it first thing tomorrow morning.

###
2018-07-11
Back at the helm, trying to fix the above concern.

OK, I wrote a script to compare the number of responses in each of the 6 files involved in getting from the original GS csv to the final lemma_conll file. Here's what I found:
Non-Matches:
the fields represent:
file_id csvcount txtcount lxcount pcount lkcount lccount
Non-Matches:
##off by one:
I03T_all_ns	157	157	157	157	157	156
I06T_all_ns	154	154	154	154	154	153
I08T_all_ns	145	145	145	145	145	144
##off by more than one:
I02U_all_ns	184	184	184	184	184	124
I04U_all_ns	176	176	176	176	176	43
I05T_all_ns	156	156	156	156	156	42
I08U_all_ns	158	158	158	158	158	39
I09U_all_ns	158	158	158	158	158	26
I10U_all_ns	156	156	156	156	156	53

Matches:
I01T_all_ns	161	161	161	161	161	161
I01U_all_ns	190	190	190	190	190	190
I02T_all_ns	157	157	157	157	157	157
I03U_all_ns	180	180	180	180	180	180
I04T_all_ns	156	156	156	156	156	156
I05U_all_ns	170	170	170	170	170	170
I06U_all_ns	166	166	166	166	166	166
I07T_all_ns	148	148	148	148	148	148
I07U_all_ns	162	162	162	162	162	162
I09T_all_ns	140	140	140	140	140	140
I10T_all_ns	138	138	138	138	138	138

I01T_all_ns
I01U_all_ns
I02T_all_ns
I03U_all_ns
I04T_all_ns
I05U_all_ns
I06U_all_ns
I07T_all_ns
I07U_all_ns
I09T_all_ns
I10T_all_ns

OK, so I can see that in each of the non-matching cases, the first five files all have matching counts, but the lemma_conll file is the odd one out. So there must be a problem in my script that combines the conll file and the lemmaxml file.

###################
SIDE PROBLEM: Somewhere in the process, somethings are not being properly lowercased...
THE BOY IS EATING PIZZA. (txt)
the boy be EATING PIZZA . (lemma_conll)

Pizza is being eaten by a boy. (txt)
Pizza be be eat by a boy . (lemma_conll)
####################

Main problem:
Here's one case where things get out of alignment:
I02U_all_ns:

116	A slice of pizza is about to be eaten by a little boy.
116	a slice of pizza be about to be eat by a little boy .

117	Littler. It eating pizza.
117	it eat pizza .

118	Guy eating pizza.
118	little boy .  ##actually 123

119	The boy is eating pizza.
119	a blonde boy with freckle be eat a slice of pizza . ##actually 131

120	The child is going to bite the pizza.
120	the boy be eat pizza . ##unsure -- many matches; probably 135, 147 or 149

121	the boy eats the pizza.
121	a piece of pizza be be eat by a boy . ##actually 152

122	The pizza is eaten by the boy.
122	that boy be about to eat the pizza . ##162

123	little boy.
123	the child be eat pizza . ##165

124	eating pizza.
124	Pizza make the boy happy . ##174

125	A boy is eating a slice of pizza.
125	EMPTY (lemma_conll)

126	A boy is about to enjoy eating a slice of pizza.
126	EMPTY (lemma_conll)

127	The boy is eating pizza.
127	EMPTY (lemma_conll)

128	The pizza is being eaten by a boy.
128	EMPTY (lemma_conll)
##
#Here's what it looks like in the txt file: (lines 116-128)
A slice of pizza is about to be eaten by a little boy.
Littler. It eating pizza.
Guy eating pizza.
The boy is eating pizza.
The child is going to bite the pizza.
the boy eats the pizza.
The pizza is eaten by the boy.
little boy.
eating pizza.
A boy is eating a slice of pizza.
A boy is about to enjoy eating a slice of pizza.
The boy is eating pizza.
The pizza is being eaten by a boy.

OK, so the text file sentence is always fine. It seems that when the process encounters a response that contains a non-final period, things get wacky. The problem has to be in my lemmatize_conll.py. I'm not sure why it skips so wildly through the remaining responses.

###
2018/07/12.

OK, I made some changes to lemmatize_conll.py, and the results are getting better. Among the all_ns files, only these have problems:
file_id csvcount txtcount lxcount pcount lkcount lccount
Non-Matches:
I03T_all_ns	157	157	157	157	157	156
I06T_all_ns	154	154	154	154	154	153
I08T_all_ns	145	145	145	145	145	144

And here are the problem sentences:
#
137	T.he man is accepting a package.
137	the man be deliver the package .

138	The man is delivering the package.
138	the woman be accept the package delivery .
#
108	This boy is carrying a large sack filled with groceries.See all the fruit in this sack?
108	see all of the fruit in this bag ?

109	See all of the fruit in this bag?
109	the little boy be try to help carry in the grocery .
#
87	T.he waiter is showing the woman a roasted turkey.
87	the waiter be serve the woman a roasted turkey .

88	The waiter is serving the woman a roasted turkey.
88	the waiter be bring the food .
#


And oddly, some sentences like this have some capital letters slipping through:
59	HELPING CARRY GROCERIES IN THE HOUSE.
59	help CARRY GROCERIES in the HOUSE .


####
I wrote find_ugly_responses.py to scan through the master_anno files and identify any sentence that contains a '.', '?', or '!' in a non-final position. Next I will simply manually modify the master_anno files to correct for this. Here are the problematic responses:
I02U_responses_only.txt
Littler. It eating pizza


I03T_responses_only.txt
T.he man is accepting a package


I04U_responses_only.txt
Good Morning Sunshine. It's a beautiful day.
A boy is waking up at 2 p.m.


I05T_responses_only.txt
The teacher is teaching the student..
She's showing them how to do the assignment. .


I06T_responses_only.txt
This boy is carrying a large sack filled with groceries.See all the fruit in this sack?


I08T_responses_only.txt
T.he waiter is showing the woman a roasted turkey


I08U_responses_only.txt
The wait-person is asking her if the roasted chicken meets her approval. And if she is ready for the meal or continue to wait.
She,s thinking, "There goes my diet!"
"Your dinner is served, madam.".


I09U_responses_only.txt
The happening is a horse show. (You have a typo in this question, perhaps.)
she is enjoying the moment!~ the horse is not sure


I10U_responses_only.txt
The baby is not happy about something. He may need a bottle or his diaper changed or just needs Mommies to hold him.
CHANGE MY DIAPER!!!!!!!
FEED ME!!!!!!!


I11T_responses_only.txt
Since the right index finger is pointing upwards, he may be asking the older male person, "Why is the sky blue?" or he may be pointing to a burnt-out light bulb and asking why it hasn't been replaced.


I12T_responses_only.txt
The woman is holding a strangely dead-eyed grin while cutting a jagged line through a piece of paper. She may be hypnotized, possessed, or otherwise not fully in control of her executive functions.


I12U_responses_only.txt
Where's the rock? (Rock, Paper, Scissors)


I14T_responses_only.txt
The man is happily exchanging a set of car keys for what appears to be a single bill of paper currency--maybe $100? It could be for a short term rental.


I14U_responses_only.txt
She just bought a new car. Hurrah for her.
He just sold the lady a new red lemon. Hurrah for him.


I15T_responses_only.txt
the man is taking the young tree out for his dinner. he haven't eat anything for 3 days.


I15U_responses_only.txt
The boy is planting trees right now.,


I16U_responses_only.txt
The frog is trying to catch a bug.The bug is trying to get away from the frog.


I17T_responses_only.txt
A girl is giving a carrot to.a horse.
Carrots make a crunchy noise when feeding them to a horse.!


I17U_responses_only.txt
The person is feeding a carrot to the horse..


I18T_responses_only.txt
a superman is ...?
The .man is hitting himself.


I18U_responses_only.txt
Fist surprise punches Elivis in the mouth! WHAM!
An adult is coughing and covering his mouth. How many more examples do you need to see one ends their sentences with a noun? Enough already.
An adult is coughing and covering his mouth. How many more examples do you need to see one ends their sentences with a noun? Enough already.


I19U_responses_only.txt
The young woman is trying totake a photo of someone not seen. She is asking that they smile.
Hey! Quit taking my picture!


I20U_responses_only.txt
The girl is la.ughing histerically.
???


I22U_responses_only.txt
The lady is showing what the weather forecast is across the U.S.
??


I23U_responses_only.txt
Little Veronica is worried that the flu vaccine that Dr. Anderson is about to give her will hurt.


I25U_responses_only.txt
The dog is chasing the cat who jumped toward the wall and the dog jumped toward the wall. When the dog realised he was "flying" he froze and fell to the ground.
The dog chasing the cat didn't really want to catch the cat because he knew the cat would jist grab him with his claws. He knows just how much it hurts.
Dog and cat seen 1.3 seconds after being launched from animal gun.
Rover is enjoying his favorite pastime, chasing Mrs. Franklin's cat, Barney.


I26U_responses_only.txt
Hey little girl... do you want some candy?


I27U_responses_only.txt
??
Sing. Sing a song. Sing it loud. Sing it long! Etc.
Sing. Sing a song. Sing it loud. Sing it long! Etc.
Margaret is trying out for the school talent show by singing her favorite song, "Stand by Me."


I28T_responses_only.txt
Fixing to be in trouble for not asking for directions before they got to the park. She is frustrated by his lack of planning and she is going to leave him when they get back home for a more responsible person.


I28U_responses_only.txt
The girl needed directions..


I29T_responses_only.txt
She is happy. She holds her dog. She seems talk to this dog.


I29U_responses_only.txt
cindy just get her birthday gift which is a dog. she loves him so much
This kitten obviously has a very loving owner. It would appear that the pet's diet is top of the line.
Yes... they are tasty when they are this small... just be sure to remove ALL the fur.
Yes.. I have given up on men.


I30T_responses_only.txt
Janet set a New Years resolution to get in shape. By Jan 9th, she was back on the sofa with a box of bon-bons.
She's wondering if anyone thinks her pony tail is sexy. (by the way, it is)
She is trying to be someone that the magazine tells her she should be. She would rather stay at home and eat a whole lot of cupcakes.


I30U_responses_only.txt
The runner is training for the upcomming soccer season. Running or jogging are excellent ways to develope endurance strength.
??

#####
2018/07/15
ALMOST there. A few of the all_ns gs files are still off a little:
file_id csvcount txtcount lxcount pcount lkcount lccount
Non-Matches:
I11T_all_ns	139	139	278	139	139	139
I11U_all_ns	154	154	308	154	154	154
I12T_all_ns	138	138	276	138	138	138
I12U_all_ns	152	152	304	152	152	152
I21T_all_ns	168	168	168	168	168	167 ##this seems to be the only *real* mismatch, and it's caused by a ";)" emoticon.

For those where the lxcount is double the other counts, I think that's just because the lemmaxml file wasn't deleted from an earlier run, and that step of the pipeline just appends to the existing file, so it basically wrote the contents twice. I'm going to try to run those again right now.



From I21T:

89	He is throwing the ball.
89	he be throw the ball .

90	He is wishing he could throw her his ball ;).
90	s.

91	s.
91	sad .

92	sad.
92	he be play catch .

DONE. All the all_ns gs files are now correct in terms of sentence counts and lemma-parse alignments.

NEXT: From the all_ns files, add the lemma parses to the other gs files (which are all subsets of all_ns)...
see add_parses_to_gscsv.py
DONE.

##
2018/07/20.
Time to start a new log...
