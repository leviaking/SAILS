2017-11-02
I've had to revisit the Answerhood annotation this week. I realized that in my attempts to automate some of the untargeted annotation based on the targeted annotation, I made some mistakes. I simply reannotated the entire targeted response set under the guidelines for the untargeted responses. Some of this was automated, some was manual. What I should have done instead was this: where appropriate, automatically apply the targeted annotations to any identical responses in the untargeted set, and then manually annotate the remaining responses.

So I now have a set of annotated targeted responses for which it would be appropriate to apply the annotation to any untargeted duplicates. My current task is to write a short script to do the string matching and autoannote the duplicates. I'm calling this applyTargetedAnnoToUntargeted.py.

KitsuneMBP:answerhood_working leviking$ for UF in $(ls *.csv); do cp $UF ${UF/.csv/TOTALLYTEMP.csv} ; rm $UF ; perl -pe 's/\r\n|\n|\r/\r\n/g' ${UF/.csv/TOTALLYTEMP.csv} > $UF ; done


2017-11-06. Meeting with Markus.

Accomplished since last week:
Re-annotated the untargeted Answerhood responses after a mix-up.
Annotated targeted Core Event.
Read Turner (2000)


Discuss Turner (2000) ("Rater Voices").
Developing rating scales: (See also Turner & Upshur (1996))
0. Consensus on testing construct
1. Divide sample into two halves -- UPPER, LOWER.
2. Discuss, revise, consensus
3. Find (binary) characteristics that distinguish upper & lower


November/December timeline:
Complete untargeted Core Event annotation (11/8 Wed)
Re-annotate all "maybe" responses (11/20 Mon)
Write Task/Participants/Data sections (11/24 Fri)

Read Turner & Upshur (1996) [PAYWALLED; find offline?], Chalhoub-Deville (1997)
Establish holistic rating scale [performance-based rubrics, such as empirically derived, binary choice, boundary definition (EBB) scales]
	3 item sample
		Split into Upper & Lower; Examine/discuss (11/10 Fri) [Ask Sabrina to help here]
		Split Upper (into L3 & L4?); Examine/discuss (11/24 Fri)
		Split Lower (into L1 & L2?); Examine/discuss (11/24 Fri)
	Write up holistic rating guidelines (11/30 Fri)
		
Implement holistic rating scale for all responses (12/14 Thurs)
Collect binary and holistic ratings (samples) from Annotator 2 (12/7 Thurs)
Apply *type* annotations to all response *tokens* (12/18 Mon)
Calculate inter-rater agreement (12/20 Wed)



RE: Answerhood "maybe": How should I handle "going to/getting ready to/about to/fixing to" <verb>?

NTS: data cleanup: "the happening is a horse show" (I09U); "the image did not load" (and similar throughout); "how many more examples do you need to see one ends their sentences with a noun? enough already" (I18U);

#####################################################
NOTES FOR 11/21 MEETING:

NTS: Could I use NER to downweight responses that use names/proper nouns? There are very few legitimate appearances of named entities in the responses. [exceptions: "riding English style", "Australian crawl", maybe others]

Could we get away with a simple yes/no holistic scale?? That would be AWESOME!

Can I work some kind of machine learning in here?

MD says: Using GS data, can we predict something about NNS data? Compare to grammatical error detection...
	Responses with 5/5 binary features ("yes") or 0/0 ("no") are clear 1 and 0 in holistic scale. Examine the 4/5 and 1/5 to get an idea of how to approach holistic annotation. [Pin down the construct by looking at the existing binary feature annotations.]


#####################################################

Re: Annotation and holistic modeling and scoring

Markus,

I just had a long talk with Xuefu about my work. I wanted to write it up while it's fresh in my mind. We discussed with two possible approaches for moving forward.

Approach 1:
I would set aside the completed binary feature annotation for now.
I would complete the holistic rating via the EBB process. (Whether this results in a 2 or 4 point scale is up for debate.)
The binary features may or may not appear to be salient in the resulting holistic ratings, which means that their main role in the dissertation would simply be to provide observation and discussion about the data and report on these features in a SLS/Linguistics oriented chapter.
The CL work would proceed more or less as expected, where the dependency-based system predicts holistic scores for responses.

Approach 2:
I would derive holistic scores from the (weighted) feature annotations. (Again, the scale itself is TBD.) This would be done with a small(?) sample and some kind of cross-validation would be used. Part of the argument here is that this kind of feature annotation is expensive, so we are exploring an approach that could be repeated with less expense. Future test/game/etc. developers could do feature annotation for a sample of responses and use that to derive a model or models which could then complete the holistic annotation.
Again, the CL work would be the same -- use the dependency-based system to predict the holistic scores.

Xuefu likes Approach 2. I don't dislike it, but I think I'd need to sell it a little and try to justify it somehow with literature, etc. I would basically need to say, "I examined the data, did some preliminary annotation, and decided that these features have the most bearing on the 'goodness' of a response. [All of which is true, up to this point.] I also used my judgment and intuitions from this process to assign weights to the features in order to derive a holistic score." This probably makes the most use of the work completed so far, where I attempted the holistic annotation, then decided it couldn't be easily done without somehow considering the features we arrived at.
In Xuefu's words, "We're providing a framework for developing an automatic rating system. We're not claiming that relying on these five features is absolutely the best approach, but we're examining the feasability of such a framework (i.e., deriving holistic scores based on a sample of linguistic feature annotations and approximating those scores with an automated system.)"

Prior to my meeting with you today, I was staring at the data and wondering how I can make the holistic judgments. It felt like I was back at square one, before I started the feature annotation. It's difficult and feels arbitrary, and this makes me think it would result in very low inter-annotator agreement. I need to either:
A) have a well-defined testing construct and clear guidelines OR,
B) as EBB handles this, simply divide the data into two halves by quality (which of course still requires a clear testing construct) and then determine what criteria separate the halves, and repeat the dividing process as necessary.
I think Approach 2 here largely avoids those problems.

I'll be thinking about this more in the coming days as I complete the feature annotation and tidy up the data. I'm interested to hear your thoughts too, now that I've better articulated the concerns. But it's Thanksgiving Break, so enjoy yourself and reply whenever it's convenient -- I have plenty to work on in the meantime.

Regards,
Levi

#####################################################

NTS: 11/21: I'm only finding the untargeted re-merged answerhood files. Troubleshoot this and find or generate the targeted files.

11/22: I found the targeted files, but they need to be sorted back to the original order -- use another feature file to do this. I'm writing a quick script to sort one csv according to another. See '.../responses/2039/sort_csvX_with_csvY.py'

OK, this problem is solved. The targeted answerhood files have been sorted with the python script.

#####################################################

Binary Feature Annotation TO DO [11/25]:
[DONE] 1. Finish remaining Verifiability items
1.b. Revisit Answerhood for responses that show immiment action: "about to/going to/getting ready to/etc."
2. Finish remaining Core Event items
	2.b. For items 1-16 (T&U), I need to revisit and ensure that no subject/object in the core event is referred to by a proper name. I updated this in the guidelines and annotated accordingly from 17 onward.
	2.c. There seems to be no clear instruction for how to handle responses that give a subject (or object?) with the incorrect number ("the women is buying a car", etc.) [As of I19U, I have updated this to say that a response that contains a subject or object with the incorrect number (singular/plural) as a part of the core event should be annotated "no" for core event. I'll need to do some clean up in I01-I19T... maybe just some ctrl+f for a few variations on the subjects and objects... ]
3. Finish second pass (annotate 'maybe' responses)
	* Write script to handle second pass annotation; it should preserve the first pass annotations! Because we may want to know later which responses were difficult to annotate.
4. Write script to reapply type annotations to tokens.
5. Clean up! Remove "The image did not load", etc.

#####################################################

NEXT MEETING [for MD meeting, 12/4]:

11/25: So, something just occurred to me, and I think it might be kind of important. If the holistic score is a function of weighted features, then we probably wouldn't simply use one holistic model, or one model per point on the holistic score... Instead, we'd have two models ('yes' & 'no') per feature... so a test response could be compared with each of these to determine its feature scores, and these could be multiplied by the feature weights to arrive at a holistic score. I'm not completely clear on how to make this work, but I think this deserves further consideration.

11/27: I think I've got the holistic scale figured out!
3: Perfect response (All binary features = 1)
2: Passing response (Core Event = 1, but one or more other binary features = 0)
1: Non-passing response (Core Event = 0)

Idea: Consider targeted responses. Some targeted responses are very short, consisting of only a verb phrase (e.g., "eating pizza"). Any such very short response that is annotated as having the core event is extra valuable, because we know there isn't any fluff here -- these dependencies are the essence of the item.* On the other end of the spectrum, we have long, rambling responses, and many of these are also annotated as having the core event. In the pilot study (if you will), we tossed all the gold standard dependencies into the same bag. I'm sure you see where I'm going with this -- I think each dependency in a GS (or whatever kind of dependency-based model we end up with) should be weighted more thoughtfully. A single response should have a weight of 1, and each dependency it that response should have a weight of 1/D, where D is the total number of dependencies in the response.
*(I may even want to think about implementing a check to see if the subject was dropped, and then automatically adding it back in so that the subject dependency is represented in the models. But this is a side thought... )

######################################################

From above:
	2.b. For items 1-16 (T&U), I need to revisit and ensure that no subject/object in the core event is referred to by a proper name. I updated this in the guidelines and annotated accordingly from 17 onward.
	2.c. There seems to be no clear instruction for how to handle responses that give a subject (or object?) with the incorrect number ("the women is buying a car", etc.) [As of I19U, I have updated this to say that a response that contains a subject or object with the incorrect number (singular/plural) as a part of the core event should be annotated "no" for core event. I'll need to do some clean up in I01-I19T... maybe just some ctrl+f for a few variations on the subjects and objects... ]
Regarding all this... I think I need to take a break and reflect on exactly what core event is, what it does for the process and how it relates to the philosophy of wthis project... Do we want to allow for proper names? Wrong pronouns? Plurals that should be singular? Shouldn't these problems be reflected through other features, like Grammaticality? [I think so...] Part of my concern is with the idea of a Gold Standard -- do "The women is taking a picture" and "The girl is using a camara" make it into a GS? I think the answer is 'no', because GS responses should fulfill all five features, and these will fail Grammaticality.

######################################################

12/4 (prior to meeting today):
I need to get a few example items ready... Let's use items 1-3. Double check for any 'maybe' annotations. I'm going to edit the individual feature files before compiling them. Then I'll sort them into the 3 point scale.

######################################################
12/4 Meeting with MD:
Core Event:
cf LFG: coherence, consistency, completeness
Maybe the criteria for Core Event is: it cannot be in conflict with the context. So a named subject (of correct gender) is ok. A pronoun of the incorrect gender is not okay. A subject that is plural but should be singular is okay -- consider lemmas here.
Avoid overlap with verifiability, grammaticality, etc. -- a response should be penalized for a given offense only once, whenever possible.

######################################################

12/5

Not really in the mood to wade back into the core event annotation and clean up. One thing I can do right now is this: The grammaticality responses (types) were combined into one file (containing targeted and untargeted) and then annotated. I need separate files -- 1 for targeted, 1 for untargeted. I need a script to do this.

I think it worked... check after returning from trivia!
Yes, it did work. The separated grammaticality files are now in the 'Gramm' folder, in the same configuration as the other feature files.

######################################################
12/13

I'm back in the core annotation now.

######################################################
12/19

Core event guidelines have been mostly updated. Check against the notes from previous meeting.
Core event annotation is finished, but some early items may need double checking. [WAIT -- A few items were skipped... revisit this.]
I have started a script for compiling all the annotation layers into one spreadsheet, but this is not yet working.
###
Meeting with MD:

We're aiming for Feb or March for a paper submission date for a paper about the dataset. (BEA or ACL)
Look at LREC and LAW proceedings for examples of papers based on dataset (and annotation schemes) releases.

Do over break:
Revisit timeline; what's happening in the spring; try to hit the ground running in January;
Continue working on the final compilation script and try to get the processing pipeline set up to read from the final compiled version of the data.

###

2018-01-09 Meeting with MD:

To do:
By Friday 1/12:
[DONE] Re-examine Core Event annotation for Items 1-18 (targeted and untargeted); Per updated guidelines, review all "no" annotations. Some "no" annotations will need to be changed to "yes".
[DONE] Ask Annotator 2 for some written feedback about the annotation scheme and the process. What's difficult, unclear, etc.?
[DONE/NA] Conference with Annotator 2 to compare annotations. Try to reach consensus; ask Annotator 2 to re-annotate if necessary. 
Compile all annotation layers from all annotators into a single CSV file; (response types).
Apply the annotations from the compiled response types file to the original response tokens file. Be careful as some minor changes were made in condensing tokens to types (punctuation stripped, etc.).
Begin data writeup / BEA paper (submissions due 3/20) or ACL paper (2/22).
Email committee update

Per MD:
Email out a brief update to committee, including this timeline (actually, put completed tasks back in the timeline to show progress).


2018-01-20.

Working with Annotator 2 today.

I have rechecked my "no" annotations for Core Event for items 1-3. [items 4-18 also need to be rechecked for "no" annotations (see notes above) but now the priority is working with items 1-3, because Annotator 2 has also completed items 1-3.]

Items 1-3 Core Event have now been rechecked for "maybe" annotations; these are in the 2039/sequestered_maybe/Core folder, where they are the "secondpassanno" files.

I will now complete the rechecking of items 1-3 for the remaining features. These will also be in 2039/sequestered_maybe/<FEATURE>.

Items 1-3, "maybes" rechecked:
	Core
	Answer
	Gramm
	Interp
	Verif


Answerhood: Guidelines must be updated (Verb Forms section?) to say that non-dynamic sentences that indicate imminent action should also be accepted here. e.g., "The boy is *about to* eat pizza"




### Feedback from Annotator 2
difficult to remember the many criteria for the different features; especially when working on more than one feature in a short time.
Grammaticality: grammar is more subjective and varied than we normally think.
Interpretability: many respondents assume that the reader knows details or even arguments (but we judge this without considering the image); slang is often a toss up and judgement call
Core event: For most items, it was very clear from the image what core event the item was intended to elicit. Slightly less so for items with more than one interpretation -- is the man delivering a package or picking one up?
Verifiability: These were mostly easy to determine, but embellishments sometimes pose a challenge. "Lunch"/"a snack" or "pepperoni"/etc. were common but not verifiable.
Answerhood: Some respondents didn't take the task seriously at all. The items were very answerable, but some people didn't try. It's easy to overlook minor changes to the form of the targeted subject, but it's not a problem if you keep this in mind. There is also some gray area between active verbs, descriptions of states, and states that indicate imminent action.

####
1/22 Meeting with MD:

We discussed agreement scores.
MD: Next step, for writing paper, would be to examine individual cases of disagreement.
Why does Core Event break the general trend of targeted having higher agreement than untargeted?
Calculate kappa scores (this accounts for chance, which is high because this is a binary choice, and because some items are biased toward 'yes' annotations, etc.). There may be better measures for this task; see the paper Markus sent


Hypothetical / probable Gold Standard versions:
All Familar NS
All Crowdsourced NS
All perfect annotations from NS ("Oracle"?)
All "yes" Core Event responses


### Back to cleaning up Core #4-18 "no" annotations.
DONE!
ALL Core annotations are now in line with the most recent updates to the guidelines.


Now it is time to complete the re-annotation of any outstanding "maybe" annotations. (I did an earlier, manual pass at this, but left many undecided...)
I will begin with the Core annotations for items 4-30 (Items 1-3 are completed).
DONE!
The Core folder inside the sequestered_maybe folder now contains "firstpass" versions (containing maybes) and "secondpass" versions, which do not contain maybes.

KitsuneMBP:2039 leviking$
diff sequestered_maybe/Core/firstpassanno_I03T_Core-2039.csv sequestered_no/Core/firstpassanno_I03T_Core-2039.csv 


####
FIRST PASS annotations are now all in the "firstpass" folder and its feature subfolders.

The 'sequestered_no' folder has been deleted. The (Core) files we needed to generate there were renamed and moved to the 'firstpass' folder, as necessary. 

We should now be working only in the 'sequestered_maybe' folder. (Core is complete -- the firstpass files there reflect any rechecked 'no' annotations, and the secondpass files there do not have 'maybe' annotations). The 'sequestered_maybe' folder should be used to generate secondpass annotation files. We will then move those files to a "secondpass" folder, or something like that.



OK, I realized there was a problem/inconsistency with the answerhood annotations. Markus and I decided to accept responses that indicate "imminent action" (The boy is about to eat the pizza) whether they use a progressive verb or not. I had not yet updated the guidelines [DONE now] or the annotations themselves.
DONE -- all the relevant response annotations have been updated re: answerhood & "imminent action". The effected files have been copied back into the "firstpass" folder. In other words, this issue is closed! Move on to the reannotation of "maybe" responses.

###
ANNOTATING REMAINING "MAYBE" RESPONSES:
DONE. Core Event
DONE. Answerhood
DONE. Verifiability
DONE. Grammaticality
DONE. Interpretability

###
Fri Feb  2 16:26:51 EST 2018
The second pass annotation is 100% complete. There are no "maybe" annotations in the secondpass* files.
Now I need to get the type annotations applied back onto the token files, so we can calculate kappa.

#############
KAPPA scores
OK, everything above is resolved (I think). I now have Cohen's Kappa scores for annotation agreement. See the file agreement_stats_output.xlsx.
Now my task is to look at cases of agreement and disagreement and see if I can find any patterns.
Then I should be ready to write the paper/chapter.


####
Re: interp disagreements:
"enjoy" & "like" "celebrate" -- A2 does not find these interpretable
Previous disagreement stemmed from A2 being more strict with regard to gender of subjects and objects (human). We since agreed that gender is an exception to the rule about use of vague subjects/objects like "someone". I'll need to specify this more clearly in the guidelines. If the gender of a particular figure in the image is not really important or relevant to the event, i.e., if picturing a person of any gender in that position is possible, a subject or object that does not specify gender should be okay. The current rule about vague subjects/objects was really to discriminate against responses like "a man is doing something for someone".
In discussion of disagreements in item 3, A2 states that she previously accepted some telic verbs (not sure how to say this, but ~"receptive" verbs?) like "accept" and "receive" without an indirect object ("from the man"). Upon discussion of disagreements, A2 concludes that such verbs cannot be interpretable without a specified indirect object; the telicity of the event demands that the indirect object, who takes a direct role in the event, must be present. A2 notes that this is in contrast to atelic or repeated actions ("delivering a package" or "delivering packages"), because while these are likely to involve an indirect object, the indirect object is not strictly necessary.
[I think the above distinction is a function of thematic roles more than telicity. When the subject has a thematic role of Recipient, the indirect object (agent) must be present. When the subject is the agent, the indirect object may not be necessary, depending more on the required verb arguments (and telicity might have some bearing on this.)]
A2 states that she was too strict in assessing slang; following reconciliation, A2 re-annotated many slang responses as "yes" for interp.
A2 convinced A1 that minor preposition errors should be overlooked here. "the boy is eating for pizza".

Re: Verif disagreements:
A2 concedes that time of day is generally not verifiable (unless explicitly demonstrated in the image). Thus eating something cannot be considered "lunch" without more information.
A2 previously rejected some responses because the role of a person in the image was specified, e.g., "secretary", "receptionist". We consulted and agreed that such labels are acceptable.

Re: Answerhood disagreements:
A2 previously accepted numerous responses that changed the form of the subject. A2 concedes these should be rejected.
A2 seemed to overlook the requirement for answerhood that responses take the progressive form; A2 accepted many simple present responses. A2 agreed these should be rejected.


Re: Core event disagreements:
Some disagreements simply stem from a different interpretation of the image and its core event. In some cases, annotators do not agree on how specific the core event should be, or whether to allow particular slang responses.

Re: Grammaticality diagreements:
In most cases, annotators disagree over whether a minor grammar issue is grounds for rejecting the response.
A2 allowed some responses with bare nouns; A2 concedes that (per guidelines), these should be rejected.
A2 allowed dropped subjects in the untargeted responses. A2 agrees these should be rejected.

###
2/12/2018
I have a few files of disagreement responses where A2 changed some annotations upon discussion. On these disagreement files, I need to see if there are any of my own annotations that I'd like to change. [DONE]
Then I need to apply the new A1 and A2 annotations to the responses and recalculate the agreement/kappa scores. [DONE]
In the meantime, A2 is annotating a new sample of items (following the discussions). I'll need to reannotate this sample as well (blindly, without seeing A2's annotations). Then I'll calculate the agreement scores for the new sample, and these will serve as the "official" agreement scores.

2/13/2018.
Currently assembling the second sample of annotations (items 28-30)


##Note:
We need to calculate agreement scores for another dimension: intransitive/transitive/ditransitive.

2/18/2018.
A2 has annotated the test sample (items 28-30).
To do:
Separate A2's test sample grammaticality annotations (from TU file --> T & U files). [DONE]
Apply A2's (type) annotations to the token master files. [DONE]
Calculate agreement/kappa for the test sample. [DONE]
(Start writing the corpus paper.)


The test agreement scores are in and look about right to me. I need to move these from the "test_sample" folder on the desktop back into the main project folders and tidy up a bit. 

2/19/2018 meeting with MD.

Why did adjudication boost the Interp score so much?

From dev to test, why did some features improve and why did the others decline?
(Why is Core lower now?)

####
Explaining raw and kappa scores by feature:

Core: Core is lower than expected based on dev set adjudicated scores. For all test items, core has a lower chance agreement than other features (less skewed). Core has the lowest rate of "yes" annotations among all features -- this feature seems to be stricter than others.
	The ditransitive ("a man is giving directions to a woman") item drags down the average Core kappa. This was a difficult item to define by core event.
	Key disagreements: A2 accepts "pet"; for "giving directions", A1 sees the individuals as strangers having a brief interaction, A2 more often accepts answers that describe the figures as a pair engaged in some activity ("reading a map") together; A1 requires an "asymmetry" to the transaction -- man is pointing or relaying some directional information, but A2 is more likely to accept responses that less specifically indicate giving directions. Re: "jogging", both annotators show some inconsistency with regard to responses that only indicate "exercising" and not some form of running.

Answer: Highest kappa & raw agreement scores among the 5 features. Test scores are between initial dev scores and adjudicated test scores, as expected. Chance agreement is 72.11%, second highest after Gramm (76.82%).
	Key disagreements: Few disagreements, but most seem to stem from fatigue or error. From both annotators, there is some inconsistent application of the rule that targeted response subject is exactly the same as the subject in the question. A few disagreements involve responses where it is unclear if the the response refers to the specific ongoing action in the image or simply the action in general: e.g., "running is this woman's chosen activity".

Gramm: Scores --> No surprises, really. k=82.6, but raw agreement is much higher. Gramm tends to be more heavily skewed (toward 'yes') than any other feature.
	Disagreements: Fatigue is an issue -- some disagreements stem from an obviously incorrect annotation from one annotator. Inconsistent application of the "no bare nouns" rule. Some responses simply are ambiguous re: gramm.

Interp: A little lower than expected. Lowest kappa scores among the 5 features. Annotators note that this feature is the most difficult to judge consistently.
	Disagreements: For the 3 items in the sample, more than half of the disagreements come from the ditransitive. Many of these disagreements involve ditransitives that omit the indirect object: "he is indicating the way", "the man is giving direction" -- both annotators show some inconsistency here. There seems to be disagreement over the level of allowable inference: A1 allows some general statements like "the woman loves her dog", "jogging is one of her hobbies" and similar -- does this response equate to a visualizable action, or should it be taken as an abstraction? (A2 accepts no such general responses.)

Verif: No surprises re Kappa.
	Disagreements: A small number of errors/fatigue -- inconsistent annotation of "cat" and "pet" responses. Most disagreements, however, stem from judging which inferences are acceptable and which are not: "the man is showing the hiker points of interest on the map" and "the woman is cuddling her dog after coming home"
	

### 03/07/2018
Meeting with MD.

Me: Write Data collection section.

MD: 3.2 Agreement, 1 Intro

LK: 2 (Data collection), 3.1 Annotation Scheme, 4, examples for table 1, Help with citations throughout.



3/13/2018:
Current priority:
Modify agreement script to report NS and NNS separately. (Also, separate Familiar and Crowdsourced NS if possible; this won't likely be reported in current BEA paper, so it can wait.)



####
How am I getting EXACTLY the same number of response tokens for 28, 29 and 30?? (431 for each)

It's legit. All three targeted files have 226 response tokens, and all three untargeted files have 205 response tokens. I used an excel formula on each file to count the non-zero cells in the response column, and that's what I got.

MEETING:
for TTR table:
Try calculating the scores without the random sampling part -- take the whole set.

Today: MD: S 1 & 2, 3, abstract

I: move tables, change examples where appropriate; write discussion; recalculate TTR for entire sets (not samples) and if similar, report this and simplify the relevant paragraphs; add Joel citation in S4 per MD's marginpar.


####
For Friday, 3/23:
Finish participant_counter.py




Prepare for April 5 video interview with Google:

Review my own past publications (process, delegation of tasks, my role, results, etc.)

Review linguistics:
	POS tagging
	Constituency parsing
	Dependency parsing
	Semantic roles / theta roles
	What is:
		LFG
		HPSG
		CCG
	Jurafsky & Martin: Speech sections (Ch. 7 - 10)
	
Review *speech* specific linguistics:
	IPA
	Phonetics
	Phonology
	Disfluency & other speech features
	
Review tools for speech processing:
	transcription
	ELAN
	forced alignment
	PRAAT


###
04/09/2018
To do:
*blog
*job search
*apply for Wells Fargo job
*revise BEA paper
*BEGIN expanding paper into chapters


BEA Revisions:
MD: I think we can now more directly (& non-anonymously) compare to our previous work/resource, as well the ETS work.  Also, Detmar Meurers' group did some work on task-based corpora, and I feel like we're approaching that a bit.

