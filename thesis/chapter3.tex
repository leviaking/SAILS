\chapter{Pilot Study}
\label{chap:pilot}

\section{Introduction}
As discussed in Chapter~\ref{chap:intro}, this dissertation grew from a desire to introduce lightweight, transparent methods of content analysis for contexts involving non-native speakers (NNSs) and second language instruction or assessment. In this chapter, I detail my initial foray into this work, where I conducted experiments to uncover the challenges involved and determine the feasibility of my goals. In that sense, the work described in this chapter can be seen as a pilot study and a proof-of-concept for the expanded study described in the chapters that follow.

In Section~\ref{sec:pilot-study}, I discuss a set of experiments that relied on custom rules to extract important elements from responses in order to match NNS responses to native speaker (NS) responses for assessment. In Section~\ref{sec:2016work}, I lay out a more data-driven set of experiments and the findings there that led directly to the collection and annotation of a much more robust corpus (see Chapters~\ref{chap:data} and~\ref{chap:annotation}) and a deeper investigation into the impact of several variables on system performance (see Chapters~\ref{chap:method} and~\ref{chap:optimization}).

%
%In this chapter, I explain my system for rating and ranking responses automatically, where the goal is to approximate the benchmark rankings described in Chapter~\ref{chap:annotation}. 
%The data-driven method used to analyze picture description task (PDT) responses throughout this dissertation represents an evolution from my own previous rule-based methods. In this chapter, I summarize my earlier work and the lessons I learned there, then lay out the current approach.

%In short, my first approach was heavily rule-based and relied on strict matching with a pre-established set of acceptable responses. This found moderate success, leading to the improved current approach, which is data-driven and relies on more flexible methods of comparison. 

%In short, my earlier approach assessed each non-native speaker (NNS) response by extracting a \textit{verb(subject,object)} triple and looking for a match among triples from the native speaker (NS) responses. This involved dependency parsing the sentence then applying custom rules based on the labels, relationships and parts of speech in order to find each element of the triple. This process found moderate success, correctly assessing roughly half of NNS responses with a very small number of NS responses. Considerable weaknesses emerged, however; the rule based approach meant that it was limited in its ability to handle variation, and the use of simple triples was a hacky simplification of meaning. This lead me to the current approach, which uses a more robust representation of meaning and replaces the rule based matching with measures of semantic similarity. These changes make for a more generalizable approach.

%%%% BEGIN material from Qual Paper (BEA 2013) %%%%
\section{Rule-based study}
\label{sec:pilot-study}
This study focused on analyzing English NNS responses to a picture description task (PDT) by comparison with NS responses. It was largely intended to determine if such a task would be feasible for a single researcher using off-the-shelf tools. This meant identifying problem areas and gauging whether variation in the form and content of responses could be manageable.

This section summarizes relevant work first presented in \citet{king:dickinson:13} and \citet{king:dickinson:14}; please see those papers for deeper discussions. However, please note that some experiments included in those papers are omitted here, and thus some results involving the averaging or ranking of scores differ slightly here.

\subsection{Data}
\label{sec:pilot-data}
My work is inspired by various intelligent computer-assisted language learning (ICALL) projects and their reliance on visual stimuli\lk{XYZ: cite}. ICALL itself is heavily influenced by video games, which tend to be visual and interactive\lk{XYZ: cite}. Thus, a dataset of responses to visual prompts was needed to develop the kind of content analysis pipeline I envisioned.

Without an appropriate dataset available, I opted to collect my own using a PDT. Linguistic research often relies on the ability of task design to induce particular behavior \citep{skehan1998assessing}, and a PDT can be used to induce behavior similar to that of an ICALL application.  Moreover, the use of the PDT as a reliable language research tool is well-established in areas of study ranging from language acquisition to Alzheimer's disease \citep{ellis2000task,forbes2005detecting}. Crucially, a PDT constrains the range of potential responses to contain only information depicted in the image, which greatly reduces the kind of processing needed to ``understand'' and assess responses with an automated system. Relatedly, particularly simple images should restrict elicited responses to a an even tighter range of expected contents.

I designed a data collection instrument consisting of a brief background questionnaire and 10 PDT items (eight line drawings and two photographs). Examples are given in Figure~\ref{fig:example-picture}. Each item was intended to elicit a single sentence and consisted of an image depicting an action canonically framed as transitive event in English, along with the question, ``What is happening?'' The instructions simply asked participants to ``view the image and describe the action in either past or present tense'' alone and without the use of any resources. The task was administered in a computer lab, where participants typed their own responses. Automatic spelling correction was intentionally disabled on the machines.


\begin{figure}[htb!]
%[width=0.8\columnwidth]
\begin{center}
\begin{tabular}{|c||c|}
\hline
\includegraphics[width=0.40\columnwidth]{figures/exampleprompt.jpg} & \includegraphics[width=0.40\columnwidth]{figures/exampleprompt2.jpg}\\
\hline
\textbf{Response (L1)} & \textbf{Response (L1)} \\
\hline
He is droning his wife pitcher. (Ar) & The man killing the beard. (Ar) \\
\hline
The artist is drawing a pretty women. (Ch) & A man is shutting a bird. (Ch) \\
\hline
The artist is painting a portrait of a lady. (En) & A man is shooting a bird. (En) \\
\hline
The painter is painting a woman's paint. (Sp) & The man shouted the bird. (Sp)\\
\hline
\end{tabular}
\end{center}
\caption{Example items from the previous study showing responses from native speakers of Arabic (Ar), Chinese (Ch), English (En) and Spanish (Sp).}
\label{fig:example-picture}
\end{figure}

I collected responses from 53 participants for a total of 530 sentences. There were 14 NSs (non-linguistics undergraduate and graduate students) and 39 NNSs (university students enrolled in intensive English as a Second Language courses at Indiana University). The distribution of NNSs' first languages (L1s) was: 16 Arabic, 7 Chinese, 2 Japanese, 4 Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish.

Once the data had been collected, I removed a small number of low quality responses from the NS set and annotated the NNS responses for appropriateness, with respect to the content of the picture. My annotation guidelines for this labeling were minimal: \textit{Given the prompt, would the response be acceptable to most English speakers?} This simplistic approach to annotation was admittedly a weak point in the study, and in subsequent work I sought to correct it (see Chapter~\ref{chap:annotation}). The resulting corpus was used in the experiments described next.


\subsection{Rule-based method}
\label{sec:rule-method}

The processing behind my work was inspired by research from areas such as sentiment analysis, topic modeling, and content assessment that used rule-based approaches to extract important elements from dependency-parsed text \citep{nastase2006,bailey:meurers:08,dicaro2013}. Because dependency parsing focuses on identifying dependency relations, rather than constituents or phrase structure, it clearly labels the subject, verb and object of a sentence, which can then map to a semantic form \citep{Kuebler.McDonald.Nivre-09}. In this study, I took a na\"ive approach in which subject, verb and object were considered sufficient for deciding whether or not a response accurately describes the visual prompt. My idea was to extract and lemmatize a \textit{verb(subj,obj)} semantic triple from the dependency parse for each sentence. Each NNS triple could be compared against the list of NS triples for a match; a NNS response with a NS triple match would be labeled automatically as ``correct,'' while a non-match would be labeled ``incorrect.''

For lemmatization, I used the pre-trained tool in the Stanford Core NLP package \citep{stanford-corenlp-2014}.\footnote{\url{https://stanfordnlp.github.io/CoreNLP/}} The purpose of lemmatization is to minimize data sparsity by reducing the number of word forms. For example, the main verb in the forms \textit{kicks}, \textit{kicked}, \textit{has kicked} and \textit{is kicking} was reduced to \textit{kick} in all cases, increasing the likelihood of finding matches. For parsing, I used the Stanford Parser, trained on the Penn Treebank \citep{demarneffe:ea:06, klein:manning:03}.\footnote{\url{http://nlp.stanford.edu/software/lex-parser.shtml}}


Using the parser's options, I set the output to be Stanford typed dependencies, a set of labels for dependency relations. The Stanford parser has a variety of options for the specific ouput, e.g., how one wishes to treat prepositions \citep{defmarneffe:manning:12}.  I used the parser's default settings, but added two non-default options (\textit{CCPropagatedDependencies} and \textit{CCprocessed}\footnote{\url{http://nlp.stanford.edu/software/dependencies_manual.pdf}}) in order to: 1) omit prepositions and conjunctions as heads and dependents and instead add such words to the dependency label between content words; and 2) propagate relations across conjunctions.  These decisions are important to consider for any semantically-informed processing of NNS language.

To see the impetus for removing prepositions, consider the learner
example in Figure~\ref{fig:prep-dependency}, where the preposition \textit{with} is
relatively unimportant to collecting the meaning.  Additionally,
learners often omit, insert, or otherwise use the wrong preposition
\citep{chodorow:et:al:07}.  The default parser would present a
\textit{prep} relation between \textit{played} and \textit{with},
obscuring what the object is; with the options set as above, however,
the dependency representation folds the preposition into the label
(\textit{prep\_with}), instead of keeping it in the parsed string, as
shown in Figure~\ref{fig:prep-dependency}. Importantly, this option results in a direct relationship between the verb \textit{played} and the object \textit{ball}.

\begin{figure}[htb!]
\begin{center}
    \begin{dependency}[arc edge,text only label,label style={above}]
    \begin{deptext}[column sep=.5em]
      \textit{root} \& The \&[1em] boy \&[1em] played \& with \& a \&[1em] ball \\
    \end{deptext}
    \depedge{4}{3}{nsubj}
    \depedge[arc angle=90]{1}{4}{root}
    \depedge{4}{7}{prep\_with}
%    \depedge[arc angle=35,edge style={dotted}]{7}{6}{det}
%    \depedge[edge style={dotted}]{3}{2}{det}
    \depedge[arc angle=35]{7}{6}{det}
    \depedge{3}{2}{det}
  \end{dependency}
\end{center}
\caption{Dependency parse showing collapsed preposition dependencies.}
\label{fig:prep-dependency}
\end{figure}

%\lk{it may be worthwhile to add the conll parse for this example so it's clear how these graphs come about}
This is a lenient approach to prepositions, as prepositions
are not without semantic meaning---e.g., \textit{the boy played in a
  ball} means something quite different from the \textit{with} example.  However, this option makes it moderately easier to compare the meaning to an expected semantic form (e.g., \textit{play(boy,ball)}).

As for propagating relations across conjunctions, this option is less important as conjunctions are rare in the data, but it is advantageous as it simplifies the representation and makes it easier to connect verbs and their arguments, as needed for the semantic form used in comparisons. For a conjunction like \textit{cats and dogs}, for example, the default settings would produce \textit{cc(cats, and)} and \textit{conj(cats, dogs)}, but the chosen settings would collapse this into \textit{conj\_and(cats, dogs)}, omitting the dependency that merely labels a conjunction relation between the first conjunct and the conjunction.

%Given the rule-based approach to matching \textit{verb(subject,object)} triples, many dependency relations are irrelevant for the next step of obtaining a semantic form.  For example, in this work I ignored determiner (\texttt{det}) relations between a noun and its determiner, allowing for variability in how a learner produces noun phrases. 

\subsection{Semantic triple representation}
%\label{sec:semantic-form}
%
%\subsubsection{Sentence types}

\begin{table*}[htb!]
\begin{center}
\begin{tabular}{|c|l|l|r|r|}
\hline
Type & Description & Example & NS & NNS \\
\hline
 A & Simple declar. trans. & The boy is kicking the ball. & 117 & 286 \\
 \hline
 B & Simple + preposition & The boy played with a ball. & 5 & 23 \\
 \hline
 C & No tensed verb & Girl driving bicycle. & 10 & 44 \\
 \hline
 D & No tensed verb + prep & Boy playing with a ball. & 0 & 1 \\
 \hline
 E & Intransitive (No object) & A woman is cycling. & 2 & 21 \\
 \hline
 F1 & Passive & An apple is being cut. & 4 & 2 \\
 \hline
 F2 & Passive with agent & A bird is shot by a man. & 0 & 6 \\
 \hline
 Ax & Existential A or C & There is a boy kicking a ball. & 0 & 0 \\
 \hline
 Bx & Existential B  or D & There was a boy playing with a ball. & 0 & 0 \\
 \hline
 Ex & Existential E & There is a woman cycling. & 0 & 0 \\
 \hline
 F1x & Existential F1 & There is an apple being cut. & 0 & 1 \\
 \hline
 F2x & Existential F2 & There is a bird being shot by a man. & 0 & 0 \\
 \hline
 Z & All other forms & The man is trying to hunt a bird. & 2 & 6 \\
 \hline
\end{tabular}
\end{center}
\caption{Sentence type examples, with distributions of types for
  native speakers (NS) and non-native speakers (NNS)}
\label{tab:sentence-type}
\end{table*}


I manually categorized the 530 sentences in the dataset into 11 types plus one catch-all category, as shown in
Table~\ref{tab:sentence-type}. I established these types because each
one corresponds to a basic sentence structure and thus has consistent
syntactic features, leading to predictable patterns in the dependency
parses. A sentence type indicates that the subject,
verb, and object can be found in a consistent place in the parse,
such as under a particular dependency label.
For simple transitive sentences (type \textit{A} in Table~\ref{tab:sentence-type}), for example, the dependents labeled \textit{nsubj}, \textit{root}, and \textit{dobj} 
pinpoint the necessary information.
Thus, the patterns for extracting semantic information---in the form
of \textit{verb(subj,obj)} triples---reference particular Stanford
typed dependency labels, part-of-speech (POS) tags, and locations
relative to word indices (see Figure~\ref{fig:conll}).


\begin{figure}[htb!]
\begin{center}
\begin{tabular}{|C{4em}|C{5em}|C{4em}|C{6em}|C{4em}|}
\hline
\textbf{Index} & \textbf{Dependent} & \textbf{POS} & \textbf{Head} & \textbf{Label} \\
\hline
\hline
%0 & \textit{root} & N/A & N/A & N/A \\
%\hline
1 & the & DET & 2 (boy) & det \\
\hline
2 & boy & NN & 4 (kicking) & nsubj \\
\hline
3 & is & VBZ & 4 (kicking) & aux \\
\hline
4 & kicking & VBG & 0 (\textit{root}) & root \\
\hline
5 & the & DT & 6 (ball) & det \\
\hline
6 & ball & NN & 4 (kicking) & dobj \\
\hline
\hline
    \multicolumn{5}{|c|}{\begin{dependency}[arc edge,text only label,label style={above}]
    \begin{deptext}[column sep=.5em]
      ROOT \& DET \&[1em] NN \& VBZ \&[1em] VBG \& DET \&[1em] NN \\
      \textit{root} \& The \&[1em] boy \& is \&[1em] kicking \& the \&[1em] ball \\
    \end{deptext}
    \depedge[arc angle=90]{5}{3}{nsubj}
    \depedge{5}{4}{aux}
    \depedge[arc angle=92]{1}{5}{root}
    \depedge[arc angle=80]{5}{7}{dobj}
%    \depedge[arc angle=35,edge style={dotted}]{7}{6}{det}
%    \depedge[edge style={dotted}]{3}{2}{det}
    \depedge[arc angle=35]{7}{6}{det}
    \depedge{3}{2}{det}
  \end{dependency}} \\
\hline
\end{tabular}
\end{center}
%\caption{The dependency parse of an example NNS response in CoNLL\footnote{Standard dependency parse format established by the Conference on Computational Natural Language Learning (CoNLL).} format and the corresponding visual representation.}
\caption{The dependency parse of an example NNS response in a standard format (CoNLL) and the corresponding visual representation}
\label{fig:conll}
\end{figure}

%More complicated sentences or those containing common learner errors
%(e.g., omission of the copula \textit{be}) required slightly more
%complicated extraction rules, but, since this work examined only transitive
%verbs, these still boiled down to identifying the
%sentence type and extracting the appropriate triple.
Determining the sentence type was accomplished by arranging a small set of binary decisions into a tree, as shown in Figure~\ref{fig:decision-tree}. This decision tree checks for the presence of various dependency labels. The extraction rules for the particular sentence type were then applied to obtain the semantic triple. Finally, for each NNS response, the resulting triple was lemmatized and checked against the list of lemmatized NS triples. Ideally, each acceptable response will find a match, and unacceptable responses will not.

\begin{figure*}[htb!]
\begin{center}
\begin{tikzpicture}
\tikzset{level distance=3.5em}
\tikzset{edge from parent/.append style={->}}
\Tree
[.{\tt expl}?
  \edge node[auto=right,pos=.6,inner sep=1pt]{Y};
  [.{\tt auxpass}? 
  	\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
  	[.{\tt agent}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.F2x ]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.F1x ]
	]
	\edge node[auto=left,pos=.6,inner sep=1pt]{N};
	[.{\tt dobj}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.Ax ]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.{\tt prep\_}$\ast$?
			\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
			[.Bx ]
			\edge node[auto=left,pos=.6,inner sep=1pt]{N};
			[.Ex ]
		]
	]
  ]
  \edge node[auto=left,pos=.6,inner sep=1pt]{N};
  [.{\tt nsubjpass}? 
  	\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
  	[.{\tt agent}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.F2 ]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.F1 ]
	]
	\edge node[auto=left,pos=.6,inner sep=1pt]{N};
	[.{\tt dobj}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.{\tt nsubj}?
			\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
			[.A ]
			\edge node[auto=left,pos=.6,inner sep=1pt]{N};
			[.C ]
		]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.{\tt nsubj}?
			\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
			[.{\tt prep\_}$\ast$?
			 	\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
				[.B ]
				\edge node[auto=left,pos=.6,inner sep=1pt]{N};
				[.E ]
			]
			\edge node[auto=left,pos=.6,inner sep=1pt]{N};
			[.D ]
			]
		]
	]
  ]
]
\end{tikzpicture}
\end{center}
\caption{Decision tree for determining sentence type and extracting semantic triple based on the presence of syntactic dependency labels}
\label{fig:decision-tree}
\end{figure*}

%To illustrate, consider the process for the example in Figure~\ref{fig:F2-dependency}.  The
%sentence is passed through the parser to obtain the dependency parse shown.
%The parsed sentence then moves to the
%decision tree shown in Figure~\ref{fig:decision-tree}.
%At the top of the tree, the sentence is checked for an {\tt expl}
%(expletive) label; having none, it moves rightward to the {\tt
%  nsubjpass} (noun subject, passive) node. Because a {\tt
%  nsubjpass} label is found, the sentence moves leftward to the {\tt agent}
%node. This label is also found, and because the sentence has reached a terminal node, it is labeled as a type F2 sentence.
%
%
%With the sentence now typed as F2, specific F2 extraction
%rules are applied. The logical subject is taken from under the {\tt agent} label,
%the verb from {\tt root}, and the logical object from {\tt nsubjpass},
%to obtain \textit{shot(man,bird)}, which can be lemmatized to
%\textit{shoot(man,bird)}. 
%%Very little effort goes into this process:
%%the parser is pre-built; the decision tree is small; and the
%%extraction rules are minimal.
%
%This much is possible with relatively little effort in part due to the constraints in the
%pictures.  For figure~\ref{fig:example-picture}, for example,
%\textit{the artist}, \textit{the man in the beret}, and \textit{the
%  man} are all acceptable subjects, whereas if there were multiple men
%in the picture, \textit{the man} would not be specific enough.
%%In future work, we expect to relax such constraints on image contents
%%by including rules to handle relative clauses, adjectives and other
%%modifiers in order to distinguish between references to similar
%%elements, e.g., 
%%\textit{a man shooting a bird} vs. \textit{a man reading the
%%  newspaper}.

\subsection{Rule-based results}
\label{sec:rule-results}

Evaluating this work required addressing two major questions.  First,
how accurately does this approach extract semantic information from potentially
innovative sentences?
%Due to the simple structures of the sentences (section~\ref{sec:sentence-distribution}), this simple system performs moderately well.
Second, how many semantic forms does one need in order to capture the variability in meaning in NNS sentences? I operationalized this second question by asking how well a given set of NS semantic forms functions as an ``answer key'' or gold standard.
% (section~\ref{sec:eval:coverage})?

An accurate extraction was defined as one in which the extraction rules chose the desired subject, verb, and object given the sentence at hand and without regard to the PDT image. Accuracy was 92.3\% for NNS responses and 92.9\% for NS responses. I attribute the high extraction scores to the constrained nature of the task and the relatively small range of sentence types it elicits. As seen in Table~\ref{tab:sentence-type}, only three sentence types (A, B, and C) account for more than 90\% of all responses.

Assessing the coverage of NNS forms required first manually determining which extracted triples \textit{should} be matched given a hypothetical perfect gold standard set of triples. To separate the problem of coverage from extraction, I first removed any incorrectly extracted triples from the NNS set and the NS gold standard.

Using the annotations discussed in Section~\ref{sec:pilot-data}, I called an appropriate NNS triple found in the NS gold standard set a \textbf{true positive (TP)} (i.e., a correct match), and an appropriate NNS triple \textit{not found} in the gold standard set a \textbf{false negative (FN)} (i.e., an incorrect non-match), as shown in Table~\ref{tab:contingencies}. I used standard terminology here (TP, FN), but because this was an investigation of what \emph{should be} in the gold standard, these were considered
false negatives and not false positives.  To address the question of
how many NS sentences are needed to obtain good coverage, \textbf{coverage} was defined as recall: \textit{TP/(TP+FN)}. I reported 23.5\% coverage for unique triple \textit{types} and 51.0\% coverage for triple \textit{tokens}.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|ll||l|l|}
  \hline
  & & \multicolumn{2}{c|}{NNS}\\
  & & $+$ & $-$ \\
  \hline
  \hline
  \multirow{2}{*}{NS} & Y & TP & FP \\
  \cline{2-4}
  & N & FN & TN\\
  \hline
\end{tabular}
\end{center}
\caption{Contingency table comparing presence of NS forms (Y/N) with
  appropriateness ($+$/$-$) of NNS forms}
\label{tab:contingencies}
\end{table}

I defined an inappropriate NNS triple (i.e., a content error)
\textit{not found} in the gold standard as a \textbf{true negative
  (TN)} (i.e., a correct non-match). \textbf{Accuracy} based on this
gold standard---assuming perfect extraction---is defined as
\textit{(TP+TN)/(TP+TN+FN)}.\footnote{Accuracy is typically defined as (TP+TN)/(TP+TN+FN+FP), but false positives (FPs) are cases where an incorrect NNS response was matched in the NS gold standard; by removing errors from the NS gold standard, I prevented this scenario (i.e., FP=0).} I reported 46.4\% accuracy for types and 58.9\% accuracy for tokens.

The immediate lesson taken from this work was this: given a strict matching approach, NS data alone does not make for a sufficient gold standard, in that many appropriate NNS answers are not counted as correct. I explored expanding the set of NS triples by separating individual subjects, verbs and objects from NS triples and recombining them into the various possible combinations. However, this recombination generates a lot of nonsensical triples and degrades the gold standard. Consider, for example, \textit{do(woman,shirt)}---an incorrect triple derived from the correct NS triples, \textit{wash(woman,shirt)} and \textit{do(woman,laundry)}. This could be improved somewhat by evaluating new combinations with a language model, but this would both complicate the approach and diverge from my vision of content analysis driven by real speaker behavior. Instead, my current work has attempted to improve coverage by prompting NSs to give an initial PDT response, followed by a second alternative, as discussed in Chapters~\ref{chap:data} and~\ref{chap:optimization}.

A related concern was that, even when only examining cases
where the meaning is literally correct, NNSs produced a wider range of
forms to describe the prompts than NSs. For example, for a picture
showing what 100\% of NSs described as a \textit{raking} action,
many NNSs referred to a man \textit{cleaning} an area. Literally,
this may be true, but it does not align with a NS gold standard. 
This behavior was expected, given that learners are encouraged
to use words they know to compensate for gaps in their vocabularies
\citep{AgustinLlach2010}. This also parallels the observation in SLA research that while second language learners may attain native-like grammar, their ability to use
pragmatically native-like language is often much lower
\citep{BardoviDornyei1998}. These findings highlighted the need for a more flexible approach.
%that considers how native-like a sentence is as well as how appropriate its meaning is.

Moreover, evaluating this strict matching approach required an annotator to decide whether a given response is correct or incorrect. Partial matching is not allowed; this is an inherent weakness of the approach, because while a complete triple gives some indication of the meaning of the sentence, any single element of the triple taken alone does not provide enough context to indicate meaning. This inflexibility means that using this approach would effectively require the manual curation of a robust gold standard set of acceptable responses, which is counter to my goal of producing an approach that can be expanded to new PDT items simply by crowdsourcing responses from NSs.

%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%Triple & Example sentence \\
%\hline
%\hline
%shoot(man, bird) & A man just shot a bird. \\
% \hline
%shoot(man, fowl) & The man shoots the fowl. \\
% \hline
%shoot(man, duck) & A man just shot a duck. \\
% \hline
%shoot(hunter, bird) & The hunter has shot a bird. \\
% \hline
%shoot(he, bird) & He shot the bird down! \\
% \hline
% \end{tabular}
%\end{center}
%\caption{The NS gold standard for Item 10.}
%\label{tab:item10GS}
%\end{table}

I followed up this work with a modification that included language models and spelling correction tools to attempt to identify and fix misspellings that lead to downstream problems \citep{king:dickinson:14}. I omit this discussion because it is not applicable or comparable to my expanded corpus; I now take a much simpler approach---respondents used their browser's spell checker as necessary during the task (see Section~\ref{sec:pdt}). This is because in most contexts where my system would be used, like a language tutoring application or game, spelling instruction is not the objective, and a built-in spell checker would likely be available. Moreover, omitting this step removes a layer of analysis---and importantly, a potential source of errors---and allows the research to focus more directly on meaning.

Ultimately, I was not satisfied with effectiveness of the rule-based approach. While the roughly 92\% accuracy rate for extraction may seem high, triple extraction only represents an intermediate step of the processing and effectively sets a ceiling for downstream performance. With regard to matching semantic triples, the coverage score of 51\% is somewhat less informative because it is at least partially a function of the small gold standard (n=14). However, the homogenous nature of NS responses seen for several items here (such as the \textit{raking} example discussed above) suggests that expanding NNS coverage is not simply a question of collecting responses from more NSs. These findings led me to experiment with new approaches, beginning with the work described in Section~\ref{sec:2016work}.
%%%% END material from Qual Paper (BEA 2013) %%%%


%%%% BEGIN material from BEA 2016 %%%%
\section{Semantic similarity study}
\label{sec:2016work}
%%%% 2020/05/21. Resume here. %%%% 

In subsequent work \citep{king:dickinson:16}, I began looking for a ``sweet spot'' of
semantic analysis \citep[cf.][]{bailey:meurers:08} for image-based learner productions. I should note, in this context, that I am discussing semantic analysis given a reliable set of NS sentences, as opposed to other areas of research tying images and language. Image processing tasks like automatic captioning often rely on breaking images into semantic primitives \citep[see, e.g.,][and references therein]{ortiz:wolff:lapata:15}, but for NNS data, I want to ensure that I can account not just for correct semantics (the \emph{what} of a picture), but natural expressions of the semantics (the \emph{how} of expressing the content).  In other words, the goal is to reason about meaning based on specific linguistic forms.

A second issue regarding content analysis, beyond correctness, stems
from using an incomplete gold standard. The productive nature of language means that a sentence can be expressed in countless ways, and thus a gold standard can never really be ``complete.'' For this reason, it is necessary to assume that novel NNS productions will occur, and the methods of analysis should be robust enough to differentiate between responses that do not appear in the gold standard because they are bad responses, and those that do not appear in the gold standard simply because they are infrequent.
In particular, using available NLP tools, I moved away from discrete representations of correctness in the form of a NS gold standard set of semantic triples to a more continuous notion of correctness using a NS model comprised of smaller, overlapping pieces of information. This obviates the need for a rule-based extraction of semantic triples, which is a source of errors and must be customized for a limited range of expected sentence types. It also allows for graded scoring of results, meaning that a response is not outright rejected because only one argument of a triple is not found. On the other hand, it means that the system does not provide a discrete ``thumbs up'' or ``thumbs down'' decision for each response, which may be desirable in some use cases.

%Starting in Section~\ref{sec:ranking}, I explain a set of revised methods, and in Section~\ref{sec:metrics}, I discuss the results of those methods applied to the same 530 response dataset. The work and findings here became the basis of the much larger study discussed in Chapters~\ref{chap:data} through~\ref{chap:optimization}. 

\subsection{Generalizing the methods}
\label{sec:ranking}

The previous work assumed that the assessment of NNS responses
involves determining whether a NS gold standard set contains the same
semantic triple that the NNS produced, i.e., whether a triple
is \textit{covered} or \textit{non-covered}.  In such a situation the
gold standard need only be comprised of \textit{types} (not \textit{tokens}) of semantic triples. Here the gold standard is comprised of the small set of NS responses---only 14. This means that exact matching is going to miss many cases,
and indeed as discussed in Section~\ref{sec:rule-results}, coverage was only 51\%. Even with a much larger gold standard, we can expect responses to follow Zipf's law; a sample of language data will always be incomplete because it will not contain all ``long tail,'' low-frequency phenomena.

Additionally, relying on matching of triples limits the utility of the method to specific semantic constraints, namely transitive sentences. By dropping the exact matching approach and instead comparing the frequencies of elements in the NNS response with those of the gold standard, I moved into a gradable, or ranking, approach to the analysis, which is agnostic with regard to an item's transitivity or sentence type (see Table~\ref{tab:sentence-type}). For this reason, I shift terminology here from NS \textit{\textbf{gold standard}} to NS \textit{\textbf{model}}. A gold standard is roughly akin to an answer key, which was appropriate for my strict triple-matching approach. A model is typically a richer data structure containing statistics from observations of known correct data. This distinction and its relevance will be more apparent with the discussion of response representations in Section~\ref{sec:response-rep}.

My goal is to emphasize the degree to which a NNS response conveys the same
meaning as the set of NS responses, necessitating an approach which can automatically
determine the importance of a piece of information among the set of NS responses.  This required two major decisions: 1) how to \textbf{represent} each response as a set of sub-elements, and 2) how exactly to \textbf{score} these sub-elements via comparison with the NS data. In Section~\ref{sec:response-rep}, I detail how I represented the information and in Section~\ref{sec:scoring}, I discuss comparing NNS information to NS information, which allowed me to rank responses from most to least similar to the NS model. In Section~\ref{sec:parameters}, I describe the system parameters that I combined to generate scores, and in Section~\ref{sec:metrics} I present and interpret the results of the various settings.
%to rank responses from least to most similar to the NS gold standard.
%\footnote{Although rankings often go from highest to lowest, I prioritize identifying problematic cases, so I rank accordingly.}
%I also discuss the handling of various other system parameters (section~\ref{sec:parameters}).

%This work used the same 10 item PDT dataset described in section~\ref{sec:first-approaches}. Another example is shown in Figure~\ref{fig:example-picture2}.
%
%\begin{figure}
%\begin{center}
%\begin{tabular}{|c|}
%\hline
%\includegraphics[width=0.7\columnwidth]{figures/exampleprompt2.jpg}\\
%\hline
%\textbf{Response (L1)} \\
%\hline
%The man killing the beard. (Arabic)\\
%\hline
%A man is shutting a bird. (Chinese) \\
%\hline
%A man is shooting a bird. (English) \\
%\hline
%The man shouted the bird. (Spanish)\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Example item and responses}
%\label{fig:example-picture2}
%\end{figure}
%
\subsection{Term representation}
\label{sec:response-rep}

%To overcome the limitations of an incomplete GS,
I first represented each NNS response as a list of dependencies taken directly from the parse. 
%\citep{demarneffe:ea:06}, the terms referring to
%%being either 
%individual dependencies (i.e., relations between words).
%or individual words. 
This eliminates the complications of extracting semantic triples from
dependency parses, which only handled a very restricted set of
sentence patterns and resulted in errors in 7--8\% of cases, as discussed in Section~\ref{sec:rule-results}.
Operating directly on individual dependencies from the overall tree also means the system can allow for ``partial credit;'' it distributes the matching over smaller,
overlapping pieces of information rather than a single, highly specific triple. As before, the responses were lemmatized to minimize data sparsity.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|C{7em}||C{6em}||C{5.5em}||C{5.5em}||C{5em}|}
\hline
\multicolumn{5}{|c|}{The boy is kicking the ball.} \\
\hline
\hline
\textbf{ldh} (labeled) & \textbf{xdh} (unlab.) & \textbf{lxh} & \textbf{ldx} & \textbf{xdx} (word)\\
\hline
\hline
det(the,boy) & \textit{x}(the,boy) & det(\textit{x},boy) & det(the,\textit{x}) & \textit{x}(the,\textit{x}) \\
\hline
nsubj(boy,kick) & \textit{x}(boy,kick) & nsubj(\textit{x},kick) & nsubj(boy,\textit{x}) &  \textit{x}(boy,\textit{x}) \\
\hline
aux(is,kick) & \textit{x}(is,kick) & aux(\textit{x},kick) & aux(is,\textit{x}) & \textit{x}(is,\textit{x}) \\
\hline
root(kick,root) & \textit{x}(kick,root) & root(\textit{x},root) & root(kick,\textit{x}) & \textit{x}(kick,\textit{x}) \\
\hline
det(the,ball) & \textit{x}(the,ball) & det(\textit{x},ball) & det(the,\textit{x}) & \textit{x}(the,\textit{x}) \\
\hline
dobj(ball,kick) & \textit{x}(ball,kick) & dobj(\textit{x},kick) & dobj(ball,\textit{x}) & \textit{x}(ball,\textit{x}) \\
\hline
\end{tabular}
\end{center}
%\caption{The dependency parse of an example NNS response in CoNLL\footnote{Standard dependency parse format established by the Conference on Computational Natural Language Learning (CoNLL).} format and the corresponding visual representation.}
\caption{Given the example sentence above, the updated approach represents responses in the dependency formats shown: ldh (for \textit{label}, \textit{head}, \textit{dependent}; i.e., labeled dependencies)), xdh (unlabeled dependencies), lxh (label+head), ldx(label+dependent), or xdx (word, or more technically, \textit{dependent}).}
\label{tab:dep-rep}
\end{table}

Next, I obtained five different representations from the lemmatized dependencies, as shown in Table~\ref{tab:dep-rep}. I refer to this variable as \textbf{term representation}, in keeping with \textit{term} frequency-inverse document frequency, discussed in Section~\ref{sec:scoring}.
%I first tokenized and lemmatized a response to a list of dependencies that represents the response.
The five term representations are then variations on dependencies. The
full form is the \textbf{labeled dependency} and includes the \textbf{l}abel, \textbf{d}ependent and \textbf{h}ead, so I refer to it in shorthand as \textbf{ldh}. The remaining four forms abstract over either the label, dependent and/or head. I refer to these forms as \textbf{xdh} (i.e., unlabeled dependency), \textbf{lxh} (label+head), \textbf{ldx} (label+dependent), and \textbf{xdx} (dependent only, roughly equivalent to \textit{word} in a bag-of-words approach).

The goal in choosing these five representations was to find the optimal combination of dependency features and the right level of detail to obtain the best system performance. The bag-of-words representation was implemented largely to provide a baseline by which to compare the others.

This processing was applied to the collection of NS responses as well. For each item, the dependencies from all NS responses was pooled into a single flat list---a ``bag-of-dependencies.'' From this list, a copy in each of the five term representations was produced, allowing for comparison with the corresponding NNS data.
%I tested the system performance using each of these term representations separately.

%The \param{xdx} model is on a par with treating the sentence as a ``bag
%of words'' (or more accurately, a bag of lemmas), except that some function words not receiving parses (e.g., prepositions) are not included (see section~\ref{sec:syntactic-form}).

\subsection{Scoring responses}
\label{sec:scoring}

Taking the five term representations, my next step was to automatically score them in a way which ranks responses from most to least appropriate.  I devised four scoring approaches, each using one of two methods to \textbf{weight} response terms combined with one of two methods to \textbf{compare} the weighted NNS terms with the NS data.

For weighting, the simplest method used the relative frequency of each term (i.e., dependency). This is the token count of a given term in a document normalized by the total count of tokens in the document.

The other method of weighting was based on \textit{term frequency-inverse document frequency} (\textit{tf-idf}), which scores the importance of terms in a test document according to their frequencies in the language generally \citep[][ch. 6]{manning-et-al:08}. It approximates this by taking a large reference corpus comprised of many smaller documents, and counting the number of those documents in which the terms in question occur. Most commonly, a \textit{term} for tf-idf purposes would be a \textit{word}. However, \textit{term} here refers to a single syntactic dependency, and this is a central conceit of this dissertation:  the dependency, which captures aspects of semantic and syntactic relationships, is an ideal atomic unit for evaluating meaning by comparing distributions in crowdsourced data. As discussed in Section~\ref{sec:response-rep}, the dependencies were represented in one of the five term representations---some combination of label, dependent and head. For the NNS data, each response was treated as a tf-idf test document. For the NS data, the entire collection of NS responses was treated as one tf-idf test document. The idea here is to obtain a similarity measure between a single NNS response and the full collection of NS responses, so each is handled as a single document.

I used tf-idf as a measure of a term's importance with the expectation that it would reduce the impact
of semantically less important terms---e.g., determiners like
\textit{the}, frequent in the responses, but mostly unnecessary for evaluating the
semantic contents---and to upweight terms which may
be salient but infrequent, e.g., only used in a handful of NS
sentences. For example, for an item depicting a man shooting a bird
(see Figure~\ref{fig:example-picture}), of 14 NS responses, 12 described the subject as \textit{man}, one as \textit{he} and one as
\textit{hunter}. Since \textit{hunter} is relatively infrequent in English, even
one instance in the NS responses should get upweighted via tf-idf, and indeed
that was the effect; in the bag-of-words approach, the term \textit{hunter} is weighted among the highest, and the same is true for dependencies containing the word \textit{hunter} among the other term representations. This is valuable, as numerous NNS responses used \textit{hunter}.

Calculating tf-idf relies on both \emph{term frequency} ($tf$) and
\emph{inverse document frequency} ($idf$).  Term frequency is simply
the raw count of a term within a document. Inverse document frequency is derived from a reference corpus of documents, and it is based on the notion that appearing in more documents makes a term less informative with respect
to distinguishing between documents.  The formula is shown in
(\ref{ex:tfidf}) for a term $t$, where $N$ is the number of documents
in the reference corpus, and $df_{t}$ is the number of documents
featuring the term ($idf_{t} = \log \frac{N}{df_{t}}$).  A term
appearing in fewer documents will thus obtain a higher $idf$ weight,
and this should readjust frequencies based on semantic importance.

\begin{exe}
\ex\label{ex:tfidf} $tfidf(t) = tf_{GS} \log \frac{N}{df_{t}}$
%, where $df_t = |\{d\in D, t \in d\}|$
\end{exe}

After this frequency counting or tf-idf weighting, the scores were then either
\textbf{averaged} to yield a response score, or NNS term
weights and NS term weights were treated as vectors and the response
score was the \textbf{cosine distance} between them.  This
yields the four approaches:

%%former approach names: b = FA; m = IC (TC); c = FC; a = IA (TA)
\paragraph{Frequency Average (FA).} 
%This approach serves as our baseline. 
Within the set of NS responses, the relative frequency of each term is calculated. The NS \textit{model} here is simply each NS term and its relative frequency. Each term in
the NNS response is then given a score equal to its frequency in the
NS model; terms missing from the NS model are scored zero. The response score is
the average of the term scores, with higher scores closer to the NS model.

\paragraph{Tf-idf Average (TA).} This involves the same
averaging of term scores as with approach FA, but here the term scores are the tf-idf scores. The NS model here is thus each NS term and its tf-idf score. 

\paragraph{Frequency Cosine (FC).} Each term score is taken as its relative frequency 
calculated within its document: either the NS response set or the single NNS response. The NS model is then the set of all NS terms and their scores. The term scores are then treated as vectors---one vector of the NS term scores (i.e., the NS model here)---and one vector of the NNS term scores. Each vector is an ordered list of term scores for each term observed in either the NS document or the NNS document. In other words, each vector represents term scores for the sorted union set of NS and NNS terms. Naturally, many of the term scores are zero for the much shorter NNS document. The response score is the cosine distance between the vectors, with lower scores being closer to the NS model.

\paragraph{Tf-idf Cosine (TC).} This involves the same distance comparison as with approach FC, but now the term scores in the vectors are tf-idf scores. The NS model here is thus the vector representing the union set of terms for the NS and NNS document, populated with the tf-idf scores from the NS document.

\bigskip

%The two cosine approaches are effectively primitive versions of sentence encoders like the currently popular BERT \citep{BertDevlin2018} and Universal Sentence Encoder \citep{UniversalSentenceEncoder}. Sentence encoders are a form of language model that learns mathematical representations of words by observing them in context, accounting for things like average distance from a given word type to another given word type. Sentence encodings are thus vectors representing these word values for a full sentence. These approaches result in very high dimensional spaces---imagine a sentence representation that consists of a vector for each word in the sentence, where each vector is a list of average distances from that word type to \textit{every other word type in the language}. Thus sentence encoders typically rely on methods of dimensionality reduction to compress these representations into vectors of manageable length.
%I say my cosine approaches constitute ``primitive'' encoders because they omit this step.
In many natural language processing scenarios where high dimensional vectors are involved, such as sentence encoders or word embeddings, methods for dimensionality reduction are employed \citep{BertDevlin2018,word2vec}. This improves efficiency by reducing the storage and computing power needed. In my FC and TC approaches, however, the number of term types remained small enough that the raw vectors representing dependencies' tf-idf scores can be processed easily with an ordinary PC. Not only does this simplify the task, it means that the process remains transparent. There are no transformers or attention mechanisms to produce compressed and unexplainable representations.  If desired, each sentence vector can be examined value by value, where each number maps to a real syntactic dependency. This is important because it leaves the door open for meaningful feedback on each response. For example, one might choose to identify the most salient dependencies in the NS model and use them to guide an ICALL user from a low scoring response to a better response.
%That is to say, a full encoder could determine how similar a response is to a GS, but it could not tell us \textit{why} it made its determination.

\subsection{System Parameters}
\label{sec:parameters}

I ran a total of 30 experiments, with each representing a combination of these system parameters for processing responses: 

\paragraph{Term representation} As discussed in
section~\ref{sec:response-rep}, the terms can take one of five
representations: \param{\textbf{ldh}}, \param{\textbf{xdh}}, \param{\textbf{lxh}}, \param{\textbf{ldx}},
or \param{\textbf{xdx}}.

\paragraph{Scoring approach.} As discussed in
section~\ref{sec:scoring}, the NNS responses can be
compared with the NS models via approaches \param{\textbf{FA}}, \param{\textbf{TA}}, \param{\textbf{FC}}, or \param{\textbf{TC}}.

\paragraph{Reference corpus.} The reference corpus for deriving tf-idf
scores can be either the Brown Corpus (\param{\textbf{Brown}}) \citep{kucera:francis:67} or the Wall Street Journal Corpus (\param{\textbf{WSJ}}) \citep{marcus-et-al:93}. The Brown Corpus is just over one million words across 500 documents intended to cover a broad range of genres, registers, and contents and to serve as a sample of the written English language at large.
The WSJ Corpus used here consists 1,640 documents; the documents are newspaper articles from 1989 and total one million words. Considering the narrative nature of PDT responses, a reference corpus of narrative texts would be ideal, but as no such reliably parsed corpus is available, I chose the widely used, pre-parsed \param{Brown} and \param{WSJ} corpora. The corpora were converted from their standard dependency parse format to each of the five term representations used in order to be compatible with the NS and NNS data for tf-idf.

%\paragraph{NNS source.} Each response has an original version
%(\param{NNSO}) and the output of a language model spelling corrector
%(\param{NNSLM}). (The current dissertation relies on a corpus for which participants used spell checking at the time of the task, so this offline spelling correction is no longer applicable. In short, it used a spelling tool to find candidate spellings for each word in a NNS sentence, pruned the lists of candidate words by comparing against words in NS responses, formed new candidate sentences by combining candidate words, and finally chose the most likely sentence by rating each candidate with a trigram word model. I omit the exact details here for brevity, but more can be found in \cite{king:dickinson:14}).
%
\subsection{Experiments and Results}
\label{sec:metrics}
\subsubsection{Evaluation metrics}

%I ran 60 response experiments, each with different system settings
I ran 30 response scoring experiments, each with different system settings
(section~\ref{sec:parameters}). Within each experiment, I ranked the 39
scored NNS responses from least to most similar to the NS model.

For assessing these settings themselves, I relied on the past annotation, which counted unacceptable responses as errors (see section~\ref{sec:rule-results}).  As the lowest numerical rank indicates the greatest distance from the NS model, a good system setting should position the unacceptable responses among those with the lowest rankings.
%Thus, I assigned each error-containing
%response a score equal to its rank (or the average rank in the case of tied responses).
To evaluate this discriminatory power, I used \textbf{(mean) average precision ((M)AP)}
\citep[][ch. 8]{manning-et-al:08}.

\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{0.3em}
\begin{tabular}{|r|c|l|c|}
\hline
Rank & Score & Sentence & Error \\
\hline
\hline
\multirow{2}{*}{1} & 1.000 & she is hurting. & 1 \\
& 1.000 & man mull bird & 1 \\
\hline
3 & 0.996 & the man is hurting duck. & 1 \\
4 & 0.990 & he is hurting the bird. & 1 \\
\hline
11 & 0.865 & the man is trying to hurt a bird & 1 \\
12 & 0.856 & a man hunted a bird. & 0 \\
\hline
17 & 0.775 & the bird not shot dead.  & 1 \\
18 & 0.706 & he shot at the bird & 0 \\
19 & 0.669 & a bird is shot by a un & 1 \\
20 & 0.646 & the old man shooting the birds & 0 \\
\hline
37 & 0.086 & the old man shot a bird. & 0 \\
38 & 0.084 & a old man shot a bird. & 0 \\
39 & 0.058 & a man shot a bird & 0 \\
\hline
\hline
\multicolumn{4}{|c|}{Average Precision: 0.75084} \\
\hline
\end{tabular}
\caption{Example item rankings for the system setting combining these parameters: \param{TC}, \param{Brown}, and \param{ldh} (labeled dependencies). This was the best system setting based on average precision scores. Note that not all 39 responses are shown.}
\label{tab:i10responses-avgprec}
\end{center}
\end{table}

For average precision (AP), one calculates the precision of error
detection at every point in the ranking, lowest to highest. Consider
Table~\ref{tab:i10responses-avgprec}, which presents an excerpt of ranked sentence
responses for one PDT item. The precision for
the first cut-off (1.000) is 1.0, as two responses have been
identified, and both are errors ($\frac{2}{2}$). At the 11th- and
12-ranked response, precision is 1.0 (=$\frac{11}{11}$) and 0.917
(=$\frac{11}{12}$), respectively, precision dropping when the item is
not an error.
AP averages over the precisions for all $m$ responses ($m=39$ for the
NNS data), as shown in (\ref{ex:ap}), with each response notated as
$R_k$.  Averaging over all 10 items results in the Mean AP (MAP).

\begin{exe}
\ex\label{ex:ap} $AP(item) = \frac{1}{m} \sum\limits_{k=1}^m
Precision(R_k)$
\end{exe}

\subsubsection{Best system parameters} 

To start the search for the best system parameters, it may help to
continue with the example in
Table~\ref{tab:i10responses-avgprec}. The best setting, as determined by the
MAP metric, uses the tf-idf cosine (\param{TC}) approach with the Brown Corpus (\param{Brown}), and the full form of the labeled dependencies (\param{ldh}). It ranks highest because errors are
well separated from non-errors; the highest ranked of 17 total errors
is at rank 19.  Digging a bit deeper, one can see in this example how
the verb \textit{shoot} is common in all the highest-ranked cases shown
(\#37--39), but absent from all the lowest, showing both the effect of
the NS model (as all NSs used \textit{shoot} to describe the action) and the
potential importance of even simple representations like lemmas.  In
this case, the labeled dependency (\param{ldh}) representation is best, likely because the
word \textit{shoot} is not only important by itself, but also in terms
of which words it relates to, and how it relates (e.g.,
\textit{dobj(bird,shoot)}).

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|r|l|c|c|c|}
\hline
Rank & MAP & Approach & Reference & Term rep.\\
\hline
\hline
1 & 0.5168 & \param{TC} & \param{Brown} & \param{ldh} \\
\hline
2 & 0.5128 & \param{TC} & \param{WSJ} & \param{ldh} \\
\hline
3 & 0.5124 & \param{TC} & \param{Brown} & \param{xdh} \\
\hline
4 & 0.5109 & \param{TC} & \param{Brown} & \param{lxh} \\
\hline
5 & 0.5102 & \param{TC} & \param{WSJ} & \param{xdh} \\
\hline
\hline
26 & 0.4826 & \param{FA} & \textit{na} & \param{ldx} \\
\hline
27 & 0.4816 & \param{TA} & \param{Brown} & \param{xdx} \\ 
\hline
28 & 0.4769 & \param{FC} & \textit{na} & \param{lxh} \\ 
\hline
29 & 0.4721 & \param{TA} & \param{WSJ} & \param{xdx} \\
\hline
30 & 0.4530 & \param{FA} & \textit{na} & \param{lxh} \\ 
\hline
\end{tabular}
\caption{Based on Mean Average Precision, the five best and five worst combinations of system parameters across all 10 PDT items.}
\label{tab:all-dist-ranked-settings}
\end{center}
\end{table}

Table~\ref{tab:all-dist-ranked-settings} shows the five best and five
worst combinations of system settings averaged across all 10 PDT items, as ranked by
MAP. The table clearly indicates that the \param{TC} approach is superior, occurring in all of the top five combinations.
%, while approaches \param{FA} and \param{FC} compete for worst. 
\param{Brown} appears among top scoring combinations more often than does \param{WSJ}. Finally, for the term representation, labeled (\param{ldh}) and unlabeled (\param{xdh}) dependencies score are used most among the top scoring combinations.

I also summarize the rankings for each isolated parameter, presented in Table~\ref{tab:dist-ranked-parameters}. For a given parameter, e.g., \param{ldh}, I averaged the scores from all settings including \param{ldh} across all 10 items. Generally, the same trends appear salient. Notably, \param{TC} outperformed the other models, with \param{FC} and \param{TA} close behind (and nearly tied). Performance fell for the simplest model, \param{FA}, which was in fact intended as a kind of baseline. With \param{\textbf{T}C}$>$\param{\textbf{F}C} and \param{\textbf{T}A}$>$\param{\textbf{F}A}, tf-idf weighting seems preferable to basic frequencies. Likewise, with \param{T\textbf{C}}$>$\param{T\textbf{A}} and \param{F\textbf{C}}$>$\param{F\textbf{A}}, for my term based scoring, taking the cosine of  score vectors outperformed simply comparing score averages.

\begin{table*}
\begin{center}
\begin{tabular}{|l|r||l|r||l|r|}
\hline
\multicolumn{2}{|c||}{Approach} & \multicolumn{2}{|c||}{Term rep.} & \multicolumn{2}{|c|}{Ref. corpus} \\
\hline
\hline
0.50630 & \param{TC} &0.50499 & \param{ldh} & 0.50461 & \param{Brown} \\
\hline
0.49609 & \param{TA} & 0.50405 & \param{xdh} & 0.49777 & \param{WSJ} \\
\hline
0.49471 & \param{FC} & 0.49287 & \param{ldx} & & \\
\hline
0.48247 & \param{FA} & 0.49190 & \param{xdx} & & \\
\hline
 & & 0.49115 & \param{lxh} & & \\
\hline
\end{tabular}
\caption{Individual parameters ranked by Mean Average Precision for all 10 PDT items.}
\label{tab:dist-ranked-parameters}
\end{center}
\end{table*}

These findings largely confirmed my expectations. The \param{TC} approach was intended to evaluate responses by focusing comparison on the most salient content. The scores here show that to be successful. Regarding the top term representations, labeled and unlabeled dependencies both capture the relationship between dependents and their heads, making them an ideal unit for analyzing ``who did what to whom'' in the context of a PDT. Finally, given the subject matter and narrative style of the task, it is unsurprising that \param{Brown} serves as a better tf-idf reference than \param{WSJ}.

The trends noted in these averages were strong overall, but a closer look at individual PDT items revealed some exceptions. For example, for the item depicting a man raking leaves, the \param{ldx} and \param{xdx} term representations were the top performers. As discussed in Section~\ref{sec:rule-results}, 100\% of NSs used the verb \textit{rake}, but only 3/39 NNSs used \textit{rake}. Consider the response, ``The man rakes leaves.'' We can expect the main verb of a transitive sentence to appear in at least three dependencies: root(rakes, ROOT), nsubj(man, rakes), and dobj(leaves, rakes). Note that in two of these three, the verb is the syntactic head. Thus, by omitting heads, the \param{ldx} and \param{xdx} representations increase the likelihood of overlap between the NS response and the NNS model. For example, ``The man rakes leaves'' and ``The man sweeps leaves'' both result in the \param{ldx} terms nsubj(man, \textit{x}) and dobj(leaves, \textit{x}).

A similar pattern emerged for an item that 12 out of 14 NSs described as some version of ``a woman is riding a bike/bicycle,'' using the same subject, verb and object. Seven of 39 NNSs  simply framed this as an intransitive, e.g., ``The woman is biking.'' Two NNSs chose another verb---\textit{pedal} or \textit{drive}. Finally, while 30 NNSs used the verb \textit{ride}, six of these misspelled it---\textit{rid/ridding} or \textit{ridging}. It is also worth noting that seven of 37 instances of \textit{bike} or \textit{bicycle} were misspelled. For this item, the top system performance came from combining the \param{FC} approach with the \param{xdx} (bag-of-words) representation. The combination of noisy (misspelled) NNS data and the small, homogenous set of NS data meant that the least granular term representation (\param{xdx}) worked best. The more sophisticated tf-idf based approaches suffer here due to the level of noise, allowing the \param{FC} to win out.

Overall, the results of the dependency-based similarity approaches to content analysis were sufficiently promising to warrant deeper investigation. The biggest challenge to the validity of these results was the size of the dataset and the nature of its annotation, so I next sought to expand on this work by building a better dataset and conducting similar experiments. In Chapter~\ref{chap:data}, I discuss the collection of a much larger dataset that includes intransitive and ditransitive items in addition to transitives. This also involved a new task variable, where half of the prompts focus the participant's attention on the desired subject of the response, and the other half do not. Additionally, NS participants were instructed to provide two responses per item. These modifications allowed for deeper investigation into task effects and the variability of responses under different conditions. In Chapter~\ref{chap:annotation}, I discuss the development of a new non-binary annotation and weighting system intended to improve the utility of the dataset and the reliability of its annotation. In Chapter~\ref{chap:method}, I detail the revisions to my methods that sprang from the findings here. These include building NS models based on familiar NSs and crowdsourced NSs and the move to rank correlation metrics to accommodate the updated annotation scheme. In Chapter~\ref{chap:optimization}, I discuss how I explored the results of the experiments to find the optimal system settings.

%In the next section (\ref{sec:current-method}), I discuss how I applied these findings in the design of the current study, including the data collection and the methods of content analysis used on the much larger corpus described in Chapters~\ref{chap:data} and ~\ref{chap:annotation}.
%
%Despite the strength of these overall trends, variability
%does exist among the best settings for different items, a point obscured
%in the averages.  In Tables~\ref{tab:i01-dist-ranked-settings} and
%\ref{tab:i05-dist-ranked-settings}, I present the best and worst
%ranked settings for two of the least similar items, 1 and 5.
%Their dissimilarity can be seen at a glance, simply from the range of
%the AP scores (0.05--0.31 for item 1 vs. 0.52--0.81 for item 5), which
%in itself reflects a differing number of erroneous responses (2 [\param{NNSO}]
%or 6 [\param{NNSLM}] for item 1 vs. 23 or 24 for item 5).
%
%For item 1, a drawing of a boy kicking a ball, there is considerable
%variability in the best scoring approach just within the top five settings:
%all four approaches (TA, TC, FA, FC) are in the top five.  Contrary to the overall
%trends, I also found the \param{ldx} form---without any head
%information---in the two best settings.  Note also that, even though
%tf-idf weighting (\param{TA}/\param{TC}) is usually among the best settings, it is
%occurs among the worst settings, too.
%
%For item 5 in Table~\ref{tab:i05-dist-ranked-settings}, a drawing of a
%man raking leaves, the most noticeable difference is that
%of \param{xdx} being among three of the top five settings.
%I believe that part of the reason for
%the higher performance of \param{xdx} (cf. lemmas), is that for this
%item, all the NSs use the verb \textit{rake}, while none of the NNSs use this word.  For item 1 (the boy kicking a ball), there is lexical variation
%for both NSs and NNSs.
%
%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|r|l|c|}
%\hline
%Rank & MAP & Settings \\
%\hline
%\hline
%1 &  &  \\
%\hline
%2 &  &  \\
%\hline
%3 &  &  \\
%\hline
%4 &  &  \\
%\hline
%5 &  &  \\
%\hline
%\hline
%26 &  &  \\
%\hline
%27 &  &  \\
%\hline
%28 &  &  \\
%\hline
%29 &  &  \\
%\hline
%30 &  &  \\
%\hline
%\end{tabular}
%\caption{Based on Average Precision, the five best and five worst settings for item 10, shown in Figure~\ref{fig:example-picture} and Table~\ref{tab:i10responses-avgprec}.}
%\label{tab:i10-dist-ranked-settings}
%\end{center}
%\end{table}


