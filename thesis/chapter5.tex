\chapter{Method}
\label{chap:method}
%Describe earlier approach:
%
%Rule based v(s,o) matching
%
%How this led to trying more generalizable approaches:
%
%Describe current process:
%
%dependency parsing gigacorpus to ldh, xdh, xdx formats;
%
%(gigacorpus info -- did I use full corpus or sample? check)
%
%same for PDT responses;
%
%dependency-based tf-idf for responses vs. gigacorpus;
%
%get sorted superset of XGS \& response terms and their tf-idf scores;
%
%get cosine of these vectors (``TC'' for tf-idf cosine; discuss how this compares to recent encoder based approaches (BERT), as this essentially is a primitive, more transparent encoder);
%
%rank responses by TC;
%
%get spearman of XGS \& TC based ranking vs ranking via the weighted feature annotations.
%
\section{Introduction}
In this chapter, I explain my system for rating and ranking responses automatically, where the goal is to approximate the benchmark rankings described in Chapter~\ref{chap:annotation}. 
The data-driven method used to analyze picture description task (PDT) responses throughout this dissertation represents an evolution from my own previous rule-based methods. In this chapter, I briefly summarize my earlier work and the lessons I learned there, then lay out the current approach.

In short, my first approach was heavily rule-based and relied on strict matching with a pre-established set of acceptable responses. This found moderate success, but lead to the current approach, which is data-driven and relies on more flexible methods of comparison. 

%In short, my earlier approach assessed each non-native speaker (NNS) response by extracting a \textit{verb(subject,object)} triple and looking for a match among triples from the native speaker (NS) responses. This involved dependency parsing the sentence then applying custom rules based on the labels, relationships and parts of speech in order to find each element of the triple. This process found moderate success, correctly assessing roughly half of NNS responses with a very small number of NS responses. Considerable weaknesses emerged, however; the rule based approach meant that it was limited in its ability to handle variation, and the use of simple triples was a hacky simplification of meaning. This lead me to the current approach, which uses a more robust representation of meaning and replaces the rule based matching with measures of semantic similarity. These changes make for a more generalizable approach.

%%%% BEGIN material from Qual Paper (BEA 2013) %%%%
\section{Previous approach: Rule-based semantic triple matching}
\label{sec:first-approaches}
%% I feel like this needs some kind of tldr up front...
This section summarizes relevant work first presented in \citet{king:dickinson:13} and \citet{king:dickinson:14}; please see those papers for deeper discussions.

Like the current research, my previous work focused on analyzing English non-native speaker (NNS) responses to a PDT by comparison with native speaker (NS) responses. I was initially unsure if such a task would be within reach for a single researcher using off-the-shelf tools, so this study sought to uncover the challenges involved and determine whether variation in the form and content of responses could be manageable.

%Research in SLA often
%relies on the ability of task design to induce particular linguistic
%behavior \citep{skehan1998assessing}, and the PDT should induce
%interactive behavior.  Moreover, the use of the PDT as a reliable
%language research tool is well-established in areas of study ranging
%from SLA to Alzheimer's disease \citep{ellis2000task,
%  forbes2005detecting}.

%The NNSs were intermediate and upper-level adult English learners in
%an intensive English as a Second Language program at Indiana
%University. We rely on visual stimuli here for a number of
%reasons. Firstly, computer games tend to be highly visual, so
%collecting responses to visual prompts is in keeping with the nature
%of our desired ILT. Secondly, by using images, the information the
%response should contain is limited to the information contained in the
%image. Relatedly, particularly simple images should restrict elicited
%responses to a tight range of expected contents.

\subsection{Rule-based matching method}
\label{sec:rule-method}

My earlier method was inspired by research from areas such as sentiment analysis, topic modeling, and content assessment that used rule-based approaches to extract important elements from dependency-parsed text (citations XYZ)\lk{XYZ}. My idea was to extract a \textit{verb(subj,obj)} triple from each sentence. Each NNS triple could be compared against the list of NS triples for a match; a NNS response with a NS triple match would be labeled ``correct,'' while a non-match would be labeled ``incorrect.''
%This research is motivated by a desire to see intelligent computer-assisted language learning (ICALL) applications move toward natural communication in game-like visual contexts, so I determined that a picture description task (PDT) would produce a fitting dataset. Research in second language acquisition (SLA) often relies on the
%ability of task design to induce particular linguistic behavior
%\citep{skehan1998assessing}, and the PDT should induce context-focused
%communicative behavior. PDT data allows one to investigate
%pure interlanguage without the influence of verbal prompts and shows
%learner language being used to convey meaning and not just manipulate forms.

For these experiments, I chose or developed PDT images that present an event that I believed to be transitive in nature and likely to elicit responses with an unambiguous subject, verb and object.

%\begin{figure}[htb!]
%%[width=0.8\columnwidth]
%\begin{center}
%\begin{tabular}{|c|}
%\hline
%\includegraphics[width=0.55\columnwidth]{figures/exampleprompt.jpg}\\
%\hline
%\textbf{Response (L1)} \\
%\hline
%He is droning his wife pitcher. (Arabic)\\
%\hline
%The artist is drawing a pretty women. (Chinese) \\
%\hline
%The artist is painting a portrait of a lady. (English) \\
%\hline
%The painter is painting a woman's paint. (Spanish)\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Example item and NNS responses from previous study.}
%\label{fig:example-picture}
%\end{figure}

\begin{figure}[htb!]
%[width=0.8\columnwidth]
\begin{center}
\begin{tabular}{|c||c|}
\hline
\includegraphics[width=0.40\columnwidth]{figures/exampleprompt.jpg} & \includegraphics[width=0.40\columnwidth]{figures/exampleprompt2.jpg}\\
\hline
\textbf{Response (L1)} & \textbf{Response (L1)} \\
\hline
He is droning his wife pitcher. (Ar) & The man killing the beard. (Ar) \\
\hline
The artist is drawing a pretty women. (Ch) & A man is shutting a bird. (Ch) \\
\hline
The artist is painting a portrait of a lady. (En) & A man is shooting a bird. (En) \\
\hline
The painter is painting a woman's paint. (Sp) & The man shouted the bird. (Sp)\\
\hline
\end{tabular}
\end{center}
\caption{Example items and responses from previous study from native speakers of Arabic (Ar), Chinese (Ch), English (En) and Spanish (Sp).}
\label{fig:example-picture}
\end{figure}

The PDT consisted of 10 items (8 line drawings and 2 photographs) intended to elicit a single sentence each; an example is given in Figure~\ref{fig:example-picture}. Participants were asked to view the image and ``describe the action in either past or present tense.'' Responses were typed by the participants themselves in a computer lab with spell checking disabled.
I collected responses from 53 participants for a total of 530 sentences. There were 14 NSs (non-linguistics undergraduate and graduate students) and 39 NNSs (university students enrolled in English as a Second Language courses).

My process was to parse a NNS sentence into a dependency representation
%(section~\ref{sec:syntactic-form})
and then extract and lemmatize a semantic triple from this parse
%(section~\ref{sec:semantic-form})
to compare to a set of gold standard semantic triples similarly derived from the NS responses. As in my current research, I used the Stanford Parser for this task, trained on the Penn Treebank \citep{demarneffe:ea:06, klein:manning:03}.\footnote{\url{http://nlp.stanford.edu/software/lex-parser.shtml}} 



%\subsection{Obtaining a syntactic representation}
%\label{sec:syntactic-form}
%Because dependency parsing focuses on identifying dependency
%relations, rather than constituents or phrase structure, it clearly
%labels the subject, verb and object of a sentence, which can then map
%to a semantic form \citep{Kuebler.McDonald.Nivre-09}. In these experiments, I took a na\"ive approach in which subject, verb and
%object were considered sufficient for deciding whether or not a
%response accurately describes the visual prompt.

%
%Using the parser's options, I set the output to be Stanford typed
%dependencies, a set of labels for dependency relations. The Stanford
%parser has a variety of options for the specific
%ouput, e.g., how one wishes to treat prepositions
%\citep{defmarneffe:manning:12}.  I used only two non-default parser options ({\tt
%  CCPropagatedDependencies} and {\tt CCprocessed}\footnote{\url{http://nlp.stanford.edu/software/dependencies_manual.pdf}}) in order to:
%%
%1) omit prepositions and conjunctions from the sentence text and
%instead add the word to the dependency label between content words;
%and 2) propagate relations across conjunctions.  These decisions
%are important to consider for any semantically-informed processing of
%learner language.
%
%To see the impetus for removing prepositions, consider the learner
%example in Figure~\ref{fig:prep-dependency}, where the preposition \textit{with} is
%relatively unimportant to collecting the meaning.  Additionally,
%learners often omit, insert, or otherwise use the wrong preposition
%\citep{chodorow:et:al:07}.  The default parser would present a
%\texttt{prep} relation between \textit{played} and \textit{with},
%obscuring what the object is; with the options set as above, however,
%the dependency representation folds the preposition into the label
%(\texttt{prep\_with}), instead of keeping it in the parsed string, as
%shown in Figure~\ref{fig:prep-dependency}.
%
%\begin{figure}[htb!]
%\begin{center}
%    \begin{dependency}[arc edge,text only label,label style={above}]
%    \begin{deptext}[column sep=.5em]
%      \textit{vroot} \& The \&[1em] boy \&[1em] played \& with \& a \&[1em] ball \\
%    \end{deptext}
%    \depedge{4}{3}{nsubj}
%    \depedge[arc angle=90]{1}{4}{root}
%    \depedge{4}{7}{prep\_with}
%%    \depedge[arc angle=35,edge style={dotted}]{7}{6}{det}
%%    \depedge[edge style={dotted}]{3}{2}{det}
%    \depedge[arc angle=35]{7}{6}{det}
%    \depedge{3}{2}{det}
%  \end{dependency}
%\end{center}
%\caption{Dependency parse showing collapsed preposition dependencies.}
%\label{fig:prep-dependency}
%\end{figure}
%
%%\lk{it may be worthwhile to add the conll parse for this example so it's clear how these graphs come about}
%This is a lenient approach to prepositions, as prepositions
%are not without semantic meaning---e.g., \textit{the boy played in a
%  ball} means something quite different from the \textit{with} example.  However, this option makes it moderately easier to compare the meaning to an expected semantic form (e.g., \textit{play(boy,ball)}).
%
%As for propagating relations across conjunctions, this also simplifies the representation somewhat and makes it easier to connect verbs and their arguments, as needed for the semantic
%form used in comparisons. For a conjunction like \textit{cats and dogs}, for example, the default settings would produce \texttt{cc(cats, and)} and \texttt{conj(cats, dogs)}, but the chosen settings would collapse this into \texttt{conj\_and(cats, dogs)}, omitting the dependency that merely labels a conjunction relation between the first conjunct and the conjunction.

%Given the rule-based approach to matching \textit{verb(subject,object)} triples, many dependency relations are irrelevant for the next step of obtaining a semantic form.  For example, in this work I ignored determiner (\texttt{det}) relations between a noun and its determiner, allowing for variability in how a learner produces noun phrases. 

%\subsection{Obtaining a semantic representation}
%\label{sec:semantic-form}
%
%\subsubsection{Sentence types}

\begin{table*}[htb!]
\begin{center}
\begin{tabular}{|c|l|l|r|r|}
\hline
Type & Description & Example & NS & NNS \\
\hline
 A & Simple declar. trans. & The boy is kicking the ball. & 117 & 286 \\
 \hline
 B & Simple + preposition & The boy played with a ball. & 5 & 23 \\
 \hline
 C & No tensed verb & Girl driving bicycle. & 10 & 44 \\
 \hline
 D & No tensed verb + prep & Boy playing with a ball. & 0 & 1 \\
 \hline
 E & Intransitive (No object) & A woman is cycling. & 2 & 21 \\
 \hline
 F1 & Passive & An apple is being cut. & 4 & 2 \\
 \hline
 F2 & Passive with agent & A bird is shot by a man. & 0 & 6 \\
 \hline
 Ax & Existential A or C & There is a boy kicking a ball. & 0 & 0 \\
 \hline
 Bx & Existential B  or D & There was a boy playing with a ball. & 0 & 0 \\
 \hline
 Ex & Existential E & There is a woman cycling. & 0 & 0 \\
 \hline
 F1x & Existential F1 & There is an apple being cut. & 0 & 1 \\
 \hline
 F2x & Existential F2 & There is a bird being shot by a man. & 0 & 0 \\
 \hline
 Z & All other forms & The man is trying to hunt a bird. & 2 & 6 \\
 \hline
\end{tabular}
\end{center}
\caption{Sentence type examples, with distributions of types for
  native speakers (NS) and non-native speakers (NNS)}
\label{tab:sentence-type}
\end{table*}


I manually categorized the 530 sentences in the dataset into 11 types plus one catch-all category, as shown in
Table~\ref{tab:sentence-type}. I established these types because each
one corresponds to a basic sentence structure and thus has consistent
syntactic features, leading to predictable patterns in the dependency
parses. A sentence type indicates that the subject,
verb, and object can be found in a consistent place in the parse,
e.g., under a particular dependency label.
For simple transitive sentences (type \textit{A} in Table~\ref{tab:sentence-type}), for example, the words labeled {\tt nsubj}, {\tt root}, and {\tt dobj} 
pinpoint the necessary information.
Thus, the patterns for extracting semantic information---in the form
of \textit{verb(subj,obj)} triples---reference particular Stanford
typed dependency labels, part-of-speech (POS) tags, and locations
relative to word indices (see Figure~\ref{fig:conll}).


\begin{figure}[htb!]
\begin{center}
\begin{tabular}{|C{4em}|C{5em}|C{4em}|C{6em}|C{4em}|}
\hline
\textbf{Index} & \textbf{Dependent} & \textbf{POS} & \textbf{Head} & \textbf{Label} \\
\hline
\hline
%0 & \textit{root} & N/A & N/A & N/A \\
%\hline
1 & the & DET & 2 (boy) & det \\
\hline
2 & boy & NN & 4 (kicking) & nsubj \\
\hline
3 & is & VBZ & 4 (kicking) & aux \\
\hline
4 & kicking & VBG & 0 (\textit{root}) & root \\
\hline
5 & the & DT & 6 (ball) & det \\
\hline
6 & ball & NN & 4 (kicking) & dobj \\
\hline
\hline
    \multicolumn{5}{|c|}{\begin{dependency}[arc edge,text only label,label style={above}]
    \begin{deptext}[column sep=.5em]
      ROOT \& DET \&[1em] NN \& VBZ \&[1em] VBG \& DET \&[1em] NN \\
      \textit{root} \& The \&[1em] boy \& is \&[1em] kicking \& the \&[1em] ball \\
    \end{deptext}
    \depedge[arc angle=90]{5}{3}{nsubj}
    \depedge{5}{4}{aux}
    \depedge[arc angle=92]{1}{5}{root}
    \depedge[arc angle=80]{5}{7}{dobj}
%    \depedge[arc angle=35,edge style={dotted}]{7}{6}{det}
%    \depedge[edge style={dotted}]{3}{2}{det}
    \depedge[arc angle=35]{7}{6}{det}
    \depedge{3}{2}{det}
  \end{dependency}} \\
\hline
\end{tabular}
\end{center}
%\caption{The dependency parse of an example NNS response in CoNLL\footnote{Standard dependency parse format established by the Conference on Computational Natural Language Learning (CoNLL).} format and the corresponding visual representation.}
\caption{The dependency parse of an example NNS response in a standard format (CoNLL) and the corresponding visual representation.}
\label{fig:conll}
\end{figure}

%More complicated sentences or those containing common learner errors
%(e.g., omission of the copula \textit{be}) required slightly more
%complicated extraction rules, but, since this work examined only transitive
%verbs, these still boiled down to identifying the
%sentence type and extracting the appropriate triple.
Determining the sentence type is accomplished by arranging a small set of binary decisions into a tree, as shown in Figure~\ref{fig:decision-tree}. This decision tree checks for the presence of various dependency labels. The extraction rules for the particular sentence type are then applied to obtain the semantic triple. Finally, for each NNS response, the resulting triple was lemmatized and checked against the gold standard list of lemmatized NS triples. Ideally, each acceptable response should find a match, and unacceptable responses should not.

\begin{figure*}[htb!]
\begin{center}
\begin{tikzpicture}
\tikzset{level distance=3.5em}
\tikzset{edge from parent/.append style={->}}
\Tree
[.{\tt expl}?
  \edge node[auto=right,pos=.6,inner sep=1pt]{Y};
  [.{\tt auxpass}? 
  	\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
  	[.{\tt agent}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.F2x ]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.F1x ]
	]
	\edge node[auto=left,pos=.6,inner sep=1pt]{N};
	[.{\tt dobj}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.Ax ]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.{\tt prep\_}$\ast$?
			\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
			[.Bx ]
			\edge node[auto=left,pos=.6,inner sep=1pt]{N};
			[.Ex ]
		]
	]
  ]
  \edge node[auto=left,pos=.6,inner sep=1pt]{N};
  [.{\tt nsubjpass}? 
  	\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
  	[.{\tt agent}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.F2 ]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.F1 ]
	]
	\edge node[auto=left,pos=.6,inner sep=1pt]{N};
	[.{\tt dobj}? 
		\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
		[.{\tt nsubj}?
			\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
			[.A ]
			\edge node[auto=left,pos=.6,inner sep=1pt]{N};
			[.C ]
		]
		\edge node[auto=left,pos=.6,inner sep=1pt]{N};
		[.{\tt nsubj}?
			\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
			[.{\tt prep\_}$\ast$?
			 	\edge node[auto=right,pos=.6,inner sep=1pt]{Y};
				[.B ]
				\edge node[auto=left,pos=.6,inner sep=1pt]{N};
				[.E ]
			]
			\edge node[auto=left,pos=.6,inner sep=1pt]{N};
			[.D ]
			]
		]
	]
  ]
]
\end{tikzpicture}
\end{center}
\caption{Decision tree for determining sentence type and extracting semantic triple based on the presence of syntactic dependency labels}
\label{fig:decision-tree}
\end{figure*}

%To illustrate, consider the process for the example in Figure~\ref{fig:F2-dependency}.  The
%sentence is passed through the parser to obtain the dependency parse shown.
%The parsed sentence then moves to the
%decision tree shown in Figure~\ref{fig:decision-tree}.
%At the top of the tree, the sentence is checked for an {\tt expl}
%(expletive) label; having none, it moves rightward to the {\tt
%  nsubjpass} (noun subject, passive) node. Because a {\tt
%  nsubjpass} label is found, the sentence moves leftward to the {\tt agent}
%node. This label is also found, and because the sentence has reached a terminal node, it is labeled as a type F2 sentence.
%
%
%With the sentence now typed as F2, specific F2 extraction
%rules are applied. The logical subject is taken from under the {\tt agent} label,
%the verb from {\tt root}, and the logical object from {\tt nsubjpass},
%to obtain \textit{shot(man,bird)}, which can be lemmatized to
%\textit{shoot(man,bird)}. 
%%Very little effort goes into this process:
%%the parser is pre-built; the decision tree is small; and the
%%extraction rules are minimal.
%
%This much is possible with relatively little effort in part due to the constraints in the
%pictures.  For figure~\ref{fig:example-picture}, for example,
%\textit{the artist}, \textit{the man in the beret}, and \textit{the
%  man} are all acceptable subjects, whereas if there were multiple men
%in the picture, \textit{the man} would not be specific enough.
%%In future work, we expect to relax such constraints on image contents
%%by including rules to handle relative clauses, adjectives and other
%%modifiers in order to distinguish between references to similar
%%elements, e.g., 
%%\textit{a man shooting a bird} vs. \textit{a man reading the
%%  newspaper}.

\subsection{Rule-based matching results}
\label{sec:rule-results}

Evaluating this work required addressing two major questions.  First,
how accurately does this approach extract semantic information from potentially
innovative sentences?
%Due to the simple structures of the sentences (section~\ref{sec:sentence-distribution}), this simple system performs moderately well.
Second, how many semantic forms does one need in order to capture the variability in meaning in NNS sentences? I operationalized this second question by asking how well a given set of NS semantic forms models a gold standard.
% (section~\ref{sec:eval:coverage})?

An accurate extraction was defined as one in which the extraction rules chose the desired subject, verb, and object given the sentence at hand and without regard to the PDT image. Accuracy was 92.3\% for NNS responses and 92.9\% for NS responses. I attribute the high extraction scores to the constrained nature of the task and the relatively small range of sentence types it elicits. As seen in Table~\ref{tab:sentence-type}, only three sentence types account for more than 90\% of all responses. 

Assessing the coverage of NNS forms required first manually determining which extracted triples \textit{should} be matched given a hypothetical perfect gold standard set of triples. To separate the problem of coverage from extraction, I first removed any incorrectly extracted triples from the NNS set and the NS gold standard. I then labeled the remaining responses as ``appropriate'' or ``inappropriate.'' Note that this label applies to the response, not the triple. My annotation guidelines for this labeling were minimal: \textit{Given the prompt, would the response be acceptable to most English speakers?} 

I called an appropriate NNS triple found in the gold standard set a \textbf{true positive (TP)} (i.e., a correct match), and an appropriate NNS triple \textit{not found} in the gold standard set a \textbf{false negative (FN)} (i.e., an incorrect non-match), as shown in Table~\ref{tab:contingencies}. I used standard terminology here (TP, FN), but because this was an investigation of what \emph{should be} in the gold standard, these were considered
false negatives and not false positives.  To address the question of
how many NS sentences are needed to obtain good coverage, \textbf{coverage} was defined as recall: \textit{TP/(TP+FN)}. I reported 23.5\% coverage for unique triple
types and 51.0\% coverage for triple tokens.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|ll||l|l|}
  \hline
  & & \multicolumn{2}{c|}{NNS}\\
  & & $+$ & $-$ \\
  \hline
  \hline
  \multirow{2}{*}{NS} & Y & TP & FP \\
  \cline{2-4}
  & N & FN & TN\\
  \hline
\end{tabular}
\end{center}
\caption{Contingency table comparing presence of NS forms (Y/N) with
  appropriateness ($+$/$-$) of NNS forms}
\label{tab:contingencies}
\end{table}

I defined an inappropriate NNS triple (i.e., a content error)
\textit{not found} in the gold standard set as a \textbf{true negative
  (TN)} (i.e., a correct non-match). \textbf{Accuracy} based on this
gold standard---assuming perfect extraction---is defined as
\textit{(TP+TN)/(TP+TN+FN)}.\footnote{Accuracy is typically defined as (TP+TN)/(TP+TN+FN+FP), but false positives (FPs) are cases where an incorrect NNS response was in the gold standard; by removing errors from the NS responses, I prevented this scenario (i.e., FP=0).} I reported 46.4\% accuracy for types and 58.9\% accuracy for tokens.

The immediate lesson taken from this was: given a strict matching approach, NS data alone does not make for a sufficient gold standard, in that many appropriate NNS answers are not counted as correct. I explored expanding the set of NS triples by separating individual subjects, verbs and objects from NS triples and recombining them into the various possible combinations. However, this recombination generates a lot of nonsensical triples and degrades the gold standard. Consider, for example, \textit{do(woman,shirt)}---an incorrect triple derived from the correct NS triples, \textit{wash(woman,shirt)} and \textit{do(woman,laundry)}. This could be improved somewhat by evaluating new combinations with a language model, but this would both complicate the approach and diverge from my vision of content analysis driven by real speaker behavior. Instead, my current work has attempted to improve coverage by prompting NSs to give an initial PDT response, followed by a second alternative.

A related concern was that, even when only examining cases
where the meaning is literally correct, NNSs produced a wider range of
forms to describe the prompts than NSs. For example, for a picture
showing what NSs overwhelmingly described as a \textit{raking} action,
many NNSs referred to a man \textit{cleaning} an area. Literally,
this may be true, but it does not align with a NS gold standard. 
This behavior was expected, given that learners are encouraged
to use words they know to compensate for gaps in their vocabularies
\citep{AgustinLlach2010}. This also parallels the observation in SLA research that while second language learners may attain native-like grammar, their ability to use
pragmatically native-like language is often much lower
\citep{BardoviDornyei1998}. \lk{Work a D Stringer citation in here?}
These findings highlighted the need for a more flexible approach.
%that considers how native-like a sentence is as well as how appropriate its meaning is.

Moreover, evaluating this strict matching approach required an annotator to decide whether a given response is correct or incorrect. Partial matching is not allowed; this is an inherent weakness of the approach, because while a complete triple gives some indication of the meaning of the sentence, any single element of the triple taken alone does not provide enough context to indicate meaning. This inflexibility means that using this approach would effectively require the manual curation of a robust gold standard set of acceptable responses, which is counter to my goal of producing an approach that can be expanded to new PDT items simply by crowdsourcing a gold standard from NSs.

%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%Triple & Example sentence \\
%\hline
%\hline
%shoot(man, bird) & A man just shot a bird. \\
% \hline
%shoot(man, fowl) & The man shoots the fowl. \\
% \hline
%shoot(man, duck) & A man just shot a duck. \\
% \hline
%shoot(hunter, bird) & The hunter has shot a bird. \\
% \hline
%shoot(he, bird) & He shot the bird down! \\
% \hline
% \end{tabular}
%\end{center}
%\caption{The NS gold standard for Item 10.}
%\label{tab:item10GS}
%\end{table}

I followed up this work with a modification that included language models and spell checking tools to attempt to identify and fix misspellings that lead to downstream problems \citep{king:dickinson:14}. I omit this discussion because it is not applicable to the current work; I now take a simpler approach --- respondents use spell checking during the task. This is because in most contexts where my system would be used, like a language tutoring application or game, spelling instruction is not the objective, and a built-in spell checker would likely be available. Moreover, omitting this step removes a layer of analysis---and importantly, a potential source of errors---and allows the research to focus more directly on meaning.

%%%% END material from Qual Paper (BEA 2013) %%%%


%%%% BEGIN material from BEA 2016 %%%%
\section{Recent work: Generalized, similarity-based approaches}

%%%% 2020/05/21. Resume here. %%%% 

In subsequent work \citep{king:dickinson:16}, I began looking for a ``sweet spot'' of
semantic analysis \citep[cf.][]{bailey:meurers:08} for image-based learner productions. I applied new methods to the same dataset, and the current dissertation research applies a refined version of these methods to the new dataset discussed in Chapters~\ref{chap:data} and~\ref{chap:annotation}.
In particular, using available NLP tools, I moved away from discrete representations of correctness in the form of a gold standard set of semantic triples to a more continuous notion of correctness using a model comprised of smaller, overlapping pieces of information (section~\ref{sec:ranking}). This obviates the need for a rule-based extraction of semantic triples, which must be customized for a limited range of expected sentence types. It also allows for graded scoring of results, meaning that a response is not outright rejected because only one argument of a triple is not found. On the other hand, it means that the system does not provide a discrete ``acceptable'' or ``not acceptable'' decision, which means that downstream tasks like assessment, providing feedback or effecting game outcomes would require more careful consideration of what to do with the system output.

I should note, in this context, that I am discussing semantic
analysis given a gold standard set of NS sentences.  Image processing
tasks often rely on breaking images into semantic primitives
\citep[see, e.g.,][and references therein]{ortiz:wolff:lapata:15}, but
for NNS data, I want to ensure that I can account not just for
correct semantics (the \emph{what} of a picture), but natural
expressions of the semantics (the \emph{how} of expressing the
content).  In other words, the goal is to reason about meaning based on
specific linguistic forms.

A second issue regarding content analysis, beyond correctness, stems
from using an incomplete GS. The productive nature of language means that a sentence can be expressed in countless ways, and thus a GS can never really be ``complete.'' Examining the degree of this variability both for NSs and NNSs is necessary to determine whether a crowd-sourced gold standard can account for a sizable portion of test responses. Additionally, it can offer insights into theoretical research on variability in learner language (cf. \citet{ellis1987variability}, \citet{kanno1998consistency}). With regard to the current work, analyzing variability can also help determine the most effective parameters for an NLP system for image based responses. By applying a set of new approaches to my older data, I was able to select the best performing approaches for refinement and further study with the larger, current dataset.
 

%That is, different types of image content might require
%different mechanisms for processing.  Additionally, knowing how
%different pictures elicit different kinds of content can provide
%feedback on appropriate types of new data to collect.
%We approach this issue by clustering responses in various ways
%(section~\ref{sec:clustering}) and seeing how the clusters connect to
%system parameters.
%
%For both the experiments involving the accuracy of different system
%parameters (section~\ref{sec:ranking}) and the clustering of different
%responses (section~\ref{sec:clustering}), we present results within
%those sections that show the promise of moving to abstract representations, but in
%different ways for different kinds of data.
%
%We build directly from \citet{king:dickinson:13,king:dickinson:14},
%where the method to obtain a semantic form from a NNS production is:
%1) obtain a syntactic dependency representation from the off-the-shelf
%Stanford Parser \citep{demarneffe:ea:06, klein:manning:03}, and 2)
%obtain a semantic form from the parse, via a small set of hand-written
%rules.  It is this method we attempt to generalize
%(section~\ref{sec:ranking}).

%\section{Data Collection}
%\label{sec:data}
%
%Because our approach requires both NS and NNS responses and
%necessitates constraining both the form and content of responses, we
%previously assembled a small corpus of NS and NNS responses to a PDT
%\citep{king:dickinson:13}.  Research in SLA often relies on the
%ability of task design to induce particular linguistic behavior
%\citep{skehan1998assessing}, and the PDT should induce context-focused
%communicative behavior.  Moreover, the use of the PDT as a reliable
%language research tool is well-established in areas of study ranging
%from SLA to Alzheimer's disease \citep{ellis2000task,
%forbes2005detecting}.

%
%The PDT consists of 10 items (8 line drawings and 2 photographs\footnote{We have not observed substantial differences between responses for the drawings and the photographs.}) intended to elicit a single sentence
%each; an example is given in Figure~\ref{fig:example-picture}. Participants
%were asked to view the image and describe the action in past or present tense.
%The data consist of responses from 53 informants (14 NSs, 39 NNSs),
%for a total of 530 sentences, with the NNSs being intermediate and
%upper-level adult English learners in an intensive English as a Second
%Language program.  The distribution of first languages (L1s) is: 14
%English, 16 Arabic, 7 Chinese, 2 Japanese, 4 Korean, 1 Kurdish, 1
%Polish, 2 Portuguese, and 6 Spanish.
%
%Responses were typed by the participants themselves, with spell checking disabled in some cases.  Even among the NNSs that used spell checking, a number of spelling errors resulted in real words. To address this, we use a spelling correction tool to obtain candidate spellings for each word, prune the candidates using word lists from the NS responses, recombine candidate spellings into candidate sentences, and evaluate these with a trigram language model (LM) to select the most likely intended response \citep{king:dickinson:14}.
%
%Once the responses had been collected, the NNS responses were
%annotated for correctness, with respect to the content of the picture.
%The lead author marked spelling and meaning errors which prevent a
%complete mapping to correct information
%\citep[see][]{king:dickinson:13}.  On the one hand, minor misspellings
%are counted as incorrect (e.g., \textit{The artiest is drawing a
%\textbf{portret}}), while, on the other hand, the annotation does
%not require distinguishing between between spelling and meaning
%errors.  In the future, we plan on fine-tuning the annotation
%criteria.

\subsection{Generalizing the methods}
\label{sec:ranking}

The previous work assumed that the assessment of NNS responses
involves determining whether a NS gold standard set contains the same
semantic triple that the NNS produced, i.e., whether a triple
is \textit{covered} or \textit{non-covered}.  In such a situation the
gold standard need only be comprised of \textit{types} (not \textit{tokens}) of semantic triples. Here the gold standard is comprised of the small set of NS responses---only 14. This means that exact matching is going to miss many cases,
and indeed as discussed in Section~\ref{sec:rule-results}, coverage was only at 51\%. Even with a much larger gold standard, we can expect responses to follow Zipf's law; a sample of language data will always be incomplete because it will not contain all ``long tail,'' low-frequency phenomena.

Additionally, relying on matching of triples limits the utility of the method to specific semantic constraints, namely transitive sentences. By dropping the exact matching approach and instead comparing the frequencies of elements in the NNS response with those of the gold standard, I moved into a gradable, or ranking, approach to the analysis. For this reason, I shift terminology here from NS \textit{\textbf{gold standard}} to NS \textit{\textbf{model}}. A gold standard is roughly akin to an answer key, which is appropriate for my triple-matching approach. A model is typically a richer data structure containing statistics from observations of known correct data. This distinction and its relevance will be more apparent with the discussion of response representations in Section~\ref{sec:representation}.

My goal is to emphasize the degree to which a NNS response conveys the same
meaning as the set of NS responses, necessitating an approach which can automatically
determine the importance of a piece of information among the set of NS responses.  This required two major decisions: 1) how to \textbf{represent} each response as a set of sub-elements, and 2) how exactly to \textbf{score} these sub-elements via comparison with the NS data. In Section~\ref{sec:representation}, I detail how I represented the information and in Section~\ref{sec:scoring}, I discuss comparing NNS information to NS information, which allowed me to rank responses from most to least similar to the NS model. In Section~\ref{sec:parameters}, I outline the system parameters that I combine to generate scores, and in Section~\ref{sec:metrics} I present and interpret the results of the various settings.
%to rank responses from least to most similar to the NS gold standard.
%\footnote{Although rankings often go from highest to lowest, I prioritize identifying problematic cases, so I rank accordingly.}
%I also discuss the handling of various other system parameters (section~\ref{sec:parameters}).

%This work used the same 10 item PDT dataset described in section~\ref{sec:first-approaches}. Another example is shown in Figure~\ref{fig:example-picture2}.
%
%\begin{figure}
%\begin{center}
%\begin{tabular}{|c|}
%\hline
%\includegraphics[width=0.7\columnwidth]{figures/exampleprompt2.jpg}\\
%\hline
%\textbf{Response (L1)} \\
%\hline
%The man killing the beard. (Arabic)\\
%\hline
%A man is shutting a bird. (Chinese) \\
%\hline
%A man is shooting a bird. (English) \\
%\hline
%The man shouted the bird. (Spanish)\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Example item and responses}
%\label{fig:example-picture2}
%\end{figure}
%
\subsection{Representing responses}
\label{sec:representation}

%To overcome the limitations of an incomplete GS,
I first represented each NNS response as a list of dependencies taken directly from the parse. 
%\citep{demarneffe:ea:06}, the terms referring to
%%being either 
%individual dependencies (i.e., relations between words).
%or individual words. 
This eliminates the complications of extracting semantic triples from
dependency parses, which only handled a very restricted set of
sentence patterns and resulted in errors in 7--8\% of cases, as discussed in Section~\ref{sec:rule-results}.
Operating directly on individual dependencies from the overall tree also means the system can allow for ``partial credit;'' it distributes the matching over smaller,
overlapping pieces of information rather than a single, highly specific triple. The words within the dependencies were lemmatized using the pre-trained Stanford CoreNLP lemmatizer (CITE XYZ) \lk{XYZ}. Lemmatizing was used here to minimize data sparsity. For example, the main verb in the forms \textit{kicks}, \textit{kicked}, \textit{has kicked} and \textit{is kicking} was reduced to \textit{kick} in all cases, increasing the likelihood of finding matches.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|C{7em}||C{6em}||C{5.5em}||C{5.5em}||C{5em}|}
\hline
\multicolumn{5}{|c|}{The boy is kicking the ball.} \\
\hline
\hline
\textbf{ldh} (labeled) & \textbf{xdh} (unlab.) & \textbf{lxh} & \textbf{ldx} & \textbf{xdx} (word)\\
\hline
\hline
det(the,boy) & \textit{x}(the,boy) & det(\textit{x},boy) & det(the,\textit{x}) & \textit{x}(the,\textit{x}) \\
\hline
nsubj(boy,kick) & \textit{x}(boy,kick) & nsubj(\textit{x},kick) & nsubj(boy,\textit{x}) &  \textit{x}(boy,\textit{x}) \\
\hline
aux(is,kick) & \textit{x}(is,kick) & aux(\textit{x},kick) & aux(is,\textit{x}) & \textit{x}(is,\textit{x}) \\
\hline
root(kick,root) & \textit{x}(kick,root) & root(\textit{x},root) & root(kick,\textit{x}) & \textit{x}(kick,\textit{x}) \\
\hline
det(the,ball) & \textit{x}(the,ball) & det(\textit{x},ball) & det(the,\textit{x}) & \textit{x}(the,\textit{x}) \\
\hline
dobj(ball,kick) & \textit{x}(ball,kick) & dobj(\textit{x},kick) & dobj(ball,\textit{x}) & \textit{x}(ball,\textit{x}) \\
\hline
\end{tabular}
\end{center}
%\caption{The dependency parse of an example NNS response in CoNLL\footnote{Standard dependency parse format established by the Conference on Computational Natural Language Learning (CoNLL).} format and the corresponding visual representation.}
\caption{Given the example sentence above, the updated approach represents responses in the dependency formats shown: ldh (for \textit{label}, \textit{head}, \textit{dependent}; i.e., labeled dependencies)), xdh (unlabeled dependencies), lxh (label+head), ldx(label+dependent), or xdx (word, or more technically, \textit{dependent}).}
\label{tab:dep-rep}
\end{table}

Next, I obtained five different representations from the lemmatized dependencies, as shown in Table~\ref{tab:dep-rep}. I refer to this variable as \textbf{term representation}, in keeping with \textit{term} frequency-inverse document frequency, discussed in Section~\ref{sec:scoring}.
%I first tokenized and lemmatized a response to a list of dependencies that represents the response.
The five term representations are then variations on dependencies. The
full form is the \textbf{labeled dependency} and includes the label, dependent and head, so I refer to it as \textbf{ldh}. The remaining four forms abstract over either the label, dependent and/or head. I refer to these forms as \textbf{xdh} (i.e., unlabeled dependency), \textbf{lxh} (label+head), \textbf{ldx} (label+dependent), and \textbf{xdx} (dependent only, roughly equivalent to \textit{word} in a bag-of-words approach).

The goal in choosing these five representations was to find the optimal combination of dependency features and the right level of detail to obtain the best system performance. The bag-of-words representation was implemented largely to provide a baseline by which to compare the others.

This processing was applied to the collection of NS responses as well. For each item, the dependencies from all NS responses was pooled into a single flat list---a ``bag-of-dependencies.'' From this list, a copy in each of the five term representations was produced, allowing for comparison with the corresponding NNS data.
%I tested the system performance using each of these term representations separately.

%The \param{xdx} model is on a par with treating the sentence as a ``bag
%of words'' (or more accurately, a bag of lemmas), except that some function words not receiving parses (e.g., prepositions) are not included (see section~\ref{sec:syntactic-form}).

\subsection{Scoring responses}
\label{sec:scoring}

Taking the five term representations, my next
step was to score them in a way which ranks responses from most to
least appropriate.  I devised four scoring approaches, each
using one of two methods to \textbf{weight} response terms combined
with one of two methods to \textbf{compare} the weighted NNS terms
with the NS data.

For weighting, I used either a simple relative frequency measure
or one based on \textit{term frequency-inverse document frequency} (\textit{tf-idf})
\citep[][ch. 6]{manning-et-al:08}. Most commonly, a \textit{term} for tf-idf purposes would be a \textit{word}. However, \textit{term} here refers to a single syntactic dependency, and this is a central conceit of both the study in reference and the larger dissertation:  the dependency, which captures aspects of semantic and syntactic relationships, is an ideal atomic unit for evaluating meaning by comparing distributions in crowdsourced data. As discussed, the dependencies were represented in one of the five term representations---some combination of label, dependent and head. For the NNS data, each response was treated as a document. For the NS data, the entire collection of NS responses was treated as one document. The relative frequency measure was simply the token count of a given term in a document normalized by the total count of tokens in the document.

I used tf-idf as a measure of a term's importance with the expectation that it would reduce the impact
of semantically unimportant terms---e.g., determiners like
\textit{the}, frequent in the responses, but mostly unimportant for evaluating the
semantic contents---and to upweight terms which may
be salient but infrequent, e.g., only used in a handful of NS
sentences. For example, for an item depicting a man shooting a bird
(see Table~\ref{tab:i10responses-avgprec} and Figure~\ref{fig:example-picture}), of 14 NS responses, 12 described the subject as \textit{man}, one as \textit{he} and one as
\textit{hunter}. Since \textit{hunter} is relatively infrequent in English, even
one instance in the NS responses should get upweighted via tf-idf, and indeed it
that was the effect; in the bag-of-words approach, the term \textit{hunter} is weighted among the highest, and the same is true for dependencies containing the word \textit{hunter} among the other term representations. This is valuable, as numerous NNS responses used \textit{hunter}.

Calculating tf-idf relies on both \emph{term frequency} ($tf$) and
\emph{inverse document frequency} ($idf$).  Term frequency is simply
the raw count of a term within a document. Inverse document frequency is derived from some reference corpus of documents, and it is based on the notion that appearing in more documents makes a term less informative with respect
to distinguishing between documents.  The formula is in
(\ref{ex:tfidf}) for a term $t$, where $N$ is the number of documents
in the reference corpus, and $df_{t}$ is the number of documents
featuring the term ($idf_{t} = \log \frac{N}{df_{t}}$).  A term
appearing in fewer documents will thus obtain a higher $idf$ weight,
and this should readjust frequencies based on semantic importance.

\begin{exe}
\ex\label{ex:tfidf} $tfidf(t) = tf_{GS} \log \frac{N}{df_{t}}$
%, where $df_t = |\{d\in D, t \in d\}|$
\end{exe}

After this frequency counting or tf-idf weighting, the scores were then either
\textbf{averaged} to yield a response score, or NNS term
weights and NS term weights were treated as vectors and the response
score was the \textbf{cosine distance} between them.  This
yields the four approaches:

%%former approach names: b = FA; m = IC (TC); c = FC; a = IA (TA)
\paragraph{Frequency Average (FA).} 
%This approach serves as our baseline. 
Within the set of NS responses, the relative frequency of each term is calculated. The NS \textit{model} here is simply each NS term and its relative frequency. Each term in
the NNS response is then given a score equal to its frequency in the
NS model; terms missing from the NS model are scored zero. The response score is
the average of the term scores, with higher scores closer to the NS model.

\paragraph{Tf-idf Average (TA).} This involves the same
averaging of term scores as with approach FA, but here the term scores are the tf-idf scores. The NS model here is thus each NS term and its tf-idf score. 

\paragraph{Frequency Cosine (FC).} Each term score is taken as its relative frequency 
calculated within its document: either the NS response set or the single NNS response. The NS model is then the set of all NS terms and their scores. The term scores are then treated as vectors---one vector of the NS term scores (i.e., the NS model here)---and one vector of the NNS term scores. Each vector is an ordered list of term scores for each term observed in either the NS document or the NNS document. In other words, each vector represents term scores for the sorted union set of NS and NNS terms. Naturally, many of the term scores are zero for the much shorter NNS document. The response score is the cosine distance between the vectors, with lower scores being closer to the NS model.

\paragraph{Tf-idf Cosine (TC).} This involves the same distance comparison as with approach FC, but now the term scores in the vectors are tf-idf scores. The NS model here is thus the vector representing the union set of terms for the NS and NNS document, populated with the tf-idf scores from the NS document.

\bigskip

%The two cosine approaches are effectively primitive versions of sentence encoders like the currently popular BERT \citep{BertDevlin2018} and Universal Sentence Encoder \citep{UniversalSentenceEncoder}. Sentence encoders are a form of language model that learns mathematical representations of words by observing them in context, accounting for things like average distance from a given word type to another given word type. Sentence encodings are thus vectors representing these word values for a full sentence. These approaches result in very high dimensional spaces---imagine a sentence representation that consists of a vector for each word in the sentence, where each vector is a list of average distances from that word type to \textit{every other word type in the language}. Thus sentence encoders typically rely on methods of dimensionality reduction to compress these representations into vectors of manageable length.
%I say my cosine approaches constitute ``primitive'' encoders because they omit this step.
In many natural language processing scenarios where high dimensional vectors are involved, such as sentence encoders or word embeddings, methods for dimensionality reduction are employed (\citet{BertDevlin2018}, \citet{word2vec}). This improves efficiency by reducing the storage and computing power needed. In my FC and TC approaches, however, the number of word types (and in turn term types) remained small enough that the raw vectors representing dependencies' tf-idf scores can be processed easily with an ordinary PC. Not only does this simplify the task, it means that the process remains transparent. There are no transformers or attention mechanisms to produce compressed and unexplainable representations.  If desired, each sentence vector can be examined value by value, where each number maps to a real syntactic dependency. This is important because it leaves the door open for meaningful feedback on each response. For example, one might choose to identify the most salient dependencies in the NS model and use them to guide an ICALL user from a low scoring response to a better response.
%That is to say, a full encoder could determine how similar a response is to a GS, but it could not tell us \textit{why} it made its determination.

\subsection{System Parameters}
\label{sec:parameters}

I ran a total of 30 experiments, with each representing a combination of these system parameters for processing responses (see also Table~\ref{tab:dist-ranked-parameters}): 

\paragraph{Term representation} As discussed in
section~\ref{sec:representation}, the terms can take one of five
representations: \param{ldh}, \param{xdh}, \param{lxh}, \param{ldx},
or \param{xdx}.

\paragraph{Scoring approach.} As discussed in
section~\ref{sec:scoring}, the NNS responses can be
compared with the NS data via approaches \param{FA}, \param{TA}, \param{FC}, or \param{TC}.

\paragraph{Reference corpus.} The reference corpus for deriving tf-idf
scores can be either the Brown Corpus \citep{kucera:francis:67} or the
Wall Street Journal (WSJ) Corpus \citep{marcus-et-al:93}. These are
abbreviated as \param{B} and \param{W} in the results
below; \param{na} indicates the lack of a reference corpus, as this is
only relevant to approaches \param{TA} and
\param{TC}. The corpora are divided into as many documents as
originally distributed (\param{W}: 1640, \param{B}: 499). The WSJ is
larger, but Brown has the benefit of containing more balance in its
genres (vs. newstext only). Considering the narrative nature of PDT
responses, a reference corpus of narrative texts would be ideal, but as no such reliably parsed corpus is available, I chose the widely used, pre-parsed Brown and WSJ corpora. The corpora were converted from their standard dependency parse format to each of the five term representations used in order to be compatible with the NS and NNS data for tf-idf.

%\paragraph{NNS source.} Each response has an original version
%(\param{NNSO}) and the output of a language model spelling corrector
%(\param{NNSLM}). (The current dissertation relies on a corpus for which participants used spell checking at the time of the task, so this offline spelling correction is no longer applicable. In short, it used a spelling tool to find candidate spellings for each word in a NNS sentence, pruned the lists of candidate words by comparing against words in NS responses, formed new candidate sentences by combining candidate words, and finally chose the most likely sentence by rating each candidate with a trigram word model. I omit the exact details here for brevity, but more can be found in \cite{king:dickinson:14}).
%
\subsection{Experiments and Results}
\subsubsection{Evaluation metrics}
\label{sec:metrics}

%I ran 60 response experiments, each with different system settings
I ran 30 response experiments, each with different system settings
(section~\ref{sec:parameters}). Within each experiment, I ranked the 39
scored NNS responses from least to most similar to the NS model.

For assessing these settings themselves, I relied on the past annotation, which counted unacceptable responses as errors (see section~\ref{sec:rule-results}).  As the lowest numerical rank indicates the greatest distance from the NS model, a good system setting should position the unacceptable responses among those with the lowest rankings.
%Thus, I assigned each error-containing
%response a score equal to its rank (or the average rank in the case of tied responses).
To evaluate this discriminatory power, I used \textbf{(mean) average precision ((M)AP)}
\citep[][ch. 8]{manning-et-al:08}.

\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{0.3em}
\begin{tabular}{|r|c|l|c|}
\hline
Rank & Score & Sentence & Error \\
\hline
\hline
\multirow{2}{*}{1} & 1.000 & she is hurting. & 1 \\
& 1.000 & man mull bird & 1 \\
\hline
3 & 0.996 & the man is hurting duck. & 1 \\
4 & 0.990 & he is hurting the bird. & 1 \\
\hline
11 & 0.865 & the man is trying to hurt a bird & 1 \\
12 & 0.856 & a man hunted a bird. & 0 \\
\hline
17 & 0.775 & the bird not shot dead.  & 1 \\
18 & 0.706 & he shot at the bird & 0 \\
19 & 0.669 & a bird is shot by a un & 1 \\
20 & 0.646 & the old man shooting the birds & 0 \\
\hline
37 & 0.086 & the old man shot a bird. & 0 \\
38 & 0.084 & a old man shot a bird. & 0 \\
39 & 0.058 & a man shot a bird & 0 \\
\hline
\hline
\multicolumn{4}{|c|}{Average Precision: 0.75084} \\
\hline
\end{tabular}
\caption{Example item rankings for the best system setting (\param{TC}, \param{Brown} Corpus, and labeled dependencies (\param{ldh}) based on average precision scores. Note that not all 39 responses are shown.}
\label{tab:i10responses-avgprec}
\end{center}
\end{table}

For average precision (AP), one calculates the precision of error
detection at every point in the ranking, lowest to highest. Consider
Table~\ref{tab:i10responses-avgprec}, which presents an excerpt of ranked sentence
responses for one PDT item. The precision for
the first cut-off (1.000) is 1.0, as two responses have been
identified, and both are errors ($\frac{2}{2}$). At the 11th- and
12-ranked response, precision is 1.0 (=$\frac{11}{11}$) and 0.917
(=$\frac{11}{12}$), respectively, precision dropping when the item is
not an error.
AP averages over the precisions for all $m$ responses ($m=39$ for the
NNS data), as shown in (\ref{ex:ap}), with each response notated as
$R_k$.  Averaging over all 10 items results in the Mean AP (MAP).

%In Table~\ref{tab:i10responses-avgprec}, an excerpt of sentence
%responses is shown for one item, ranked from lowest to highest.  To
%take one example, the third-ranked sentence, \textit{the man is hurting duck}, has a score of 0.996, and it is annotated as an error (1 in
%the \textit{E} column). 
%Thus, the evaluation metric adds a score of 3
%to the overall sum.  The sentence ranked 18, by contrast, is not an
%error, and so nothing is added.  In the case of the top rank, two
%responses with errors are tied, covering rank 1 and 2, so each adds a score of 1.5.
%
%The sum of these scores is taken as the \textbf{Raw} metric for that
%experimental setting. In many cases, one version of a response
%(\param{NNSO} or \param{NNSLM}) contains an error, but the other
%version does not. Thus, for example, an \param{NNSO} experiment may
%result in a higher error count than the \param{NNSLM} equivalent, and
%in turn a higher Raw score.
%In this sense, Raw scores emphasize error reduction and incorporate
%item difficulty.
%
%However, it is possible that the \param{NNSO} experiment, even with
%its higher error count and Raw score, does a better job ranking the
%responses in a way that separates good and erroneous ones.

\begin{exe}
\ex\label{ex:ap} $AP(item) = \frac{1}{m} \sum\limits_{k=1}^m
Precision(R_k)$
\end{exe}

%As mentioned, the Raw metric emphasizes error reduction, as it
%reflects not just performance on identifying errors, but also the
%effect of the overall number of errors.
%In this way, it may be useful
%for predicting future system performance, an issue we explore in the
%evaluation of clustering items (section~\ref{sec:clusteringresults}).
%MAP, on the other hand, emphasizes finding the optimal separation
%between errors and non-errors and is thus more of the focus in the
%evaluation of the best system parameters next.

\subsubsection{Best system parameters} 

To start the search for the best system parameters, it may help to
continue with the example in
Table~\ref{tab:i10responses-avgprec}. The best setting, as determined by the
MAP metric, uses the tf-idf cosine (\param{TC}) approach with the Brown Corpus (\param{B}), and the full form of the labeled dependencies (\param{ldh}). It ranks highest because errors are
well separated from non-errors; the highest ranked of 17 total errors
is at rank 19.  Digging a bit deeper, one can see in this example how
the verb \textit{shoot} is common in all the highest-ranked cases shown
(\#37--39), but absent from all the lowest, showing both the effect of
the NS model (as all NSs used \textit{shoot} to describe the action) and the
potential importance of even simple representations like lemmas.  In
this case, the labeled dependency (\param{ldh}) representation is best, likely because the
word \textit{shoot} is not only important by itself, but also in terms
of which words it relates to, and how it relates (e.g.,
\textit{dobj(bird,shoot)}).

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|r|l|c|c|c|}
\hline
Rank & MAP & Approach & Reference & Term rep.\\
\hline
\hline
1 & 0.5168 & TC & Brown & ldh \\
\hline
2 & 0.5128 & TC & WSJ & ldh \\
\hline
3 & 0.5124 & TC & Brown & xdh \\
\hline
4 & 0.5109 & TC & Brown & lxh \\
\hline
5 & 0.5102 & TC & WSJ & xdh \\
\hline
\hline
26 & 0.4826 & FA & \textit{na} & ldx \\
\hline
27 & 0.4816 & TA & Brown & xdx \\ 
\hline
28 & 0.4769 & FC & \textit{na} & lxh \\ 
\hline
29 & 0.4721 & TA & WSJ & xdx \\
\hline
30 & 0.4530 & FA & \textit{na} & lxh \\ 
\hline
\end{tabular}
\caption{Based on Mean Average Precision, the five best and five worst combinations of system parameters across all 10 PDT items.}
\label{tab:all-dist-ranked-settings}
\end{center}
\end{table}

Table~\ref{tab:all-dist-ranked-settings} shows the five best and five
worst combinations of system settings averaged across all 10 PDT items, as ranked by
MAP. The table clearly indicates that the TC approach is superior, occurring in all of the top five combinations, while approaches FA and FC compete for worst. The Brown Corpus appears among top scoring combinations than does the WSJ Corpus. Finally, for the term representation, labeled (ldh) and unlabeled (xdh) dependencies score are used among the top scoring combinations.

I also summarize the rankings for the individual parameter classes, presented in Table~\ref{tab:dist-ranked-parameters}. For a given parameter, e.g., \param{ldh}, I averaged the scores from all settings including \param{ldh} across all 10 items. Generally, the same trends appear salient. Notably, \param{TC} outperformed the other models, with \param{FC} and \param{TA} close behind (and nearly tied). Performance fell for the simplest model, \param{FA}, which was in fact intended as a baseline. With \param{\textbf{T}C}$>$\param{\textbf{F}C} and \param{\textbf{T}A}$>$\param{\textbf{F}A}, tf-idf weighting seems preferable to basic frequencies. Likewise, with \param{T\textbf{C}}$>$\param{T\textbf{A}} and \param{F\textbf{C}}$>$\param{F\textbf{A}}, for my term based scoring, comparing score vectors outperformed simply comparing score averages.

\begin{table*}
\begin{center}
\begin{tabular}{|l|r||l|r||l|r|}
\hline
\multicolumn{2}{|c||}{Approach} & \multicolumn{2}{|c||}{Term rep.} & \multicolumn{2}{|c|}{Ref. corpus} \\
\hline
\hline
0.50630 & TC &0.50499 & ldh & 0.50461 & Brown \\
\hline
0.49609 & TA & 0.50405 & xdh & 0.49777 & WSJ \\
\hline
0.49471 & FC & 0.49287 & ldx & & \\
\hline
0.48247 & FA & 0.49190 & xdx & & \\
\hline
 & & 0.49115 & lxh & & \\
\hline
\end{tabular}
\caption{Individual system parameters ranked by Mean Average Precision for all 10 PDT items.}
\label{tab:dist-ranked-parameters}
\end{center}
\end{table*}

These findings largely confirmed my expectations. The TC approach was intended to evaluate responses by focusing comparison on the most salient content. The scores here prove that to be successful. Regarding the top term representations, labeled and unlabeled dependencies both capture the relationship between dependents and their heads, making them an ideal unit for analyzing ``who did what to whom'' in the context of a PDT. Finally, given the subject matter and narrative style of the task, it is unsurprising that the Brown Corpus serves as a better reference corpus than the WSJ Corpus for tf-idf.

%Despite the strength of these overall trends, variability
%does exist among the best settings for different items, a point obscured
%in the averages.  In Tables~\ref{tab:i01-dist-ranked-settings} and
%\ref{tab:i05-dist-ranked-settings}, I present the best and worst
%ranked settings for two of the least similar items, 1 and 5.
%Their dissimilarity can be seen at a glance, simply from the range of
%the AP scores (0.05--0.31 for item 1 vs. 0.52--0.81 for item 5), which
%in itself reflects a differing number of erroneous responses (2 [\param{NNSO}]
%or 6 [\param{NNSLM}] for item 1 vs. 23 or 24 for item 5).
%
%For item 1, a drawing of a boy kicking a ball, there is considerable
%variability in the best scoring approach just within the top five settings:
%all four approaches (TA, TC, FA, FC) are in the top five.  Contrary to the overall
%trends, I also found the \param{ldx} form---without any head
%information---in the two best settings.  Note also that, even though
%tf-idf weighting (\param{TA}/\param{TC}) is usually among the best settings, it is
%occurs among the worst settings, too.
%
%For item 5 in Table~\ref{tab:i05-dist-ranked-settings}, a drawing of a
%man raking leaves, the most noticeable difference is that
%of \param{xdx} being among three of the top five settings.
%I believe that part of the reason for
%the higher performance of \param{xdx} (cf. lemmas), is that for this
%item, all the NSs use the verb \textit{rake}, while none of the NNSs use this word.  For item 1 (the boy kicking a ball), there is lexical variation
%for both NSs and NNSs.

%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|r|l|c|}
%\hline
%Rank & MAP & Settings \\
%\hline
%\hline
%1 &  &  \\
%\hline
%2 &  &  \\
%\hline
%3 &  &  \\
%\hline
%4 &  &  \\
%\hline
%5 &  &  \\
%\hline
%\hline
%26 &  &  \\
%\hline
%27 &  &  \\
%\hline
%28 &  &  \\
%\hline
%29 &  &  \\
%\hline
%30 &  &  \\
%\hline
%\end{tabular}
%\caption{Based on Average Precision, the five best and five worst settings for item 10, shown in Figure~\ref{fig:example-picture} and Table~\ref{tab:i10responses-avgprec}.}
%\label{tab:i10-dist-ranked-settings}
%\end{center}
%\end{table}

\section{Current method}

The current method relies on a much larger corpus with more detailed annotation. As discussed in Chapter 4, the annotations do not encode for errors, as before, but instead give a binary score for five different features. This means the evaluation cannot take the form of MAP, but must instead compare response rankings, i.e., how well does the system rank responses in comparison to an ideal ranking based on manual annotations? Spearman rank correlation is used to provide these scores.

Because the annotations and evaluation are much different in the current work, it does not exactly follow that findings from the previous work will hold true. However, I believe that the previous work has highlighted some of the system parameters that are most likely to perform well, and I choose to experiment among the top contenders. For example, all of the current experiments rely on the tf-idf cosine (TC) approach, as this consistently outperformed the others. The frequency average (FA) approach is used \lk{Is it?} as a baseline where appropriate, as this presents a very simplistic distributional approach as opposed to the more linguistically sophisticated TC. The large increase in the size of the datasets also means that running an exhaustive search for the best parameters among all combinations is not reasonable; as discussed, the previous work used 60 different system settings in total. For that reason, I have chosen to focus on experiments that optimize for each parameter individually, then combine the best parameter settings for use on held out data to ensure they perform optimally. These experiments and their results are discussed in Chapter 6.

%TODO:
%This section should briefly summarize the takeaways from the chapter and explain which of the settings described here are used in the current experiments.
%Note that the current data is not annotated for errors, which means the current experiments rely on Spearman rank correlation scores, not MAP scores. This means I can't assume that the best settings found in 2016 will apply, but I should argue that the 2016 results do show some ability to discriminate as desired. and after all --- errors don't appear in the GS, and thus for our purposes they aren't native-like, and TC especially does seem to perform well here. Generally, the top settings seem fairly unsurprising and intuitive.
%This leads directly into the next chapter (Experiments), which contains a section for each experiment, which is basically varying a given parameter (ldh, xdh, xdx), and the results for each experiment.
%
%Question:
%Do I need some kind of baseline to compare to? Perhaps Frequency Average (FA)?