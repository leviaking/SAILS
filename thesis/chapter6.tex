%\chapter{Method}
%\label{chap:method}
%\section{Introduction}
\chapter{Optimization}
\label{chap:optimization}

In this chapter, I seek to answer questions surrounding the quality of the dataset and its annotation scheme, the feasibility of implementing a transparent system based on dependency and term frequency-inverse document frequency (tf-idf) for approximating manual annotations, and metrics for evaluating my approach. I see this work as a proof-of-concept, where finding consistent, exploitable trends and correlations validates the overall project. The findings here are presented not as hard and fast universal truths, but as indications of the utility of the annotations  and when and where such a ranking system can find success.

The work here consisted of experiments aimed at optimizing the configuration of my system for rating and ranking responses automatically, first introduced as a pilot study in Chapter~\ref{chap:pilot}. In that study, I began with a pipeline composed of a lemmatizer and dependency parser, and custom rules for extracting \textit{verb-subject-object} triples; a non-native speaker (NNS) triple that found a match in the native speaker (NS) set was considered \textit{correct} or \textit{covered}, and manual \textit{correct/incorrect} annotations were used to evaluate this output. This found limited success, and I abandoned the custom extraction rules. Instead, my revised approach, introduced in Section~\ref{sec:2016work} and used throughout this chapter, involved scoring all dependencies with tf-idf via a reference corpus, then vectorizing these dependency scores and calculating the cosine distance between an NS response vector and the NNS response collection vector. In Section~\ref{sec:current-method}, I present a closer look at the updates to this approach used here to process the new, larger dataset described in Chapter~\ref{chap:data}.

Optimizing this approach meant using my system with various settings to produce response rankings, then examining how these system rankings correlate with an annotation-based benchmark ranking to find which system settings work best. Specifically, I looked for correlations between the performance of my system (and its particular settings) and known features of the native speaker (NS) and non-native speaker (NNS) responses, such as the transitivity of the PDT item event, the size of the NS model, and whether models contained only primary responses or a mix of primary and secondary responses. I also considered correlations between system performance and observable measures of the NS and NNS data, namely type-to-token ratios and mean response lengths.
%Where sufficient data allows, I also look for patterns indicating correlations between pairs of parameter settings used in a given system configuration.

%My earliest attempts at ranking responses were rule-based and relied on strict matching with a pre-established set of acceptable responses, described in Section~\ref{sec:pilot-study}. This found moderate success, leading to the improved approach described in Section~\ref{sec:2016work}, which is data-driven and relies on more flexible methods of comparison.


%I present some initial statistics and observations regarding the NS models and NNS test sets in Section~\ref{sec:sample-stats}, as these observations help explain trends seen in the experiments that follow. Ideally, for each of the five annotation features introduced in Chapter~\ref{chap:annotation}, my system rankings should maximize the separation of positively and negatively annotated NNS responses. In Section~\ref{sec:exp-annotations}, I investigate this using mean average precision to see how well each feature correlates with my dependency-based similarity results, and to see which system settings and model sizes work best for different types of items. Some use cases in ICALL or testing will place more or less importance on particular annotation features, so the findings here are useful in determining the system settings that best capture each feature. In Section~\ref{sec:exp-holistic}, I step back from the individual annotation features to compare my system's rankings against the weighted annotation rankings described in Section~\ref{sec:holistic-scoring}. These rankings are derived from human judgements and are intended to represent responses' overall quality, so these experiments seek to find the optimal settings for tasks where a holistic measure is desired. I use Spearman rank correlations between system rankings and the weighted annotation rankings for these experiments.

Following discussion of system updates in Section~\ref{sec:current-method}, Section~\ref{sec:sampling} addresses my decisions in sampling NS models from the larger corpus to control for size effects. In Section~\ref{sec:bert-benchmark}, I discuss the use of a state-of-the-art machine learning sentence encoder instead of my own similarity pipeline to help identify strengths and weaknesses of my system.
%I give an overview of the updates to this approach used here to process the new, larger dataset described in Chapter~\ref{chap:data}.
I present some initial statistics and observations regarding the NS models and NNS test sets in Section~\ref{sec:sample-stats}, as these observations help explain trends seen in the experiments that follow. 
Ideally, for each of the five annotation features introduced in Chapter~\ref{chap:annotation}, my system rankings should maximize the separation of positively and negatively annotated NNS responses.
In Section~\ref{sec:exp-annotations}, I examine how well each annotation feature correlated with my dependency-based similarity results and which system settings and model sizes maximized these correlations. In Section~\ref{sec:exp-holistic}, I step back from trying to approximate the individual feature annotations with my system, looking instead to automatically rank NNS responses according to overall quality and determine which settings produce rankings that best correlate with the benchmark weighted annotation rankings described in Section~\ref{sec:holistic-scoring}.


\section{Updated methodology}
\label{sec:current-method}
The pilot study discussed in Chapter~\ref{chap:pilot} relied on a shaky implementation of ``correctness'' or ``appropriateness'' for responses, and this needed improvement, first and foremost. Developing a better and more reliable annotation scheme was a key goal for me in expanding the work to its current scale, in order to give the work more meaning and context and make my corpus useful for a broad range of uses. The annotation planning discussed in Section~\ref{sec:scheme} was a direct result of the challenges of working with an inadequate annotation scheme. 

As discussed in Sections~\ref{sec:est-feat-weights} and~\ref{sec:holistic-scoring}, in the current corpus, annotations no longer directly encode for \textit{errors}, but instead give a binary score for five different features, which can then be weighted and combined to produce a score between zero and one. As a result, the metrics for judging system performance have changed. Mean average precision (MAP), previously used with the \textit{good/bad} annotations (Section~\ref{sec:metrics}), was used here to judge system performance focused on individual annotation features, presented in Section~\ref{sec:exp-annotations}. I also compared system-produced rankings against the weighted annotation rankings for a holistic approach to evaluating responses; i.e., how well does the system rank responses in comparison to a ranking derived from all five manual feature annotations? As presented in Section~\ref{sec:exp-holistic}, I evaluated the holistic experiments with Spearman rank correlation, which is a measure of correlation between two sets of rankings \cite{dodge2008concise}.

Because the annotations and evaluation used in this chapter are different from those used in the pilot study, it does not exactly follow that the previous findings will hold true. However, I believe that the previous work highlighted some of the system settings most likely to perform well, and I chose to focus my experiments on some of the best performing settings to allow me to also examine some previously unexplored parameters. For example, all of the current experiments relied on the tf-idf cosine (\param{TC}) approach, as this generally outperformed the others. As discussed in Section~\ref{sec:metrics}, the \param{TC} performance suffers for items where the non-native speaker (NNS) data is noisiest (with regard to spellings) and the native speaker (NS) data is relatively homogenous (particularly with regard to verb choice). Rather than continue experimenting with the underperforming approaches, I chose to address these issues directly instead. As discussed in Chapter~\ref{chap:data}, to address the spelling noise, I made sure that data collection participants had access to spelling correction while typing their responses. To address the uniformity of the NS responses, I surveyed a much larger group of participants and instructed each of them to provide two responses per picture description task (PDT) item. In other words, the limitations of the \param{TC} model have already been addressed, and so I expect results in this chapter to be generally applicable.

The current work retained just three of the five \param{term rep\-re\-sent\-a\-tions} previously used (see Section~\ref{sec:response-rep}): \param{ldh} (for \textit{label}, \textit{dependent}, \textit{head}), \param{xdh}, and \param{xdx}. The \param{ldh} and \param{xdh} forms performed best with the older dataset. Moreover, as these represent labeled (\param{ldh}) and unlabeled (\param{xdh}) dependencies, their use in linguistics is well-established. The \param{xdx} representation was kept here as a rough equivalent of a bag-of-words model, another well-established linguistic representation. The omitted forms, \param{ldx} and \param{lxh}, were not competitive in the pilot study, most likely because they do not capture the important relationship between heads and dependents.

Finally, as the Brown Corpus overwhelmingly outperformed the Wall Street Journal corpus as a tf-idf reference, the current experiments relied exclusively on the Brown Corpus. Note that both of these corpora are manually annotated, so using either is preferable to using an automatically parsed corpus, even a more recent or more narratively-focused one. The larger PDT dataset follows a similar narrative style to that described in Section~\ref{sec:pilot-data}, so I am confident \param{Brown} was again the best option here.

All experiments throughout this chapter scored and ranked 70 NNS responses per item; this was the largest number of NNS responses available across all PDT items. Where more than 70 responses were available, a random sample of 70 was used.
%The large increase in the number of PDT items and the size of the datasets means that running an exhaustive search for the best parameters among all combinations is not feasible.
The experiments in Chapter~\ref{chap:pilot} used 30 different combinations of system settings (i.e., \textbf{configurations}) for each of 10 items. By comparison, the variables and parameters used in this chapter resulted in up to 72 different configurations (see Table~\ref{tab:all-params}). Subtracting \param{trans\-i\-ti\-vi\-ty} (which cannot be varied for a given item) leaves 24 configurations which apply to all 30 PDT items. To make sense of this large number of results, I chose to focus my optimization efforts on each parameter individually, rather than attempt an exhaustive search through all configurations. I began this by producing NNS response rankings for all PDT items using all 24 system configurations. Then, in order to compare performance for \param{tar\-get\-ed} and \param{un\-tar\-get\-ed} settings, for example, I formed one pool of results from all configurations including the \param{tar\-get\-ed} setting and another pool from all configurations including the \param{un\-tar\-get\-ed} settings. Pooling the results in this way avoids the data sparsity that would be encountered in an exhaustive comparison of configurations, but it also limits the ability to observe interactions between the various parameter settings.


\begin{table*}
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
\param{Trans\-i\-ti\-vi\-ty} & \param{Tar\-get\-ing} & \param{Fam\-il\-iar\-i\-ty} & \param{Prim\-a\-cy} & \param{Term Rep.} \\
\hline
\hline
\param{in\-trans\-i\-tive} & \param{tar\-get\-ed} & \param{Familar} & \param{prim\-a\-ry} & \param{xdx} \\
\hline
\param{trans\-i\-tive} & \param{un\-tar\-get\-ed} & \param{crowd\-sourced} & \param{mix\-ed} & \param{xdh} \\
\hline
\param{di\-trans\-i\-tive} & & & & \param{ldh} \\
\hline
\end{tabular}
\caption{All parameters or variables and their settings; a system configuration combines one setting from each column.}
\label{tab:all-params}
\end{center}
\end{table*}

%%% INCLUDES TERM NORM
%\begin{table*}
%\begin{center}
%\begin{tabular}{|l|l|l|l|l|l|}
%\hline
%Transitivity & Targeting & Familiarity & Primacy & Term Norm. & Term Rep. \\
%\hline
%\hline
%Intransitive & Targeted & Familar & Primary & Normalized & \param{ldh} \\
%\hline
%Transitive & Untargeted & Crowdsourced & Mixed & Non-norm. & \param{xdh} \\
%\hline
%Ditransitive & & & & & \param{xdx} \\
%\hline
%\end{tabular}
%\caption{All parameters or variables and their settings; a system configuration combines one setting from each column.}
%\label{tab:all-params}
%\end{center}
%\end{table*}


%\chapter{Optimization}
%\label{chap:optimization}
%In this chapter, I detail my research applying the methods discussed in Chapter~\ref{chap:pilot} to the much larger and more richly annotated dataset described in Chapters~\ref{chap:data} and \ref{chap:annotation}. The chapter primarily consists of a series of experiments focused on isolating and optimizing a number of parameters or variables in my picture description task (PDT) response analysis pipeline. 

To help contextualize the results of these experiments, I used a state-of-the-art language modeling tool called SBERT to rank responses according to their similarity to the NS model. I discuss this tool and its use as a benchmark in Section~\ref{sec:bert-benchmark}.

I begin discussion of the optimization experiments in Section~\ref{sec:exp-holistic} with a new parameter that I call \param{term norm\-al\-iz\-a\-tion}. In the usual \param{non-norm\-al\-iz\-ed} setting, all \textit{terms} in the NS model carry equal weight, but in the \param{norm\-al\-iz\-ed} setting, all \textit{responses} carry equal weight. This is achieved by normalizing the weight of each NS term in the model according to the length of the response in which it appeared. While I expected the \param{norm\-al\-iz\-ed} setting to improve performance, the opposite was true, so I did not include it elsewhere in system configurations or in Table~\ref{tab:all-params}. I discuss the findings in Section~\ref{sec:exp-term-norm}, before moving on to focus on the more useful parameters that I retained in configurations throughout this chapter.

These sections are organized according to the sequence in which the parameters are relevant in my process, which begins with data collection and ends with scoring and ranking non-native speaker (NNS) responses. Thus, in Section~\ref{sec:exp-transitivity}, I begin with the variable I refer to as \param{trans\-i\-ti\-vi\-ty}, which emerged during task design for the PDT described in Chapter~\ref{chap:data}; I look at the effects of applying my dependency-based tf-idf cosine pipeline to new item types, namely \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives}, and compare against performance on \param{trans\-i\-tive} items. Next, in Section~\ref{sec:exp-targeting}, I turn to experiments regarding \param{tar\-get\-ing}, which refers to whether or not the PDT item subject was referenced in the prompt (as discussed in Section~\ref{sec:pdt}). In Section~\ref{sec:exp-familiarity}, I examine a variable I call \param{fam\-il\-iar\-i\-ty}, which refers to whether the native speakers (NSs) contributing to the model are \param{fam\-il\-iar} to me personally or are \param{crowd\-sourced}. Another new variable follows in Section~\ref{sec:exp-primacy}; I call this \param{prim\-a\-cy}, which refers to whether the NS model contains only first (\param{prim\-a\-ry}) responses, or an equal number of first and second (\param{mix\-ed}) responses (also discussed in Section~\ref{sec:pdt}). Note that I did not investigate the use of models comprised only of secondary responses. This is deliberate, because in real use cases there would never be a scenario in which one has secondary responses but not primary responses. I return in Section~\ref{sec:exp-term-reps} to the best performing dependency \param{term rep\-re\-sent\-a\-tions} from Section~\ref{sec:response-rep} to discuss how they performed with the current dataset.

%After examining these variables individually, in Section~\ref{sec:exp-combos} I consider the hypothesis that particular configurations \lk{Check research Qs and sync this up} will perform better than others in particular conditions. This is a ``non-exhaustive,'' mostly future-looking set of experiments, where I report some promising trends.

%Finally, in Section~\ref{sec:exp-bert}, I present a set of experiments comparing the performance of my best system settings against BERT, a more sophisticated, state of the art language model capable of ranking NS responses according to their similarity to the collection of NNS responses. These results provide insights into the trade-offs between using my simpler, highly transparent tool and a more powerful yet highly opaque tool.

\section{Sampling NS Response Models}
\label{sec:sampling}

Throughout this chapter, the experiments were performed with randomly sampled NS models of two sizes in order to examine the effects of model size on system performance. The larger models sampled 50 NS responses per PDT item. This was the largest number of NS responses available across all PDT items using relevant system configurations. For example, item 26 in the corpus contains 290 total NS responses, but roughly 90\% are \param{crowd\-sourced}, 50\% are \param{tar\-get\-ed}, and 50\% are first (\param{prim\-a\-ry}) responses, and selecting for this configuration leaves exactly 50 responses from which I formed a model. The smaller models throughout this chapter sampled 14 NS responses per item. This was the largest number of responses available across all items from \param{fam\-il\-iar} NSs, so I chose this size in order to fairly compare \param{crowd\-sourced} and \param{fam\-il\-iar} responses (Section~\ref{tab:primacy-results}) and I retained it throughout this chapter to best contextualize the \param{fam\-il\-iar\-i\-ty} experiments.
%I26: 158 Targeted, 132 Untargeted

\section{SBERT as a Benchmark}
\label{sec:bert-benchmark}
The central task of my work---ranking NNS responses using a set of NS responses---is not a standard task with established metrics in any relevant field like natural language processing (NLP) or language testing. Moreover, this work relied on a custom dataset which has not been widely used. These facts make it challenging to assess the performance of my ranking system and its various configurations or to compare this work against similar research. In order to give some frame of reference for this work, I chose to use a widely adopted language embedding tool known as SBERT \cite{sbert2020}. In practice, this means swapping the sentence similarity component of my pipeline (dependency tf-idf cosine) for SBERT.

SBERT is a modified version of BERT (Bidirectional Encoder Representations from Transformers; \citealp[][]{BertDevlin2018}), which is a pre-trained transformer network that has set records on NLP tasks including semantic textual similarity, topic modeling, and question answering. I chose SBERT over BERT because it is faster, more efficient and more readily usable on a typical personal computer. Specifically, I used the pre-trained version of SBERT distributed in the \texttt{sentence\_transformers} Python package \cite{sbert-python}. For semantic textual similarity, SBERT is trained on the Stanford Natural Language Inference (SNLI) corpus \cite{bowman2015} and the Multi-Genre Natural Language Inference (MultiNLI) corpus \cite{MultiNLI2018}. These two corpora consist of sentence pairs manually labeled as \textit{contradiction}, \textit{entailment}, or \textit{neutral}; the SNLI corpus contains 570,000 sentence pairs, and the MutliNLI corpus contains 430,000 sentence pairs.
 
My system scored each single NNS test response according to its similarity with the set of NS model responses. Measuring semantic textual similarity is one of SBERT's most used functions, so for each scoring and ranking experiment in this chapter, I used SBERT to generate corresponding output. In other words, both my system and SBERT generated a similarity score for each NNS response, and these scores were used to generate response rankings that could then be compared with a benchmark ranking. The resulting mean average precision (MAP) and Spearman rank correlation scores are discussed throughout to help contextualize my system's performance.

For each experiment reported throughout this chapter, my system and SBERT were given access to the same NS responses as the basis for their similarity measures. Naturally, however, each is trained on or makes use of very different language resources. The linguistic ``intelligence'' of my system comes largely from the Stanford Parser and its pre-trained grammar model, as discussed in Section~\ref{sec:pilot-study} \cite{klein:manning:03}. The model is trained on the standard training sections of the Penn Treebank \cite{marcus-et-al:93}, which contain over one million words of English text, manually part-of-speech tagged and parsed, sourced from the Wall Street Journal and the Brown Corpus  \cite{kucera:francis:67}. My approach also used the Brown Corpus for tf-idf, meaning a word (or more accurately, dependency) frequency model extracted from the Brown Corpus also serves as a linguistic resource. SBERT, on the other hand, is trained on a million sentence pairs, each manually labeled to indicate similarity.

My use of SBERT here varied slightly from the way I implemented the similarity scoring used in my system throughout most of this chapter. In my system, each NS response in the model was processed and dumped into a single ``bag of dependencies,'' which was then used to generate a single similarity score (via tf-idf cosine). SBERT operates directly on plain text sentences, and because the use of punctuation in the PDT responses is not consistent, concatenating all NS model responses in order to generate a single similarity score is not ideal. Instead, I used SBERT to do a pairwise comparison between the NNS test response and each NS response in the model and then averaged these similarity scores.

%I did explore an implementation of my system's similarity scoring that is more consistent with this individual treatment of model responses for BERT, and this is discussed in Section~\ref{sec:exp-term-norm}.

\section{Sample Statistics}
\label{sec:sample-stats}
Before jumping into the experiments in Sections~\ref{sec:exp-annotations} and~\ref{sec:exp-holistic}, I present here some initial statistics for the samples used as NS models and NNS test sets. In general, I expected the NS models that most resemble the NNS test sets to perform best in the ranking experiments, so these statistics may shed light on the experimental results that follow.

\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.5em}
\begin{tabular}{|l||l|l|l||l|}
\hline
  & \multicolumn{2}{c|}{n=14} & n=50 & n=70\\
\hline
   & \param{Fam} & \param{Crowd} & \param{Crowd} 	& NNS			\\ \hline
\hline
\param{Intrans} & 5.5 	  		& 4.9 			& 4.9 		& 4.9 			\\ \hline
\param{Trans}   & 6.9          	& 6.3          	& 6.2       & 6.7    	    \\ \hline
\param{Ditrans} & 7.8          	& 7.2          	& 7.2       & 8.3    	    \\ \hline
\hline
\param{Target}  & 6.5 			& 5.4	 		& 5.4 		& 6.3			\\ \hline
\param{Untarg}  & 6.9        	& 6.8        	& 6.8    	& 6.9        	\\ \hline
\hline
\param{prim\-a\-ry} & N/A        	& 5.7 			& 5.8		& 6.6		 	\\ \hline
\param{mix\-ed}   & 6.7          	& 6.5          	& 6.4       & N/A	        \\ \hline
\hline
Total	& 6.7			& 6.1			& 6.1		& 6.6			\\ \hline
\end{tabular}
\caption{\label{tab:response-length}Comparing average response length (in words) for the samples used throughout this chapter as NS models and NNS test sets, in total and by parameter setting. Note that not enough data was available for a \param{primary} model for \param{fam\-il\-iar} participants. 
}
\end{center}
\end{table}

Table~\ref{tab:response-length} presents average response length for the samples, in total and broken down by parameter settings. As expected, for all samples we see an increase in response length as we move from \param{in\-trans\-i\-tives} to \param{trans\-i\-tives} to \param{di\-trans\-i\-tives}. The same is true in moving from \param{tar\-get\-ed} to \param{un\-tar\-get\-ed} settings, and from \param{prim\-a\-ry} to \param{mix\-ed} settings (where applicable). These numbers also show that in most cases, the \param{fam\-il\-iar} responses were the longest and the \param{crowd\-sourced} responses were the shortest, with NNS responses falling somewhere in between. The only exception is for \param{di\-trans\-i\-tive} items, where the NNS responses were longest. By comparing the 14-response and 50-response \param{crowd\-sourced} samples, we can see that the response length was quite stable across sample sizes.

As an indication of complexity or lexical density, in Table~\ref{tab:sttr}, I present the standardized type-to-token ratios (STTR) for the response samples. Higher complexity inherently means greater data sparsity and more low-frequency events, and this generally necessitates the use of richer models and more sophisticated methods in natural language processing \cite{malvern2004lexical}. I used STTR as opposed to TTR as a means to normalize for large differences in size, with the smallest samples containing only 14 responses (per item), and the largest containing 70. A \textit{standardized} type-to-token ratio, sometimes referred to as a \textit{mean segmental} type-to-token ratio, simply calculates the TTR for each segment of \textit{n} words in the sample, then averages these TTRs at the end \cite{johnson1944studies, richards2000accommodation}. Here I used a window of 40 words, because the smallest sample contained only 40 words. 

\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.5em}
\begin{tabular}{|l||l|l|l||l|}
\hline
 	& \multicolumn{2}{c|}{n=14} & n=50 & n=70 \\
\hline
   	& \param{Fam} & \param{Crowd} & \param{Crowd} 			& NNS			\\ \hline
\hline
\param{Intrans} & 0.558 	  	& 0.525 			& 0.535 		& 0.391 		\\ \hline
\param{Trans}   & 0.569        	& 0.580          	& 0.581        	& 0.517    	    \\ \hline
\param{Ditrans} & 0.598        	& 0.640          	& 0.637        	& 0.606    	    \\ \hline
\hline
\param{Target}  & 0.545 		& 0.535	 			& 0.545 		& 0.481			\\ \hline
\param{Untarg}  & 0.610        	& 0.633        		& 0.621    		& 0.528        	\\ \hline
\hline
\param{prim\-a\-ry} & N/A        	& 0.517 			& 0.523			& 0.505		 	\\ \hline
\param{mix\-ed}   & 0.576         & 0.652          	& 0.645       	& N/A	        \\ \hline
\hline
\param{xdx}     & 0.364			& 0.424 			& 0.421			& 0.364			\\ \hline
\param{xdh}     & 0.658        	& 0.661          	& 0.660       	& 0.572	        \\ \hline
\param{ldh}     & 0.665        	& 0.664          	& 0.671       	& 0.578	        \\ \hline
\hline
Total    & 0.576        & 0.583          	& 0.584    		& 0.505	        \\ \hline
\end{tabular}
\caption{\label{tab:sttr}Comparing average standardized type-to-token ratio (STTR) for the samples used throughout this chapter as NS models and NNS test sets, in total and by parameter setting. Tokens here are \textit{dependencies}.
}
\end{center}
\end{table}

Within each parameter, STTR showed a similar pattern to response length. Complexity increases as we move from \param{in\-trans\-i\-tives} to \param{trans\-i\-tives} to \param{di\-trans\-i\-tives}, from \param{tar\-get\-ed} to \param{un\-tar\-get\-ed} settings, and from \param{prim\-a\-ry} to \param{mix\-ed} response models. As the representation is simplified from labeled dependencies to unlabeled dependencies to dependents only, STTRs decrease. 
For example, as seen in Table~\ref{tab:sttr}, for the 70 response NNS set, these values decrease from 0.578 to 0.572 to 0.364.
This decrease is small when moving from labeled to unlabeled dependencies, indicating that for a given head and dependent, syntactic labels tend to be the same. A much larger decrease is observed when moving from unlabeled dependencies to dependents only, indicating that dependents occur with a wider variety of heads than labels. This is to be expected, but it may be useful in understanding differences in performance across \param{trans\-i\-ti\-vi\-ty} types, for example; \param{di\-trans\-i\-tives} mean more verb arguments than \param{in\-trans\-i\-tives}, and this means more variety in the combinations of heads and dependents.
%It is also notable that this decrease in STTR from \param{xdh} to \param{xdx} is much greater for \param{fam\-il\-iar} samples than for \param{crowd\-sourced} samples. \lk{Actually, I think this is probably just a result of the fact that the familiar STTRs here cover only mixed samples, while the crowdsourced STTRs here cover both primary and mixed models; the primary models tend to have a lower STTR.}

In comparing across the models, the story is complicated. First, STTR was less stable than response length across sample sizes, as seen between the 14-response and 50-response \param{crowd\-sourced} models. One consistent pattern seen here is that across the board (i.e., on any given row of Table~\ref{tab:sttr}), the 50-response \param{crowd\-sourced} models were least like the NNS test sets. This is a pattern which plays out in the following sections. In the STTRs seen here, the NNS test sets were sometimes closest to the 14-response \param{fam\-il\-iar} models, and at other times closest to the 14-response \param{crowd\-sourced} models. As I will point out in the following sections, this aligned with some of the patterns seen in the feature and holistic experiments.

\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.5em}
\begin{tabular}{|l||l|l|l|}
\hline
 & \multicolumn{2}{c|}{NS = 14} & NS = 50 \\
\hline
   & \param{Fam} & \param{Crowd} & \param{Crowd} \\ \hline
\hline
\param{Intrans} & \textbf{0.410} & \textbf{0.382} & \textit{\textbf{0.342}} \\ \hline
\param{Trans}   & 0.490          & 0.504          & 0.465                   \\ \hline
\param{Ditrans} & 0.576          & 0.601          & 0.563                   \\ \hline
\hline
\param{Target}    & \textbf{0.466} & \textbf{0.470} & \textit{\textbf{0.436}} \\ \hline
\param{Untarg}  & 0.517          & 0.521          & 0.478                   \\ \hline
\hline
\param{prim\-a\-ry}    & N/A             & \textbf{0.475} & \textit{\textbf{0.447}} \\ \hline
\param{mix\-ed}   & 0.492          & 0.516          & 0.466                   \\ \hline
\hline
\param{ldh}     & 0.553          & 0.557          & 0.513                   \\ \hline
\param{xdh}     & 0.541          & 0.546          & 0.503                   \\ \hline
\param{xdx}     & \textbf{0.382} & \textbf{0.383} & \textit{\textbf{0.353}} \\ \hline
\hline
SBERT    & 0.675          & 0.640          & \textit{0.641}                   \\ \hline
\end{tabular}
\caption{\label{tab:param-response-distances}Comparing average NNS response scores across parameter settings and NS models, using the dependency tf-idf cosine scoring approach introduced in Section~\ref{sec:scoring}. The same sets of 70 NNS responses per model and configuration were scored here and throughout this chapter. Scores represent the NNS \textit{distance} from the NS model, so lower scores are closer to NS behavior. Within each parameter, the score for the setting that minimizes distance is \textbf{bolded}, and the score for the model that minimizes distance is \textit{italicized}.
}
\end{center}
\end{table}

The average response length and STTR statistics presented in Tables~\ref{tab:response-length} and~\ref{tab:sttr} can both be calculated before any similarity scoring and ranking, and do not require annotation. This means that any predictive patterns observed there could be easily applied to new items. In Table~\ref{tab:param-response-distances}, I present similarly organized statistics for response scores. These scores are averages of response scores, which represent the \textit{distance} between an NNS test response and an NS model. In other words, these averaged scores give an indication of which settings minimize the distance between the NS and NNS data. Note that there is no column for the NNS sample here because the NS columns are calculated according to how they score that same NNS 70-response sample.
%Naturally, such figures would not be available for new items, but they also do not require annotation, only similarity scoring.
%By examining patterns here in comparison with those seen in the experiments that follow, we may find correlations that can guide the scoring of new items, however.\lk{return to this...}

These scores average over all NNS test response scores---the response scores that were used to rank responses. Because my work is more interested in the ranking of responses than the scores, a given average score in the table is not particularly meaningful. In combination, however, they give an indication of how skewed the scores are for each model and parameter setting. In all cases, the 50-response \param{crowd\-sourced} models did best at minimizing the distance between NNS test responses and the model; in contrast, these same models were least like the NNS test responses in terms of STTR (see Table~\ref{tab:sttr}). As seen in the experiments in Section~\ref{sec:exp-holistic}, these average scores may be predictive of my system's performance at holistic ranking, where the larger NS models provided better response rankings.


%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.5em}
%\begin{tabular}{|l||l|l|l||l|}
%\hline
% & \multicolumn{2}{c|}{NS = 14} & NS = 50 & NNS = 70\\
%\hline
%   & \param{Fam} & \param{Crowd} & \param{Crowd} 	& NNS			\\ \hline
%\hline
%Intrans & in 	  		& in 			& in 		& in 			\\ \hline
%Trans   & tr          	& tr          	& tr        & tr    	    \\ \hline
%Ditrans & di          	& di          	& di        & di    	    \\ \hline
%\hline
%Target  & targ 			& targ	 		& targ 		& targ			\\ \hline
%Untarg  & untg        	& untg        	& untg    	& untg        	\\ \hline
%\hline
%Primary & N/A        	& prim 			& prim		& prim		 	\\ \hline
%Mixed   & mix          	& mix          	& mix       & mix	        \\ \hline
%\hline
%ldh     & ldh          	& ldh          	& ldh       &ldh	        \\ \hline
%xdh     & xdh          	& xdh          	& xdh       &xdh	        \\ \hline
%xdx     & xdx 			& xdx 			& xdx		&xdx			\\ \hline
%\hline
%BERT    & bert          & bert          & bert      &bert	        \\ \hline
%\end{tabular}
%\caption{\label{tab:template1}This is the caption for my table template.
%}
%\end{center}
%\end{table}

%% sec:metrics (label for sec in Chapter 3 re: MAP)

\section{Annotation Features Experiments}
\label{sec:exp-annotations}
In this section, I revisit the five annotation features discussed in Chapter~\ref{chap:annotation} to see how they correlate with the performance of my system. For a given feature, I present mean average precision (MAP) scores to indicate how well the system rankings separated the positively and negatively annotated responses. In other words, for the purposes of calculating MAP for the \feat{core e\-vent} feature, a ``0'' annotation for \feat{core e\-vent} was treated as an error. As discussed in Section~\ref{sec:metrics}, average precision is used as a measure of how well a ranking separates errors from non-errors. For ease of use, I shifted to the \param{a\-ver\-age\-\_\-pre\-cis\-ion\-\_\-score} implementation from the Scikit-learn package for Python. (This differs slightly from the version described in Section~\ref{sec:metrics}, but either implementation, applied consistently, results in the same rankings.) The Scikit-learn documentation explains that average precision ``summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight,'' \cite{scikit-learn}. The formula is presented in Figure~\ref{fig:exp-ap}. I refer to \textit{mean} average precision throughout this section because I calculated average precision for all 30 items, then averaged these values for the purpose of evaluating the system configuration that produced the rankings. This approach overlooks variability on the item level, but it allows for scoring of parameter settings where data would otherwise be too sparse.



\begin{figure}[htb!]
\begin{center}
%\text{AP} = ${\sum_n (R_n - R_{n-1}) P_n}$
\[
\text{AP} = {\sum_n (R_n - R_{n-1}) P_n}
\]
\caption{\label{fig:exp-ap} The formula used here for average precision, where $P_n$ and $R_n$ are the precision and recall at the nth threshold.}
\end{center}
\end{figure}



%\text{AP} = \sum_n (R_n - R_{n-1}) P_n

I present these MAP scores isolated for each setting within the \param{trans\-i\-ti\-vi\-ty}, \param{tar\-get\-ing}, and \param{prim\-a\-cy} variables, as well as in total. I also compare across \param{term rep\-re\-sent\-a\-tions}. To contextualize these scores, I include MAP scores for the weighted annotation rankings (WAR) and SBERT rankings. Note that the relatively high WAR MAP scores that will be seen here are to be expected. First, the weights are based on the preference judgments of the same annotators who developed the annotation scheme. More importantly, the WAR is derived from weighting and combining the five feature annotations for each response, so precision in assessing any one of these features is inherently correlated to the WAR. 
%In some sense, this means I am training and testing on the same data, but I believe these scores still provide valuable insight.

The function of combining a response's five binary annotations into a single annotation score using weights is a form of lossy compression; the WAR MAP gives a sense of just how lossy this weighted compression function is in terms of recovering the annotations for the five individual features and across different item types. In other words, the WAR MAP scores give some indication of what a good performance looks like on this task, given the annotation scheme and its weights. The WAR MAP does not represent an upper bound, however, and the tables throughout Section~\ref{sec:exp-annotations} show that the system MAP for predicting individual features slightly outperforms the WAR MAP in roughly half of all cases. The rate and degree to which system scores exceed the WAR MAP score is likely a function of the interaction between a feature's weight carried within the WAR and the correlation between the feature and the system's approach to semantic textual similarity. This correlation, in other words, indicates how reliably the system can predict the feature (because the feature correlates with surface forms---and in turn, dependencies). While not pursued here, exploring these correlations could lead to metrics that add to and help contextualize the results presented throughout this section.

These experiments would be useful to anyone considering an approach to content analysis like mine. In an intelligent computer-assisted language learning (ICALL) game, for example, there may be times when \feat{core e\-vent} is the main concern, or others where \feat{an\-swer\-hood} or another feature is most relevant. By isolating each annotation feature and examining how model sizes and \param{term rep\-re\-sent\-a\-tions} effect MAP---both overall and for individual variables, such as \param{in\-trans\-i\-tives} or \param{tar\-get\-ed} items---I observed a number of trends that would be helpful in designing an ICALL system that selects an optimal system configuration for handling each user response.

I present here two tables for each of the five annotation features. For each feature, the first table presents MAP scores for models comprised of \param{crowd\-sourced} responses, showing both the 14-response and 50-response models. This table serves to examine whether the task of recognizing the feature is sensitive to differences in model size, and whether parameters like \param{tar\-get\-ing} or \param{prim\-a\-cy} interact with any such differences. The second table compares MAP scores for the \param{fam\-il\-iar} and \param{crowd\-sourced} NS models, using models of 14 responses each,
%. Note that the first table covers both \param{prim\-a\-ry} and \param{mix\-ed} response models, but the second table covers only \param{mix\-ed} response models (for both \param{fam\-il\-iar} and \param{crowd\-sourced} responses)
 because for some items, 14 was the largest number of \param{fam\-il\-iar} responses available (including first and second responses). Thus the \param{crowd\-sourced} 14-response models in each first table and corresponding second table differ; in the first table, the total average includes \param{primary} and \param{mixed} settings, and in the second table, the total average includes only \param{mixed} settings. In all 10 tables, I use \textbf{bold} to indicate the highest \param{term rep\-re\-sent\-a\-tion} MAP \textit{within} each of the two models presented and \textit{\textbf{bold with italics}} to indicate the highest MAP \textit{between} the two models.

Overall, a few trends stood out, which I outline here and discuss in detail in the following sections. First, both as a trend and a caveat, note that in many cases the relevant differences in MAP scores observed here were small, and may not always be statistically significant.
%Within all 10 of the tables presented in this section, the maximum difference between any two MAP scores ranges between 0.08 and 0.24.
The observations here are thus not intended as specific guidance for high stakes decisions, but in aggregate as a proof-of-concept for the overall project, from the annotated data to the content-focused, similiarity-based analysis. Individually, the trends discussed in this chapter serve as an indication of promising directions for future work. 

One notable observation is the underperformance of unlabeled dependencies (\param{xdh}). In some cases, labeled dependencies (\param{ldh}) achieved the highest MAP, and in some cases dependent-only terms (\param{xdx}) worked best, but there were no cases in which unlabeled dependencies won. This suggests that future iterations of my system could safely eliminate the use of unlabeled dependencies for the sake of simplicity. I also observed that for the features where labeled dependencies worked best (\feat{core e\-vent}, \feat{in\-ter\-pret\-a\-bil\-ity}, and \feat{ver\-i\-fi\-a\-bil\-ity}), the \param{crowd\-sourced} NS models also worked best, but for the features where the dependent-only terms worked best (\feat{an\-swer\-hood} and \feat{gram\-mat\-i\-cal\-ity}), the \param{fam\-il\-iar} NS models worked best. In other words, as annotated in the corpus, \feat{an\-swer\-hood} and \feat{gram\-mat\-i\-cal\-ity} did not accept much variation, and thus required only a small but high-quality NS set, while the other three features allowed more variability and thus required a larger NS set to model valid variations.

 
%It is worth noting, however, that no attempts were made to optimize BERT for this task, beyond the use of the two different size NS models.
SBERT scores are included here primarily as a frame of reference, and a thorough investigation of its performance on this task is beyond the scope of these experiments. However, it is clear that the SBERT MAP scores seen throughout the following tables generally trended with the system MAP scores, which is to be expected as SBERT and the system are both approaches to semantic textual similarity. However, the MAP scores also show that SBERT underperformed the system in all cases (although it nearly ties the system performance when predicting \feat{gram\-mat\-i\-cal\-ity}; see Tables~\ref{tab:gramm-map} and \ref{tab:gramm-fam-map}). This means that with regard to recognizing the five annotation features, a custom pipeline based on dependency parsing and tf-idf can outperform newer machine learning approaches. 

Another notable trend seen here is that \param{trans\-i\-tive} items were often an exception; where \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives} saw higher performance with a given model size or \param{term rep\-re\-sent\-a\-tion}, \param{trans\-i\-tives} frequently differed. Yet another observation is that with regard to NS model size, less was overwhelmingly more. For each of the five features, the total MAP (which covers all rankings for all items, provided by all available system configurations) was highest for the smaller model. In some cases, the larger model may have been better for a particular item type or parameter setting, but the total MAP was always highest for the smaller model, likely because larger models tend to sample more low-quality responses, i.e., \textit{noise}.

The MAP scores also show that \param{crowd\-sourced} models usually outperform \param{fam\-il\-iar} models. On the surface, this may seem counterintuitive, as \param{fam\-il\-iar} participants were expected to complete the PDT most faithfully. However, as seen in this chapter and Section~\ref{sec:holistic-scoring}, \param{crowd\-sourced} responses were more like NNS responses than were \param{fam\-il\-iar} responses. \param{Crowd\-sourced} participants are less motivated than \param{fam\-il\-iar} participants, and this manifested in a higher rate of lazy or bad faith responses; this noise may simply better model NNS responses. Moreover, the \param{fam\-il\-iar} participants were all handpicked native English speakers, whereas the \param{crowd\-sourced} participants were anonymous, with no way of confirming that they were in fact native English speakers. It is possible that some ``NS'' responses came from non-native speakers, which could explain why \param{crowd\-sourced} models achieved higher MAP; in theory, the best model would be one of responses that are well-formed enough to have perfect annotations but nonetheless capture the range of NNS responses, which sometimes include non-nativelike forms.

Another salient pattern seen in all 10 tables is that MAP scores were highest for \param{in\-trans\-i\-tives} and lowest for \param{di\-trans\-i\-tives}. In other words, as sentence complexity increases, there was a monotonic decrease in system performance. Because \param{trans\-i\-tives} and \param{di\-trans\-i\-tives} cannot simply be avoided or transformed to \param{in\-trans\-i\-tives}, this trend suggests that an ICALL tool or similar application should carefully consider how to best optimize for more complex items, e.g., by selecting the best \param{term rep\-re\-sent\-a\-tion}, as this correlates with item complexity.

\subsection{\feat{core e\-vent} Experiments}
\label{sec:map-core}

%% CORE EVENT MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}   & \textit{\textbf{0.859}} & 0.856 & 0.854 & 0.865 & 0.835  & \textbf{0.855} & 0.854 & 0.852 & 0.865 & 0.831 \\ \hline
\param{Tran}    & \textit{\textbf{0.737}} & 0.735 & 0.728 & 0.742 & 0.703   & \textbf{0.736} & 0.733 & 0.725 & 0.742 & 0.701 \\ \hline
\param{Ditr}    & \textit{\textbf{0.665}} & 0.661 & 0.664 & 0.660 & 0.634  & 0.657 & 0.656 & \textbf{0.661} & 0.660 & 0.629 \\ \hline
\hline
\param{Targ}    & \textit{\textbf{0.739}} & 0.738 & 0.732 & 0.735 & 0.708  & \textbf{0.737} & 0.735 & 0.729 & 0.735 & 0.704 \\ \hline
\param{Untg}    & \textit{\textbf{0.768}} & 0.763 & 0.765 & 0.777 & 0.740  & 0.762 & 0.759 & \textbf{0.763} & 0.777 & 0.736 \\ \hline
\hline
\param{Prim}    & \textit{\textbf{0.754}} & 0.752 & 0.747 & 0.756 & 0.723  & \textbf{0.750} & 0.748 & 0.745 & 0.756 & 0.719 \\ \hline
\param{Mix}      & \textit{\textbf{0.753}} & 0.749 & 0.750 & 0.756 & 0.725  & \textbf{0.749} & 0.746 & 0.746 & 0.756 & 0.721 \\ \hline
\hline
Total 	 & \textit{\textbf{0.753}} & 0.751 & 0.748 & 0.756 & 0.724 	& \textbf{0.750} & 0.747 & 0.746 & 0.756 & 0.720 \\ \hline
\end{tabular}
\caption{\label{tab:core-map}Mean Average Precision (MAP) scores for the \feat{core e\-vent} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{prim\-a\-ry} NS models), and for the full set (Total).
}
\end{center}
\end{table}


\feat{core e\-vent}, as discussed in Section~\ref{sec:scheme}, assesses whether a response captures the main action of the PDT item and requires that the event is linked to a subject (and an object or objects where necessary).

Table~\ref{tab:core-map} presents \feat{core e\-vent} MAP scores for the 14-response and 50-response \param{crowd\-sourced} models. These scores show that for assessing \feat{core e\-vent}, the smaller model, used with labeled dependencies was superior across the board. The 50-response model came closest for \param{trans\-i\-tives}, but never outperformed the 14-response model. We can also observe here that the \param{term rep\-re\-sent\-a\-tion} setting was more relevant in the larger model, with \param{xdx} besting \param{ldh} in the case of \param{di\-trans\-i\-tives} and \param{un\-tar\-get\-ed} items. Responses to \param{di\-trans\-i\-tives} and \param{un\-tar\-get\-ed} items tend to  be the least homogenous; they have higher standardized type-to-token ratios than their counterparts (see Table~\ref{tab:sttr}). Moving from \param{ldh} to \param{xdx} representations results in a lower standardized type-to-token ratio (Table~\ref{tab:sttr}), so it is unsurprising that doing so improved performance for these items. In other words, models that simplify representations in the face of relatively high complexity seemed to benefit performance here.

%% CORE EVENT MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Fam\-il\-iar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.859                   & 0.859 & \textit{\textbf{0.865}} & 0.865 & 0.838 & \textbf{0.857}          & 0.852 & 0.848                   & 0.865 & 0.833 \\ \hline
\param{Tran}  & \textit{\textbf{0.740}} & 0.737 & 0.726                   & 0.742 & 0.703 & \textbf{0.738}          & 0.735 & 0.728                   & 0.742 & 0.702 \\ \hline
\param{Ditr}  & 0.651                   & 0.648 & \textbf{0.660}          & 0.660 & 0.625 & 0.663                   & 0.659 & \textit{\textbf{0.673}} & 0.660 & 0.641 \\ \hline
\hline
\param{Targ}  & \textbf{0.733}          & 0.732 & 0.732                   & 0.735 & 0.707 & \textit{\textbf{0.739}} & 0.736 & 0.733                   & 0.735 & 0.709 \\ \hline
\param{Untg}  & 0.767                   & 0.764 & \textit{\textbf{0.769}} & 0.777 & 0.737 & \textbf{0.767}          & 0.761 & \textbf{0.767}          & 0.777 & 0.742 \\ \hline
\hline
Total & 0.750                   & 0.748 & \textbf{0.751}          & 0.756 & 0.722 & \textit{\textbf{0.753}} & 0.749 & 0.750                   & 0.756 & 0.725 \\ \hline
\end{tabular}
\caption{\label{tab:core-fam-map}Mean Average Precision (MAP) scores for the \feat{core e\-vent} annotation feature, comparing \param{fam\-il\-iar} and \param{crowd\-sourced} responses. MAP is derived from various response rankings: the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), weighted annotation ranking (WAR), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{tar\-get\-ed} items), and for the full set (Total). Note that all models represented here are \param{mix\-ed} due to the small number of \param{fam\-il\-iar} participants.
}
\end{center}
\end{table}


Table~\ref{tab:core-fam-map} compares \feat{core e\-vent} MAP scores for \param{fam\-il\-iar} and \param{crowd\-sourced} models. As seen in the total MAP scores, the \param{crowd\-sourced} models outperformed the \param{fam\-il\-iar} models overall, but the difference was slight. The table also shows that for \feat{core e\-vent}, the \param{fam\-il\-iar} models generally performed best with the dependent-only (\param{xdx}) \param{term rep\-re\-sent\-a\-tion}, whereas the \param{crowd\-sourced} models generally performed best with labeled dependencies. As seen in Table~\ref{tab:core-map}, the trend was unchanged for the \param{crowd\-sourced} 50-response models, but more data is needed to see how larger \param{fam\-il\-iar} models behave.



\subsection{\feat{an\-swer\-hood} Experiments}
\label{sec:map-answer}

\feat{an\-swer\-hood}, as discussed in Section~\ref{sec:scheme}, assesses whether a response presents a direct answer to the question asked in the PDT prompt.

%%ANSWERHOOD MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.868 & 0.871 & \textit{\textbf{0.878}} & 0.881 & 0.869 & 0.866 & 0.868 & \textbf{0.874} & 0.881 & 0.868 \\ \hline
\param{Tran}  & 0.816 & 0.819 & \textbf{0.846} & 0.845 & 0.838 & 0.818 & 0.823 & \textit{\textbf{0.851}} & 0.845 & 0.838 \\ \hline
\param{Ditr}  & 0.824 & 0.826 & \textit{\textbf{0.841}} & 0.837 & 0.833 & 0.821 & 0.822 & \textbf{0.840} & 0.837 & 0.833 \\ \hline
\hline
\param{Targ}  & 0.787 & 0.788 & \textbf{0.810} & 0.817 & 0.799 & 0.787 & 0.789 & \textit{\textbf{0.811}} & 0.817 & 0.798 \\ \hline
\param{Untg}  & 0.885 & 0.890 & \textit{\textbf{0.900}} & 0.892 & 0.894 & 0.883 & 0.886 & \textbf{0.899} & 0.892 & 0.895 \\ \hline
\hline
\param{Prim}  & 0.837 & 0.840 & \textit{\textbf{0.854}} & 0.854 & 0.845 & 0.837 & 0.840 & \textit{\textbf{0.854}} & 0.854 & 0.846 \\ \hline
\param{Mix}   & 0.835 & 0.838 & \textit{\textbf{0.857}} & 0.854 & 0.848 & 0.833 & 0.835 & \textbf{0.856} & 0.854 & 0.847 \\ \hline
\hline
Total & 0.836 & 0.839 & \textit{\textbf{0.855}} & 0.854 & 0.847 & 0.835 & 0.838 & \textbf{0.855} & 0.854 & 0.846 \\ \hline
\end{tabular}
\caption{\label{tab:answer-map}Mean Average Precision (MAP) scores for the \feat{an\-swer\-hood} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{prim\-a\-ry} NS models), and for the full set (Total).
}
\end{center}
\end{table}

Table~\ref{tab:answer-map} presents \feat{an\-swer\-hood} MAP scores for the 14-response and 50-response \param{crowd\-sourced} models. The dependent-only (xdx) models outperformed the others across the board here. We can also observe that unlabeled dependencies outperformed labeled dependencies in every case. This would indicate that identifying a direct answer based on my similarity metrics is a relatively simple task that works best with the simplest representations. The fact that the 14-response model slightly outperformed the 50-response model also indicates that larger models may degrade performance by including more noise.


%% ANSWERHOOD MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Fam\-il\-iar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.868 & 0.871 & \textit{\textbf{0.882}} & 0.881 & 0.868 & 0.869 & 0.873 & \textbf{0.878} & 0.881 & 0.870 \\ \hline
\param{Tran}  & 0.824 & 0.826 & \textit{\textbf{0.852}} & 0.845 & 0.840 & 0.817 & 0.818 & \textbf{0.847} & 0.845 & 0.840 \\ \hline
\param{Ditr}  & 0.820 & 0.822 & \textit{\textbf{0.846}} & 0.837 & 0.832 & 0.820 & 0.822 & \textbf{0.845} & 0.837 & 0.835 \\ \hline
\hline
\param{Targ}  & 0.786 & 0.787 & \textit{\textbf{0.815}} & 0.817 & 0.798 & 0.785 & 0.787 & \textbf{0.813} & 0.817 & 0.802 \\ \hline
\param{Untg}  & 0.889 & 0.892 & \textit{\textbf{0.904}} & 0.892 & 0.896 & 0.885 & 0.889 & \textbf{0.900} & 0.892 & 0.894 \\ \hline
\hline
Total & 0.837 & 0.840 & \textit{\textbf{0.860}} & 0.854 & 0.847 & 0.835 & 0.838 & \textbf{0.857} & 0.854 & 0.848 \\ \hline
\end{tabular}
\caption{\label{tab:answer-fam-map}Mean Average Precision (MAP) scores for the \feat{an\-swer\-hood} annotation feature, comparing \param{fam\-il\-iar} and \param{crowd\-sourced} responses. MAP is derived from various response rankings: the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), weighted annotation ranking (WAR), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{tar\-get\-ed} items), and for the full set (Total). Note that all models represented here are \param{mix\-ed} due to the small number of \param{fam\-il\-iar} participants.
}
\end{center}
\end{table}

Table~\ref{tab:answer-fam-map}, which compares \param{fam\-il\-iar} and \param{crowd\-sourced} models, confirms the higher performance of dependent-only models. We also see a slight but consistent advantage for the \param{fam\-il\-iar} models on this task.
The scores here also confirm the relative ease of assessing this feature. The top scores seen in the two the \feat{an\-swer\-hood} MAP tables were only bested by MAP scores for one other feature---\feat{in\-ter\-pret\-a\-bil\-ity}. The fact that the highest \feat{an\-swer\-hood} MAP scores came from the smaller 14-response models (both \param{fam\-il\-iar} and \param{crowd\-sourced}) again support this idea. The task is simple enough for small models, and larger models risk adding noise.


\subsection{\feat{gram\-mat\-i\-cal\-ity} Experiments}
\label{sec:map-gramm}

The \feat{gram\-mat\-i\-cal\-ity} feature, as discussed in Section~\ref{sec:scheme}, indicates whether a response is free from any form errors, including grammar and spelling. 

%%GRAMMATICALITY MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.868 & 0.870 & \textit{\textbf{0.872}} & 0.887 & 0.866 & 0.863 & 0.864 & \textbf{0.866} & 0.887 & 0.864 \\ \hline
\param{Tran}  & 0.753 & 0.756 & \textbf{0.757} & 0.781 & 0.757 & 0.758 & 0.760 & \textit{\textbf{0.761}} & 0.781 & 0.757 \\ \hline
\param{Ditr}  & 0.682 & 0.685 & \textit{\textbf{0.700}} & 0.695 & 0.694 & 0.679 & 0.685 & \textbf{0.697} & 0.695 & 0.693 \\ \hline
\hline
\param{Targ}  & 0.777 & 0.778 & \textit{\textbf{0.784}} & 0.800 & 0.782 & 0.776 & 0.776 & \textbf{0.783} & 0.800 & 0.781 \\ \hline
\param{Untg}  & 0.758 & 0.763 & \textit{\textbf{0.769}} & 0.776 & 0.762 & 0.757 & 0.762 & \textbf{0.766} & 0.776 & 0.761 \\ \hline
\hline
\param{Prim}  & 0.769 & 0.773 & \textit{\textbf{0.776}} & 0.788 & 0.770 & 0.768 & 0.770 & \textbf{0.774} & 0.788 & 0.770 \\ \hline
\param{Mix}   & 0.766 & 0.768 & \textit{\textbf{0.776}} & 0.788 & 0.774 & 0.765 & 0.768 & \textbf{0.775} & 0.788 & 0.772 \\ \hline
\hline
Total & 0.768 & 0.770 & \textit{\textbf{0.776}} & 0.788 & 0.772 & 0.767 & 0.769 & \textbf{0.775} & 0.788 & 0.771 \\ \hline
\end{tabular}
\caption{\label{tab:gramm-map}Mean Average Precision (MAP) scores for the \feat{gram\-mat\-i\-cal\-ity} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{prim\-a\-ry} NS models), and for the full set (Total).
}
\end{center}
\end{table}

Table~\ref{tab:gramm-map} presents \feat{gram\-mat\-i\-cal\-ity} MAP scores for the 14-response and 50-response \param{crowd\-sourced} models. These scores show a slight but consistent preference for dependent-only (\param{xdx}) models, with labeled dependencies consistently producing the lowest MAP and unlabeled dependencies falling in the middle. The table also shows that the smaller, 14-response models outperformed the larger, 50-response models, but these differences were very small. There is one exception to this pattern, with the larger models performing slightly better for \param{trans\-i\-tives}. Keeping in mind that in all cases, the heads and dependents within dependencies are lemmatized, these observations suggest that like \feat{an\-swer\-hood}, \feat{gram\-mat\-i\-cal\-ity} is relatively simple to assess, requiring only a small ``bag of lemmatized dependents'' model.


%% GRAMMATICALITY MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Fam\-il\-iar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.863 & 0.864 & \textbf{0.873}          & 0.887 & 0.863 & 0.868 & 0.869 & \textit{\textbf{0.874}} & 0.887 & 0.869 \\ \hline
\param{Tran}  & 0.760 & 0.759 & \textit{\textbf{0.762}} & 0.781 & 0.760 & 0.752 & 0.754 & \textbf{0.757}          & 0.781 & 0.758 \\ \hline
\param{Ditr}  & 0.678 & 0.685 & \textbf{0.698}          & 0.695 & 0.698 & 0.678 & 0.680 & \textit{\textbf{0.699}} & 0.695 & 0.696 \\ \hline
\hline
\param{Targ}  & 0.776 & 0.776 & \textit{\textbf{0.787}} & 0.800 & 0.783 & 0.776 & 0.777 & \textbf{0.786}          & 0.800 & 0.786 \\ \hline
\param{Untg}  & 0.757 & 0.762 & \textit{\textbf{0.768}} & 0.776 & 0.764 & 0.756 & 0.759 & \textbf{0.767}          & 0.776 & 0.763 \\ \hline
\hline
Total & 0.767 & 0.769 & \textit{\textbf{0.778}} & 0.788 & 0.773 & 0.766 & 0.768 & \textbf{0.776}          & 0.788 & 0.774 \\ \hline
\end{tabular}

\caption{\label{tab:gramm-fam-map}Mean Average Precision (MAP) scores for the \feat{gram\-mat\-i\-cal\-ity} annotation feature, comparing \param{fam\-il\-iar} and \param{crowd\-sourced} responses. MAP is derived from various response rankings: the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), weighted annotation ranking (WAR), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{tar\-get\-ed} items), and for the full set (Total). Note that all models represented here are \param{mix\-ed} due to the small number of \param{fam\-il\-iar} participants.
}
\end{center}
\end{table}

Table~\ref{tab:gramm-fam-map} presents the \feat{gram\-mat\-i\-cal\-ity} MAP scores for \param{fam\-il\-iar} and \param{crowd\-sourced} models. As before, the models here show a consistent preference for the dependent-only (\param{xdx}) \param{term rep\-re\-sent\-a\-tion}. The total MAP scores for  \param{fam\-il\-iar} and \param{crowd\-sourced} models were nearly identical, with \param{fam\-il\-iar} models only 0.002 points higher. In fact, the largest difference between any \param{fam\-il\-iar} model MAP and its corresponding \param{crowd\-sourced} MAP was only 0.005 (\param{fam\-il\-iar} being higher), which again suggests that this feature is relatively easy to account for, as even small \param{crowd\-sourced} models were competitive.

%\param{Trans\-i\-tives} stand out again here, where the best \param{fam\-il\-iar} model score is approximately 0.005 higher than the best \param{crowd\-sourced} score, which is the largest such difference in the table; for \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives}, the \param{crowd\-sourced} models are higher by approximately 0.001 points. This suggests that in describing transitive events, \param{fam\-il\-iar} NSs behave more like NNSs than do \param{crowd\-sourced} NSs, at least with regard to grammar usage. Participant motivation is the mostly likely factor here.

\subsection{\feat{in\-ter\-pret\-a\-bil\-ity} Experiments}
\label{sec:map-interp}

\feat{in\-ter\-pret\-a\-bil\-ity}, as discussed in Section~\ref{sec:scheme}, assesses whether a response evokes a clear mental image, although it does not need to resemble the actual image in the PDT item. This feature also requires a response's verb to have all necessary arguments specified.

%%INTERPRETABILITY MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.932                   & 0.931 & \textit{\textbf{0.933}} & 0.930 & 0.922 & 0.928          & 0.927 & \textbf{0.933} & 0.930 & 0.923 \\
\hline
\param{Tran}  & \textit{\textbf{0.823}} & 0.821 & 0.811                   & 0.803 & 0.806 & \textbf{0.821} & 0.816 & 0.812                   & 0.803 & 0.804 \\
\hline
\param{Ditr}  & 0.789                   & 0.784 & \textit{\textbf{0.794}} & 0.721 & 0.777 & 0.786          & 0.782 & 
\textbf{0.792}          & 0.721 & 0.772 \\
\hline
\hline
\param{Targ}  & 0.835                   & 0.832 & \textit{\textbf{0.836}} & 0.804 & 0.828 & 0.833          & 0.829 & \textbf{0.834}          & 0.804 & 0.826 \\
\hline
\param{Untg}  & \textit{\textbf{0.862}} & 0.858 & 0.856                   & 0.833 & 0.842 & \textbf{0.857} & 0.855 & \textbf{0.857}          & 0.833 & 0.840 \\
\hline
\hline
\param{Prim}  & \textit{\textbf{0.847}} & 0.845 & 0.846                   & 0.818 & 0.837 & 0.845          & 0.842 & \textbf{0.846}          & 0.818 & 0.833 \\
\hline
\param{Mix}   & \textit{\textbf{0.849}} & 0.846 & 0.846                   & 0.818 & 0.833 & 0.844          & 0.841 & \textbf{0.845}          & 0.818 & 0.833 \\
\hline
\hline
Total & \textit{\textbf{0.848}} & 0.845 & 0.846                   & 0.818 & 0.835 & \textbf{0.845} & 0.842 & \textbf{0.845}          & 0.818 & 0.833 \\
\hline
\end{tabular}
\caption{\label{tab:interp-map}Mean Average Precision (MAP) scores for the \feat{in\-ter\-pret\-a\-bil\-ity} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{prim\-a\-ry} NS models), and for the full set (Total).
}
\end{center}
\end{table}

Table~\ref{tab:interp-map} presents \feat{in\-ter\-pret\-a\-bil\-ity} MAP scores for the 14-response and 50-response \param{crowd\-sourced} models. The most prominent trend here is that the smaller, 14-response models outperformed the 50-response models in all cases. (The top \param{in\-trans\-i\-tive} scores are both shown as 0.933, but these are truncated for space and the smaller model score was actually higher by 0.0003.) These differences across model sizes were small, at roughly 0.002 points or less each; the \param{un\-tar\-get\-ed} setting was the only exception, with a difference of approximately 0.005 points. This suggests that with regard to \feat{in\-ter\-pret\-a\-bil\-ity}, the \param{un\-tar\-get\-ed} PDT setting elicits a greater degree of noise than does the \param{tar\-get\-ed} setting, and this plays a larger role in the larger models. In other words, the larger the sample size of the \param{crowd\-sourced} NS model, the more likely it will include responses which do not align well with those of NNSs. Thus, when attempting to infer the \feat{in\-ter\-pret\-a\-bil\-ity} of an NNS response based on models containing outlier NS responses, performance suffers. This is almost certainly tied to motivation, and it tracks with my own observations from the data. An \param{un\-tar\-get\-ed} prompt gives the participant greater freedom---or perhaps ``creative license''---to describe the image. NNSs and \param{fam\-il\-iar} NSs  stuck to the spirit of the task, whereas off-target responses are easy to find among the \param{crowd\-sourced} NS data. For example, for the item showing a woman teaching a math class to a student, \param{crowd\-sourced} NSs in the \param{tar\-get\-ed} setting overwhelmingly referred to the action of teaching. Responses in the \param{un\-tar\-get\-ed} setting include several that did not address the action (and incidentally, failed at \feat{an\-swer\-hood} as well), like ``school'' and ``Time for math''; multiple others simply transcribed the math problems shown on the chalkboard or commented inappropriately on the teacher's appearance. None of these responses are helpful in capturing the more constrained behavior of the NNS participants, and future work should investigate methods to automatically filter out low-quality NNS responses.


%% INTERPRETABILITY MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Fam\-il\-iar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.930          & 0.930 & \textit{\textbf{0.934}} & 0.930 & 0.923 & \textbf{0.933}          & 0.931 & 0.932          & 0.930 & 0.922 \\ \hline
\param{Tran}  & \textbf{0.822} & 0.819 & 0.811                   & 0.803 & 0.805 & \textit{\textbf{0.826}} & 0.824 & 0.811          & 0.803 & 0.805 \\ \hline
\param{Ditr}  & 0.787          & 0.786 & \textit{\textbf{0.796}} & 0.721 & 0.782 & 0.788                   & 0.783 & \textbf{0.795} & 0.721 & 0.772 \\ \hline
\hline
\param{Targ}  & 0.835          & 0.833 & \textit{\textbf{0.836}} & 0.804 & 0.830 & \textbf{0.835}          & 0.832 & \textbf{0.835} & 0.804 & 0.825 \\ \hline
\param{Untg}  & \textbf{0.858} & 0.857 & \textbf{0.858}          & 0.833 & 0.843 & \textit{\textbf{0.863}} & 0.859 & 0.857          & 0.833 & 0.841 \\ \hline
\hline
Total & \textbf{0.847} & 0.845 & \textbf{0.847}          & 0.818 & 0.837 & \textit{\textbf{0.849}} & 0.846 & 0.846          & 0.818 & 0.833 \\ \hline
\end{tabular}
\caption{\label{tab:interp-fam-map}Mean Average Precision (MAP) scores for the \feat{in\-ter\-pret\-a\-bil\-ity} annotation feature, comparing \param{fam\-il\-iar} and \param{crowd\-sourced} responses. MAP is derived from various response rankings: the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), weighted annotation ranking (WAR), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{tar\-get\-ed} items), and for the full set (Total). Note that all models represented here are \param{mix\-ed} due to the small number of \param{fam\-il\-iar} participants.
}
\end{center}
\end{table}

Another trend seen here is that although the effect of \param{term rep\-re\-sent\-a\-tion} was small, the highest MAP scores for \param{trans\-i\-tives} came from labeled dependencies, whereas dependent-only representations worked best for \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives}. The dependent-only representation collapses labeled dependencies into a smaller number of terms, reducing the distance between NS models and NNS responses (as seen in Table~\ref{tab:param-response-distances}). Given this fact, the trend here suggests that the NNS participants and \param{crowd\-sourced} NS participants exhibited more convergent behavior in response to \param{trans\-i\-tive} items (as opposed to \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives}), and thus a relatively granular representation (labeled dependencies) worked well in similarity measures, leading to better discriminatory power for the \feat{in\-ter\-pret\-a\-bil\-ity} feature.

Turning to comparisons of \param{fam\-il\-iar} and \param{crowd\-sourced} NS models shown in Table~\ref{tab:interp-fam-map}, this pattern was repeated, with labeled dependencies performing best for \param{trans\-i\-tives} and dependents-only performing best for \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives}. Overall, the \param{crowd\-sourced} models performed better than the \param{fam\-il\-iar} models, but this difference is very slight.



\subsection{\feat{ver\-i\-fi\-a\-bil\-ity} Experiments}
\label{sec:map-verif}

\feat{ver\-i\-fi\-a\-bil\-ity}, as discussed in Section~\ref{sec:scheme}, requires that all information presented in a response must be clearly verifiable from the PDT image.


%%VERIFIABILITY MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.852                   & 0.852 & \textit{\textbf{0.853}} & 0.866 & 0.840 & 0.849                   & 0.849          & \textbf{0.851} & 0.866 & 0.836 \\
\hline
\param{Tran}  & \textit{\textbf{0.809}} & 0.808 & 0.803                   & 0.798 & 0.787 & \textbf{0.807}          & 0.806 & 0.803          & 0.798 & 0.785 \\
\hline
\param{Ditr}  & 0.814                   & 0.812 & \textit{\textbf{0.815}} & 0.780 & 0.798 & 0.811                   & 0.809          & \textbf{0.812} & 0.780 & 0.796 \\
\hline
\hline
\param{Targ}  & \textit{\textbf{0.825}} & 0.824 & 0.825 & 0.815 & 0.812 & \textbf{0.825} & 0.824          & 0.823          & 0.815 & 0.810 \\
\hline
\param{Untg}  & \textit{\textbf{0.825}} & 0.824 & 0.822                   & 0.815 & 0.805 & \textbf{0.820}          & 0.819          & 0.820 & 0.815 & 0.802 \\
\hline
\hline
\param{Prim}  & \textit{\textbf{0.826}} & 0.824 & 0.823                   & 0.815 & 0.808 & \textbf{0.824}          & 0.823          & 0.822          & 0.815 & 0.806 \\
\hline
\param{Mix}   & \textit{\textbf{0.825}} & 0.824 & 0.824                   & 0.815 & 0.808 & \textbf{0.821}          & 0.821 & 0.821 & 0.815 & 0.805 \\
\hline
\hline
Total & \textit{\textbf{0.825}} & 0.824 & 0.824                   & 0.815 & 0.808 & \textbf{0.823}          & 0.822          & 0.822          & 0.815 & 0.806 \\
\hline
\end{tabular}
\caption{\label{tab:verif-map}Mean Average Precision (MAP) scores for the \feat{ver\-i\-fi\-a\-bil\-ity} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{prim\-a\-ry} NS models), and for the full set (Total).
}
\end{center}
\end{table}


As seen in Table~\ref{tab:verif-map}, for discriminating the \feat{ver\-i\-fi\-a\-bil\-ity} feature, the smaller 14-response models consistently outperformed the 50-response models by a slim margin. This effect was greatest for \param{un\-tar\-get\-ed} and \param{mix\-ed} settings. As compared to their counterparts (\param{tar\-get\-ed} and \param{prim\-a\-ry} settings, respectively), these settings resulted in greater response distances from the NS models (see Table~\ref{tab:param-response-distances}). These are relatively unconstrained settings, both eliciting a wider variety of responses and potential noise, and such outlier responses have a greater chance of appearing in larger models. This is supported by the fact that MAP differences between \param{tar\-get\-ed} and \param{un\-tar\-get\-ed} settings and \param{prim\-a\-ry} and \param{mix\-ed} settings were greater for the 50-response models than the 14-response models. The results seen here show that the \feat{ver\-i\-fi\-a\-bil\-ity} feature was sensitive to this pattern across model sizes. 

These results also show that \param{term rep\-re\-sent\-a\-tion} had a very small effect on discriminating for \feat{ver\-i\-fi\-a\-bil\-ity}. The effect was greatest, however, when comparing \param{in\-trans\-i\-tive}, \param{trans\-i\-tive} and \param{di\-trans\-i\-tive} items. While \param{in\-trans\-i\-tives} and \param{di\-trans\-i\-tives} showed a slight preference for dependents-only \param{term rep\-re\-sent\-a\-tion}, \param{trans\-i\-tive} items worked best with labeled dependencies (\param{ldh}), regardless of model size. Again, this indicates that NS and NNS behavior was most convergent in response to \param{trans\-i\-tive} items, allowing for a relatively granular representation. \param{Trans\-i\-tives} also stand out here because they resulted in the lowest MAP scores for \feat{ver\-i\-fi\-a\-bil\-ity}. The reasons for this are not obvious, but it may be that the relatively clear and concrete nature of transitive PDT items means annotators behaved more strictly when marking \feat{ver\-i\-fi\-a\-bil\-ity} for \param{trans\-i\-tives} than for \param{in\-trans\-i\-tives} or \param{di\-trans\-i\-tives}.


%% VERIFIABILITY MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Fam\-il\-iar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT & \param{ldh}	& \param{xdh} &	\param{xdx} & WAR	& SBERT \\ \hline
\hline
\param{Intr}  & 0.847                   & 0.847 & \textbf{0.852} & 0.866 & 0.836 & 0.852                   & 0.852 & \textit{\textbf{0.854}} & 0.866 & 0.843 \\ \hline
\param{Tran}  & \textit{\textbf{0.808}} & 0.807 & 0.803          & 0.798 & 0.787 & \textbf{0.807}          & 0.807 & 0.802                   & 0.798 & 0.786 \\ \hline
\param{Ditr}  & 0.811                   & 0.811 & \textbf{0.812} & 0.780 & 0.802 & 0.815                   & 0.812 & \textit{\textbf{0.817}} & 0.780 & 0.796 \\ \hline
\hline
\param{Targ}  & 0.821                   & 0.821 & \textbf{0.822} & 0.815 & 0.814 & 0.824                   & 0.824 & \textit{\textbf{0.826}} & 0.815 & 0.811 \\ \hline
\param{Untg}  & \textbf{0.824}          & 0.822 & 0.823          & 0.815 & 0.803 & \textit{\textbf{0.825}} & 0.824 & 0.823                   & 0.815 & 0.806 \\ \hline
\hline
Total & 0.822                   & 0.822 & \textbf{0.823} & 0.815 & 0.808 & \textit{\textbf{0.825}} & 0.824 & 0.824                   & 0.815 & 0.808 \\ \hline
\end{tabular}
\caption{\label{tab:verif-fam-map}Mean Average Precision (MAP) scores for the \feat{ver\-i\-fi\-a\-bil\-ity} annotation feature, comparing \param{fam\-il\-iar} and \param{crowd\-sourced} responses. MAP is derived from various response rankings: the three system \param{term rep\-re\-sent\-a\-tion} rankings (labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}), and dependents only (\param{xdx})), weighted annotation ranking (WAR), and SBERT rankings. MAP scores are shown for each item type or parameter setting (e.g, \param{in\-trans\-i\-tive} items, \param{tar\-get\-ed} items), and for the full set (Total). Note that all models represented here are \param{mix\-ed} due to the small number of \param{fam\-il\-iar} participants.
}
\end{center}
\end{table}

When comparing \param{fam\-il\-iar} and \param{crowd\-sourced} models for discriminating for \feat{ver\-i\-fi\-a\-bil\-ity}, as shown in Table~\ref{tab:verif-fam-map}, a slight preference for \param{crowd\-sourced} models is evident. \param{Trans\-i\-tives} were again an exception here, where \param{fam\-il\-iar} models slightly outperformed \param{crowd\-sourced} models on this task.

The \param{crowd\-sourced} 14-response models covered in Table~\ref{tab:verif-fam-map} contained \textit{only} \param{mix\-ed} response models and were thus not identical to the the \param{crowd\-sourced} 14-response models covered in Table~\ref{tab:verif-map}, which also included \param{prim\-a\-ry} models. To clarify, the \param{crowd\-sourced} 14-response \textit{total} row in Table~\ref{tab:verif-fam-map} is equivalent to the \param{mix\-ed} row in Table~\ref{tab:verif-map}. In terms of performance, the \param{crowd\-sourced} results in Table~\ref{tab:verif-fam-map} differ from those in Table~\ref{tab:verif-map} in that they show dependents-only representations working best for \param{tar\-get\-ed} settings, but labeled dependencies (\param{ldh}) working best for \param{un\-tar\-get\-ed} settings. The \param{fam\-il\-iar} models also exhibited this pattern. One explanation here is related to the fact that  \param{un\-tar\-get\-ed} settings elicit more undesirable responses; it is not uncommon for such responses to mention the entities and actions in the image but somehow mischaracterize the scenario. Such responses will appear more like the NS model given a dependents-only setting than they do with a labeled dependencies setting. In other words, \feat{ver\-i\-fi\-a\-bil\-ity} is challenging to capture with a bag-of-words style similarity approach because it means verifying not only the words but also their relationships.


\section{Holistic Experiments}
\label{sec:exp-holistic}
In this section I turn from the use of similarity scoring to discriminate annotations for individual features to the use of similarity scoring to approximate an ideal holistic ranking of NNS responses. These experiments rely on the weighted annotation ranking (WAR), which is based on the weighted annotation score (WAS) for each NNS response, as described in Section~\ref{sec:holistic-scoring}. While in many use cases it may be preferable to focus on specific features, the experiments here provide insight into the feasibility of using annotation-free, surface-level similarity measures to approximate a determination of response ``goodness'' based on human judgment.

This section is broken into subsections for each parameter, where I compare the performance of different settings for that parameter. Ideally, this would allow me to identify which parameter settings work best and in which contexts (i.e., with which item types and in combination with which other parameter settings). Such trends could then suggest how to configure my system for new items. Clear, predictive trends were not always evident here, however, and the observations are likely not robust enough for immediate application. Instead, they are presented in the hopes that they can guide future investigations into topics like ICALL design and the variability of NNS language.

As before, I compared models of two different sizes in order to see the effects of sample size on performance. The smaller of the two models contained 14 NS responses per item, and the larger model contained 50 NS responses per item. My system used these models as the basis of its tf-idf cosine similarity measure that was used to score each response and in turn rank the full set of 70 NNS responses. As a benchmark, I also used SBERT with the same NS models to produce similarity scores and rank the NNS responses.

The metric used throughout this section is Spearman's rank correlation coefficient, as implemented in the SciPy Python package \cite{2020scipy} and described in \citet{zwillinger1999crc}. I used this metric to assess how well each system (or SBERT) ranking of the NNS responses correlated with the weighted annotation ranking (WAR). I chose Spearman over similar measures, such as Pearson correlation, because Spearman has been found to be better suited for semantic textual similarity tasks \cite{reimers2016}.

A caveat is in order here. A Spearman score is always accompanied by a p-value, which is a measure of statistical significance. The p-value gives some indication of the probability of achieving the correlation score under the null hypothesis; in other words, how likely is the observed correlation (or a stronger one) if there is no real relationship between the two rankings? A long-standing tenet holds that p-values should be less than 0.05 to indicate statistical significance \cite{zar1972significance}. In recent years, the use of p-values and claims around them has been the subject of much debate in numerous fields, and the reliance on p-values in linguistic research has slipped from an unassailable orthodoxy to a point of contention \cite{moran2012revisiting, tomczak2014need}. For the results presented here, roughly half of p-values fell below 0.05, but many exceeded it by varying degrees. This was largely a function of the relatively small size of the test sets (70 NNS responses); the SciPy authors note that p-values ``are not entirely reliable but are probably reasonable for datasets larger than 500 or so'' \cite{2020scipy}. It is important to note that p-values above 0.05 do not indicate that the null hypothesis \textit{is} true, but rather that the evidence is not statistically significant enough to claim a correlation exists \cite{vasishth2016statistical}.

It is also important to note that higher p-values naturally occur in cases where the annotations underlying the rankings are heavily skewed. Many of the PDT items in this study exhibited a ceiling effect, where a majority of responses received perfect annotations. For the inter-annotator response set used in developing the annotation scheme, the overall rate of positive annotations ranged from 0.717 to 0.872 for all five features (see Table~\ref{tab:agreement}). For the full corpus, the overall rate of perfectly annotated (i.e, 5/5 positive feature annotations) responses was 0.614 for NNSs and 0.528 for NSs (see Table~\ref{tab:was1}).
This resulted in an unevenly distributed weighted annotation ranking, with the perfect responses tied in rank. Tied ranks were also common among less-than-perfect responses. For example, it was common for NNS responses to receive positive annotations for all features except \feat{gram\-mat\-i\-cal\-ity}, and thus these responses shared a weighted annotation rank, resulting skewed rankings and high p-values. Applying the annotation feature weights helped differentiate response scores, but with only five features, such ties were still common. Moreover, skewedness was common in the system-produced rankings as well, because it was common for NNSs to provide identical responses, especially in \param{tar\-get\-ed} PDT settings (as seen in the response-level TTRs in Table~\ref{tab:ttr}).

With regard to the analysis in this section, one could opt to handle concerns about p-values by omitting any results where the Spearman correlation p-value is above the 0.05 threshold. This would leave a patchwork of results that are unevenly distributed with regard to the various item types, parameter settings, and model sizes that I am interested in comparing, effectively complicating and limiting the analysis. Another option would be to point out and attempt to explain wherever results involve p-values above 0.05, but this is not practically feasible. I opted instead to avoid presenting or interpreting p-values and the significance of Spearman correlations altogether and instead present my findings here with the caveat that they may not always be statistically significant. They should not be relied upon for any immediate decision-making. These findings are likely to indicate useful trends, but analysis with much larger datasets should be explored before making any determinations about the statistical significance of these results or the reliability and predictive power of trends seen here.

To help mitigate this fact, I report Spearman correlations not as, for example, a single mean for each setting within a parameter, but as a set of descriptive statistics including mean, median, minimum, maximum and standard deviation. These figures should give a better depiction of the shape of the results than a single mean score and thus better indicate potential trends and guide future research.

\subsection{\param{Term Norm\-al\-iz\-a\-tion} Experiments}
\label{sec:exp-term-norm}

In my pipeline, the NS model for each PDT item is comprised of some number of NS responses.  The length of these responses can vary; some valid responses contain only one or two words\footnote{Participants were instructed to provide complete sentences, but incomplete sentences were still judged valid where appropriate; see Chapter~\ref{chap:data} and the Annotation Guide in Appendix~\ref{appendix:annotation_guide}.}, while the longest perfectly annotated responses top out at around 15 words and some less-than-perfect responses exceed 30 words. Understanding the impact of response lengths on a model is an important step in optimizing my response rating process. So far, I treated each NS model as a flat ``bag of terms'' in which each term (roughly, \textit{dependency}; see Sections~\ref{sec:response-rep} and~\ref{sec:exp-term-reps}) contributes equally, meaning longer responses carry more weight than shorter responses in the model. This has the potential to introduce noise. 

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Response A & Response B & \param{Norm} wt & \param{Non-norm} wt\\
\hline
\multirow{2}{*}{The girl is singing} & The girl in the cute purple & & \\
& dress is singing a song & & \\
\hline
\hline
det(the, girl) & det(the, girl) & 0.175 & 0.143 \\
\hline
nsubj(girl, sing) & nsubj(girl, sing) & 0.175 & 0.143 \\
\hline
%& erased(in, ERASED) & 0.050 & 0.071 \\
%& \textit{erased(in, ERASED)} & 0.000 & 0.000 \\
& \textit{erased(in, ERASED)} & --- & --- \\
\hline
& det(the, dress) & 0.050 & 0.071 \\
\hline
& amod(cute, dress) & 0.050 & 0.071 \\
\hline
& amod(purple, dress) & 0.050 & 0.071 \\
\hline
%& prep\_in(dress, girl) & 0.050 & 0.071 \\
& prep\_in(dress, girl) & 0.050 & 0.071 \\
\hline
aux(be, sing) & aux(be, sing) & 0.175 & 0.143 \\
\hline
root(sing, ROOT) & root(sing, ROOT) & 0.175 & 0.143 \\
\hline
& det(a, song) & 0.050 & 0.071 \\
\hline
& dobj(song, sing) & 0.050 & 0.071 \\
\hline
\hline
4 & 10 & 1.0 & 1.0 \\
\hline
\end{tabular}
\caption{\label{tab:normalize-responses-deps} A ``toy'' model consisting of lemmatized syntactic dependencies from only two NS responses, each with perfect annotation scores. (Note that the version of Stanford typed dependencies used in this work collapses dependencies containing prepositions and incorporates prepositions in a label, resulting in the ``prep\_in'' and ``erased'' dependencies above. See Section~\ref{sec:rule-method} for more on the parsing and lemmatization.)}
\end{center}
\end{table}

My hypothesis is that system performance should improve by using NS models where each term token is re-weighted by $1/\textit{n}$ before it is added to the model, where \textit{n} is the number of term tokens in the response containing the term token. In other words, dependencies should be re-weighted to ensure that every NS \textit{response}---not \textit{term}---contributes equal weight to the model. The rationale here is simple. Every response used in the NS model is assumed to contain the information that is crucial for satisfying the PDT prompt, and the number of terms conveying this information is roughly equivalent from one response to another. Thus, as the number of terms in a response increases (above some minimum number), the likelihood that any given term in that response is crucial decreases.

This is illustrated by the responses in Table~\ref{tab:normalize-responses-deps}. If we take these two responses to constitute one NS model, Response A contributes four dependencies, each of which is necessary to satisfy the five annotated features and contributes meaningfully to the model. Response B, however, contributes 10 dependencies, some of which, like \textit{amod(purple, dress)}, add non-critical (but still verifiable and accurate) detail. In a \param{non-norm\-al\-iz\-ed} setting, this dependency constitutes one out of a total 14 dependencies in the NS model, approximately 0.071. In a \param{norm\-al\-iz\-ed} setting, however, this dependency appears as zero of four (0.0) dependencies in Response A, and one of 10 (0.1) in Response B, making it 0.05 of the overall model (0.1 divided by two responses). This should have the effect of making extraneous information in the model less impactful on response ratings.

\begin{table}[htb!]
\begin{center}
%\begin{tabular}{|l||p{0.12\textwidth}|p{0.12\textwidth}|p{0.12\textwidth}||l|l|l|}
\begin{tabular}{|l||l|l|l||l|l|l|}
\hline
 & \multicolumn{3}{c||}{NS model sample size = 14} & \multicolumn{3}{c|}{NS model sample size = 50} \\
\hline
		& \param{Non-n} 		& \param{Norm} 			& SBERT 								& \param{Non-n} 			& \param{Norm} 				& SBERT 		\\
\hline
\hline
count 	& 360 			& 360 			& 120 								& 360 				& 360 				& 120		 \\
\hline
mean 	& \textbf{0.340} & 0.335 & \textit{\textbf{0.487}}					& \textbf{0.349} 	& 0.347 		& \textit{\textbf{0.509}}		 \\
\hline
median 	& \textbf{0.332} & 0.313 & \textit{\textbf{0.507}} 					& \textbf{0.348} 	& 0.333 		& \textit{\textbf{0.523}}		 \\
\hline
min 	& \textbf{-0.181} 	& -0.219 & \textit{\textbf{-0.138}} 			& \textbf{-0.185} 	& -0.230 		& \textit{\textbf{-0.090}}		 \\
\hline
max	& \textit{\textbf{0.900}} & 0.891 	& 0.881 							& 0.898 	& \textit{\textbf{0.899}} 	& 0.881		 \\
\hline
std dev & 0.226 		& 0.227 		& 0.196 							& 0.225 		& 0.230 				& 0.177		 \\
\hline
\end{tabular}
\caption{\label{tab:term-norm-results} Comparing Spearman rank correlation scores where all dependencies (terms) in \param{Non-n(ormalized)} NS models carry equal weight, and all dependencies in \param{Norm(alized)} NS models have their scores normalized proportionally to the length of the parent response. Results are shown using NS models of 14 responses and 50 responses. Each \param{Norm(alized)} and \param{Non-n(ormalized)} column represents 360 different rankings (12 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{SBERT} column represents 120 rankings (4 system configurations $\times$ 30 items; SBERT operates on plain text, so the \param{term rep\-re\-sent\-a\-tion} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}


Taking this example further, consider the dependency \textit{nsubj(girl, sing)}, also from Table~\ref{tab:normalize-responses-deps}. In the \param{non-norm\-al\-iz\-ed} setting, this dependency appears as two out of a total 14 dependencies, or 0.143 of the NS model. In the \param{norm\-al\-iz\-ed} setting, the dependency appears as one out of four (0.25) dependencies in Response A, and one out of 10 (0.1) dependencies in Response B, equating to 0.175 of the NS model (0.35 divided by two responses). Because this dependency is critical, raising its weight from 0.143 to 0.175 should have a positive impact on system performance; NNS responses containing the dependency should rise in the rankings relative to those without it. 

To test my hypothesis, I compared the performance of \param{norm\-al\-iz\-ed} and \param{non-norm\-al\-iz\-ed} models. 
%This followed my standard design of ranking NNS responses by their system scores, then comparing this ranking with the weighted annotation ranking using Spearman rank correlation. I isolate this variable as before: for all system configurations, I generate a Spearman score. Then I obtain an average score for all normalized configurations and an average score for all non-normalized configurations and compare the two.
%Such normalization can be sensitive to the effects of size\lk{XYZ: Citation?}, so I conducted two experiments: one in which each NS model contains a sample 50 responses, and one in which each model contains 14 responses.
The results of this experiment are shown in Table~\ref{tab:term-norm-results}. This re-weighting does not apply to SBERT, but SBERT scores are included as a benchmark. For both sample sizes, the comparisons showed slightly higher mean and median Spearman correlations for the \param{non-norm\-al\-iz\-ed} NS models, disproving my hypothesis. This means that the desired NNS response ranking---the weighted annotation ranking---is best approximated via similarity comparisons with the \param{non-norm\-al\-iz\-ed} NS responses. In other words, the level of noise in the NS responses is already optimal prior to applying the normalization.
% and with its impact reduced via \param{term norm\-al\-iz\-a\-tion}, the NS models are \textit{less} like the NNS responses. 
Because the \param{non-norm\-al\-iz\-ed} NS models outperformed their \param{norm\-al\-iz\-ed} counterparts, and for the sake of simplicity, this parameter was not included in the other experiments discussed in this chapter; only \param{non-norm\-al\-iz\-ed} configurations were used elsewhere.

\subsection{\param{Trans\-i\-ti\-vi\-ty} Experiments}
\label{sec:exp-transitivity}
Here I examine the performance of my ranking system when applied to items that are canonically either intransitive, transitive, or ditransitive. Unlike the other variables throughout this chapter, \param{trans\-i\-ti\-vi\-ty} is not a parameter setting. Individual PDT items are assumed to fit predominately only one of the three types here. In other words, I cannot choose to process a given PDT item with any of the three \param{trans\-i\-ti\-vi\-ty} settings. Rather, the experiments in this section examine my system's performance across three sets of 10 items each, representing \param{in\-trans\-i\-tive}, \param{trans\-i\-tive} and \param{di\-trans\-i\-tive} events.


\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l|}
\hline
 & \multicolumn{6}{c|}{NS model sample size = 14} \\
\hline
 & \multicolumn{2}{c||}{\param{in\-trans\-i\-tives}} & \multicolumn{2}{c||}{\param{trans\-i\-tives}} & \multicolumn{2}{c|}{\param{di\-trans\-i\-tives}} \\
\hline
		& System 	& SBERT 		& System 	& SBERT 		& System 	& SBERT 		\\
\hline
\hline
count 	& 120 		& 40 		& 120 		& 40 		& 120 		& 40		 \\
\hline
mean 	& 0.439 	& \textbf{0.497} 	& 0.314 	& \textit{\textbf{0.563}}		& 0.267 	& \textbf{0.400}	 \\
\hline
median 	& 0.416 	& \textbf{0.479} 	& 0.304 	& \textit{\textbf{0.555}}		& 0.276 	& \textbf{0.444}	 \\
\hline
min 	& -0.119 	& \textit{\textbf{0.199}} 	& -0.110 	& \textit{\textbf{0.199}}	& -0.181 	& \textbf{-0.138} \\
\hline
max 	& \textit{\textbf{0.900}} 	& 0.881		& \textbf{0.777} 	& 0.772		& \textbf{0.710} 	& 0.697	 \\
\hline
std dev & 0.228 	& 0.189		& 0.218 	& 0.134		& 0.198 	& 0.222	 \\
\hline
\multicolumn{7}{c}{} \\
\hline
 & \multicolumn{6}{c|}{NS model sample size = 50} \\
\hline
 & \multicolumn{2}{c||}{\param{in\-trans\-i\-tives}} & \multicolumn{2}{c||}{\param{trans\-i\-tives}} & \multicolumn{2}{c|}{\param{di\-trans\-i\-tives}} \\
\hline
		& System 	& SBERT 				& System 	& SBERT 						& System 	& SBERT \\
\hline
\hline
count 	& 120 		& 40 				& 120 		& 40 						& 120 		& 40 	\\
\hline
mean 	& 0.423 	& \textbf{0.516} 	& 0.345 	& \textit{\textbf{0.566}}	& 0.278 	& \textbf{0.446} \\
\hline
median 	& 0.426 	& \textbf{0.517}	& 0.331 	& \textit{\textbf{0.561}}	& 0.286 	& \textbf{0.471} \\
\hline
min 	& -0.076 	& \textbf{0.200}	& -0.204 	& \textit{\textbf{0.222}}	 & -0.185 	& \textbf{-0.090} \\
\hline
max & \textit{\textbf{0.898}} & 0.881	& \textbf{0.778} & 0.771	 			& 0.708 	& \textbf{0.709} \\
\hline
std dev & 0.249 	& 0.172 			& 0.207 	& 0.135 					& 0.195 	& 0.200 \\
\hline
\end{tabular}
\caption{\label{tab:transitivity-results} Comparing Spearman rank correlation scores for \param{in\-trans\-i\-tive}, \param{trans\-i\-tive} and \param{di\-trans\-i\-tive} PDT items, using NS models of either 14 or 50 random responses per item. Each \textit{System} column represents 120 different rankings (12 system configurations $\times$ 10 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{SBERT} column represents 40 rankings (4 system configurations $\times$ 10 items; SBERT operates on plain text, so the \param{term rep\-re\-sent\-a\-tion} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}




%
%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.6em}
%\begin{tabular}{|l|l||l|l|l||l|l|l|}
%\hline
% && \multicolumn{3}{c||}{NS model = 14} & \multicolumn{3}{c|}{NS model = 50} \\
%\hline
%	&	& Intrans 	& Trans 	& Ditrans 		& Intrans 	& Trans 	& Ditrans 		\\
%\hline
%\multirow{3}{*}{\begin{sideways}Words/Response~ \end{sideways}} & responses	& 560  & 560 & 560 & 2000 & 2000 & 2000	 \\
%\cline{2-8}
%& mean 		& 4.9 	& 6.2		& \textbf{7.1} 	& 4.9		& 6.2 	& \textbf{7.3}	 \\
%\cline{2-8}
%& median 	& 4.0 	& 6.0		& \textbf{7.0} 	& 4.0		& 6.0 	& \textbf{7.0}	 \\
%\cline{2-8}
%& min 		& 1.0 	& 1.0		& 1.0 			& 1.0		& 1.0 	& 1.0	 \\
%\cline{2-8}
%& max 		& 19.0 	& 16.0	& \textbf{26.0} & \textbf{26.0}	& 24.0 	& \textbf{26.0}	 \\
%\cline{2-8}
%& std dev 	& 2.6 	& 2.5		& 3.2 			& 2.6		& 2.7 	& 3.2	 \\
%\hline
%\hline
%\multirow{3}{*}{\begin{sideways}Model TTR~~~~~ \end{sideways}} & models	& 40 & 40 & 40 	& 40 & 40 & 40	 \\
%\cline{2-8}
%& mean 		& 0.360 	& \textbf{0.329} 		& 0.334 	& 0.220		& 0.192 		& \textbf{0.189}	 \\
%\cline{2-8}
%& median 	& 0.369 	& \textbf{0.329}		& 0.337 	& 0.218		& 0.190 		& \textbf{0.184}	 \\
%\cline{2-8}
%& min 		& 0.169 	& \textbf{0.166}		& 0.210 	& 0.116		& 0.122 		& \textbf{0.084}	 \\
%\cline{2-8}
%& max 		& 0.543 	& 0.542			& \textbf{0.484} 	& 0.302		& \textbf{0.274} & 0.276	 \\
%\cline{2-8}
%& std dev 	& 0.101 	& 0.079					& 0.075 	& 0.046		& 0.040 		& 0.049	 \\
%\hline
%\end{tabular}
%\caption{\label{tab:transitivity-model-stats}Comparing the number of words per response and the model word type-to-token ratio (TTR) for \param{in\-trans\-i\-tive}, \param{trans\-i\-tive} and \param{di\-trans\-i\-tive} PDT items, using NS models of either 14 or 50 random responses per item.
%}
%\end{center}
%\end{table}

Sets of descriptive statistics for the Spearman rank correlation scores produced by my system and SBERT using these models are presented in Table~\ref{tab:transitivity-results}. There are 10 items per type. Each item has a \param{tar\-get\-ed} and \param{un\-tar\-get\-ed} version, which are separate datasets. For each dataset, I sampled both \param{prim\-a\-ry} and \param{mix\-ed} models. My system converts the response text to three different \param{term rep\-re\-sent\-a\-tions} (\param{ldh}, \param{xdh}, \param{xdx}). This results in 12 system configurations: 2 \param{tar\-get\-ing} settings $\times$ 2 \param{prim\-a\-cy} settings $\times$ 3 \param{term rep\-re\-sent\-a\-tions}. Using all 12 configurations results in 120 Spearman rank correlations per \param{trans\-i\-ti\-vi\-ty} type. The system statistics presented in Table~\ref{tab:transitivity-results} average over these 120 scores. Because SBERT operates on plain text and cannot make use of the \param{term rep\-re\-sent\-a\-tion} variable, it involves only four configurations, resulting in 40 Spearman scores per \param{trans\-i\-ti\-vi\-ty} type.

Table~\ref{tab:transitivity-results} shows some notable patterns. In all cases, SBERT outperformed my system on the most important metrics here: mean and median Spearman scores. This is expected, as SBERT is a powerful, highly developed, state-of-the-art approach to language modeling and sentence similarity and is trained on much more data. My similarity measure is far less powerful and involves tools trained on much smaller datasets, but also has the advantage of preserving a high degree of transparency and explainability. Although SBERT underperformed my system in predicting individual feature annotations in Section~\ref{sec:exp-annotations}, the results here would suggest that SBERT is relatively good at scoring and ranking responses holistically in a way that correlates with the WAR (and thus with human annotation).
%Future iterations of this work would be wise then to rely on BERT for scoring and ranking, and back off to my system when more transparency is needed, such as when providing feedback.

Notably, my system reached a higher maximum Spearman score than SBERT in all but one case. However, leveraging this fact for unseen and unannotated items would require consistently predicting cases in which system scores outperform SBERT scores, and I have not uncovered any pattern that would enable this.
%\lk{WHY? what's going on there?}

For both size models, my system performed best on \param{in\-trans\-i\-tives}, followed by \param{trans\-i\-tives} and then \param{di\-trans\-i\-tives}. For \param{in\-trans\-i\-tives}, my system achieved its highest mean and median Spearman scores using the 14-response models. For \param{trans\-i\-tives}, the median was highest with the 14-response models, but the mean was highest with the 50-response models. For \param{di\-trans\-i\-tives}, the mean and median were highest with the 50-response model. If we take standardized type-to-token ratio (STTR) as a metric of complexity (see Table~\ref{tab:sttr}), the observations here show a monotonic pattern in which more complex items were best handled with larger models. Future research could explore this curve by sampling a wider range of model sizes, including models larger than 50 responses. Assuming the pattern holds, for each new PDT item, one could use the item's STTR to help determine the optimal model size for ranking NNS responses.

%We may expect intransitives to be the simplest of these types, and ditransitives the most complex, because ditransitive verbs require more arguments. I examined type-to-token ratios (TTRs) for the words in NS models, and for the 50-response NS models, this pattern was shown to be true, as seen in Table~\ref{tab:sttr}. Combined with the TTRs,\lk{Let's revisit this with STTR} the performance observations above suggest a trend: where the responses can be expected to be relatively simple and constrained, my system performs better with a smaller sample, but where the responses are relatively varied, my system performs better with a larger sample. Future research could explore this curve by sampling a wider range of model sizes, ideally including models larger than 50 responses.

Like my system, SBERT achieved its highest mean and median Spearman scores with the smaller model. For both size models, SBERT performed best on \param{trans\-i\-tives}, however, followed by \param{in\-trans\-i\-tives} and then \param{di\-trans\-i\-tives}.
The relatively strong performance on \param{trans\-i\-tives} appears to be an inherent feature of BERT architectures (at least for English), as \citet{thrush2020} showed by testing BERT's predictions with novel verbs. They found that when BERT is shown a single instance of a novel verb without an object during training, BERT expects the verb to also occur with an object. The reverse was not true, however; novel verbs seen once in a transitive context are not expected to occur intransitively. This shows what \citet{thrush2020} call BERT's ``transitivity bias,'' which likely explains why I found SBERT performance to be weaker for \param{in\-trans\-i\-tives}, despite the fact those responses were generally shorter than for \param{trans\-i\-tives}.
%% This is for mBERT, so I'm not sure it applies here...
%, which research suggests is an inherent feature of BERT architectures. \citet{papadimitriou2021multilingual} examine a similar version of BERT's accuracy in encoding nouns as either a subject or an object. BERT does not produce these labels, so the researchers used embeddings corresponding to the nouns derived from BERT sentence embeddings and used them to train classifiers to recognize these noun roles in new sentences. For nominative languages like English, they found that BERT is highly accurate at the task overall, but least accurate at handling intransitive subjects; the most common errors being when the classifiers label these noun embeddings as objects. This is consistent with SBERT performance peaking on \param{trans\-i\-tives} in my study. The researchers found that BERT is more accurate with \textit{agent} role subjects than with \textit{experiencer} subjects. As the former are more consistently present in \param{trans\-i\-tive} items, this helps explain SBERT's higher performance on \param{trans\-i\-tives}. 

For \param{di\-trans\-i\-tives}, SBERT's lower performance may have be influenced by the increased complexity, but research suggests this is more directly related to sentence length, and \param{di\-trans\-i\-tive} items, naturally, were the longest in my dataset, as they require more verb arguments (see Table~\ref{tab:response-length}). \citet{warstadt2019} used classifiers trained on a corpus of sentences with corresponding BERT output and human grammaticality judgments and found that the classifiers' ability to predict human judgments from the BERT sentence embeddings correlated strongly with sentence length, with a sharp drop appearing for sentences of 11 words or more. Another factor may simply be the fact that transitive sentences are more common in the data these tools are trained on and thus easier for them to handle accurately.

\subsection{\param{Tar\-get\-ing} Experiments}
\label{sec:exp-targeting}

Comparing Spearman rank correlations across the \param{tar\-get\-ing} parameter reveals that \param{tar\-get\-ed} settings consistently outperformed \param{un\-tar\-get\-ed} settings, as shown in Table~\ref{tab:targeting-results}. That is, \param{tar\-get\-ed} settings allowed my similarity-based ranking system to rank NNS responses in a way that better captured responses' overall quality as represented by the weighted annotation rankings. Responses in \param{tar\-get\-ed} settings tend to be shorter and have lower standardized type-to-token ratios, as shown in Tables~\ref{tab:response-length} and~\ref{tab:sttr}; as seen with \param{trans\-i\-ti\-vi\-ty}, these measures correlated with higher Spearman scores for \param{tar\-get\-ed} settings.

These results also show that SBERT generally outperformed my system at approximating the desired holistic response rankings. There were exceptions---as seen in the table, my system achieved higher maximum Spearman scores with both size models. Upon examining these cases, I found no discernible pattern that could predict when the system score will exceed the SBERT score, but it is possible a pattern would emerge with more data.

Comparing the results for the 14-response NS models and 50-response NS models here reveals that performance was consistently better with the 50-response models, both for my system and for SBERT. For my system scores, if we compare the differences for \param{tar\-get\-ed} settings versus the differences for \param{un\-tar\-get\-ed} settings across these model sizes, Table~\ref{tab:targeting-results} shows that \param{tar\-get\-ed} settings benefited more from the increased model size than did \param{un\-tar\-get\-ed} settings. In fact, these mean and median differences for \param{tar\-get\-ed} settings were more than double the mean and median differences for \param{un\-tar\-get\-ed} settings.
% although the ``signal'' here is a slight one; it may not be reliable or statistically significant, but given a larger dataset, future research should probe this finding.
With more NS responses, one could experiment with larger NS models to find the sizes that lead to optimal system performance for \param{tar\-get\-ed} settings and \param{un\-tar\-get\-ed} settings, which could inform design choices for applications relying on a system like mine.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
& \multicolumn{4}{c|}{NS model sample size = 14} \\
\hline
 & \multicolumn{2}{c||}{\param{tar\-get\-ed}} & \multicolumn{2}{c|}{\param{un\-tar\-get\-ed}} \\
\hline
	& System 		& SBERT 		& System 	& SBERT \\
\hline
count 	& 180 		& 60 		& 180 		& 60 \\
\hline
mean 	& 0.380 	& \textit{\textbf{0.530}} 	& 0.300 	& \textbf{0.444} \\
\hline
median 	& 0.369 	& \textit{\textbf{0.545}} 	& 0.314 	& \textbf{0.472} \\
\hline
min & -0.147 	& \textbf{-0.138} & -0.181 	& \textit{\textbf{-0.107}} \\
\hline
max 	& 0.840 	& \textbf{0.879} 	& \textit{\textbf{0.900}}	& 0.881 \\
\hline
std dev 	& 0.241 	& 0.192 	& 0.204 	& 0.191 \\
\hline
\multicolumn{5}{c}{} \\
\hline
& \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
& \multicolumn{2}{c||}{\param{tar\-get\-ed}} & \multicolumn{2}{c|}{\param{un\-tar\-get\-ed}} \\
\hline
		& System 		& SBERT 						& System 		& SBERT \\
\hline
count	& 180 			& 60 						& 180 			& 60 \\
\hline
mean 	& 0.393 		& \textit{\textbf{0.550}} 	& 0.305 		& \textbf{0.469}  \\
\hline
median 	& 0.389 		& \textit{\textbf{0.564}} 	& 0.323			& \textbf{0.496} \\
\hline
min 	& \textbf{-0.048} & -0.090					& -0.185 		& \textit{\textbf{0.132}} \\
\hline
max 	& 0.872 		& \textbf{0.881}			& \textit{\textbf{0.898}} 	& 0.880 \\
\hline
std dev 				& 0.234 		& 0.173		& 0.208 		& 0.172 \\
\hline
\end{tabular}
\caption{\label{tab:targeting-results} Comparing Spearman rank correlation scores for \param{tar\-get\-ed} and \param{un\-tar\-get\-ed} versions of the PDT data, using NS models of either 14 or 50 random responses per item. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{SBERT} column represents 60 rankings (2 system configurations $\times$ 30 items; SBERT operates on plain text, so the \param{term rep\-re\-sent\-a\-tion} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}


%
%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.6em}
%\begin{tabular}{|l|l||l|l||l|l|}
%\hline
% && \multicolumn{2}{c||}{NS model = 14} & \multicolumn{2}{c|}{NS model = 50} \\
%\hline
%	&	& Targeted 	& Untarget 	& Targeted 	& Untargeted  	\\
%\hline
%\multirow{3}{*}{\begin{sideways}words / response \end{sideways}} & responses & 840 & 840 & 3000 & 3000  \\
%\cline{2-6}
%& mean 		& 5.40 	& 6.85		& 5.49 	& 6.79	 \\
%\cline{2-6}
%& median 	& 5.0 	& 6.0		& 5.0 	& 6.0	\\
%\cline{2-6}
%& min 		& 1.0 	& 1.0		& 1.0 	& 1.0	 \\
%\cline{2-6}
%& max 		& 18.0 	& 26.0		& 26.0 	& 26.0	 \\
%\cline{2-6}
%& std dev 	& 2.63 	& 3.16		& 2.84 	& 3.07	 \\
%\hline
%\hline
%\multirow{3}{*}{\begin{sideways} model TTR \end{sideways}} & models	& 60  & 60 & 60 & 60  \\
%\cline{2-6}
%& mean 		& 0.316 	& 0.367		& 0.197 	& 0.205	 \\
%\cline{2-6}
%& median 	& 0.312 	& 0.371		& 0.196 	& 0.205	\\
%\cline{2-6}
%& min 		& 0.166 	& 0.180		& 0.084 	& 0.116	 \\
%\cline{2-6}
%& max 		& 0.543 	& 0.542		& 0.302 	& 0.292	 \\
%\cline{2-6}
%& std dev 	& 0.081 	& 0.085		& 0.049 	& 0.045	 \\
%\hline
%\end{tabular}
%\caption{\label{tab:targeting-model-stats}Comparing the number of words per response and the model type-to-token ratio (TTR) for \param{tar\-get\-ed} and \param{un\-tar\-get\-ed} PDT items, using NS models of either 14 or 50 random responses per item.
%}
%\end{center}
%\end{table}
%


\subsection{\param{Fam\-il\-iar\-i\-ty} Experiments}
\label{sec:exp-familiarity}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
 \hline
 & \multicolumn{2}{c||}{\param{fam\-il\-iar} NS} & \multicolumn{2}{c|}{\param{Crowd} NS} \\
\hline
		& System 			& SBERT 						& System 			& SBERT 				\\
\hline
\hline
count 	& 180 				& 60 						& 180 				& 60 				\\
\hline
mean 	& 0.338 		& \textit{\textbf{0.499}} 		& 0.339 			& \textbf{0.481} 	\\
\hline
median 	& 0.329 		& \textit{\textbf{0.513}} 		& 0.326 			& \textbf{0.500}   \\
\hline
min & -0.239 			& \textit{\textbf{-0.026}} 		& -0.181 			& \textbf{-0.125}  \\
\hline
max & \textit{\textbf{0.896}} & 0.880 					& 0.875 			& \textbf{0.879} 	\\
\hline
std dev & 0.217 			& 0.173 					& 0.224 			& 0.185 			\\
\hline
\end{tabular}
\caption{\label{tab:familiarity-results} Comparing Spearman rank correlation scores where \param{fam\-il\-iar} NS models contain only responses from participants \textit{fam\-il\-iar} to the researcher and \param{Crowd} NS models contain only responses from \param{crowd\-sourced} participants. Results are shown using NS models of 14 responses; note the models used here are \param{mix\-ed} (containing first and second responses; see Section~\ref{sec:exp-primacy}) due to the sparsity of \param{fam\-il\-iar} data. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{SBERT} column represents 60 rankings (2 system configurations $\times$ 30 items; SBERT operates on plain text, so the \param{term rep\-re\-sent\-a\-tion} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}


I used the limited amount of \param{fam\-il\-iar} NS data available to compare system performance when using \param{fam\-il\-iar} versus \param{crowd\-sourced} models. The \param{fam\-il\-iar} and \param{crowd\-sourced} models discussed here were all \param{mix\-ed} (first and second responses), because too few \param{fam\-il\-iar} responses were available for adequate \param{prim\-a\-ry} response models. The results, presented in Table~\ref{tab:familiarity-results}, show no practical difference between \param{fam\-il\-iar} NS models and \param{crowd\-sourced} NS models when it comes to system performance. The mean Spearman score was slightly higher for the \param{crowd\-sourced} NS model, but the median Spearman score was slightly higher for the \param{fam\-il\-iar} NS model. Future work should collect more \param{fam\-il\-iar} responses in order to explore whether this pattern holds for larger model sizes or not. If there are differences, it may be possible to pinpoint the optimal model size depending on whether the available NS responses come from \param{fam\-il\-iar} or \param{crowd\-sourced} NSs.

SBERT performance differed here in that it is clearly better for the \param{fam\-il\-iar} models than the \param{crowd\-sourced} models. Standardized type-to-token ratio may be informative here. For the NNS test sets, the mean STTR was 0.505, which was closer to that of the \param{fam\-il\-iar} models (0.576) than that of the \param{crowd\-sourced} models (0.652; Table~\ref{tab:sttr}). This suggests that SBERT is more sensitive than my system to differences in complexity between the NS model and NNS test sample.



%
%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|l||l|l||l|l|}
%\hline
% & \multicolumn{4}{c|}{NS model sample size = 14} \\
% \hline
% & \multicolumn{2}{c||}{words / response} & \multicolumn{2}{c|}{model TTR} \\
%\hline
%	& \param{Fam} 	& \param{Crowd} 	& \param{Fam} 			& \param{Crowd} 		\\
%\hline
%\hline
%count 	& 840 			& 840 				& 60 				& 60 		\\
%\hline
%mean 	& 6.76 			& 6.50 				& 0.180 			& 0.382 		\\
%\hline
%median 	& 6.0 			& 6.0 				& 0.225 			& 0.383   	\\
%\hline
%min 	& 2.0 			& 1.0 				& 0.134 			& 0.252  		\\
%\hline
%max 	& 31.0 			& 26.0 				& 0.292 			& 0.543 		\\
%\hline
%std dev & 2.78 			& 3.24 				& 0.042 			& 0.070 		\\
%\hline
%\end{tabular}
%\caption{\label{tab:familiarity-model-stats}Comparing the number of words per response and the model type-to-token ratio (TTR) for \param{fam\-il\-iar} and \param{crowd\-sourced} responses, using 14 responses per PDT item. \textit{Count} shows the total number of responses (840) and the total number of models (60) per participant group.
%}
%\end{center}
%\end{table}
%


\subsection{\param{Prim\-a\-cy} Experiments}
\label{sec:exp-primacy}
I also compared how well my system approximated the weighted annotation ranking when using models comprised of NSs' first responses (\param{prim\-a\-ry} models) versus models comprised of a mix of NSs' first and second responses (\param{mix\-ed} models). I collected these second responses in order to broaden the range of NS responses, which were observed in Chapter~\ref{chap:pilot} to be highly convergent for some items (e.g., \textit{raking}). I expected noticeably different performance from the \param{prim\-a\-ry} and \param{mix\-ed} settings, but the results, presented in Table~\ref{tab:primacy-results}, show that the parameter had little impact. For the 50-response models, the best setting was unclear: the mean Spearman score was slightly higher for the \param{prim\-a\-ry} setting, but the median Spearman score was slightly higher for the \param{mix\-ed} setting. For the 14-response models, the mean and median Spearman scores were both slightly higher for the \param{mix\-ed} setting. These differences are likely too small to claim statistical significance, but it is reasonable that at smaller sample sizes, performance is improved by including secondary responses, as in the \param{mix\-ed} models. These secondary responses elicit greater variety, as reflected in standardized type-to-token ratios (Table~\ref{tab:sttr}).


\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
 \hline
 & \multicolumn{2}{c||}{\param{prim\-a\-ry}} & \multicolumn{2}{c|}{\param{mix\-ed}} \\
\hline
			& System 		& SBERT 		& System 	& SBERT \\
\hline
\hline
count 		& 180 			& 60 		& 180 		& 60 \\
\hline
mean 		& 0.339 & \textit{\textbf{0.493}} 	& 0.340 	& \textbf{0.481} \\
\hline
median 	& 0.326 & \textit{\textbf{0.517}} & 0.334 	& \textbf{0.500} \\
\hline
min 	& -0.181 & \textbf{-0.138} 		& -0.158 	& \textit{\textbf{-0.125}} \\
\hline
max & \textbf{0.875} & 0.881 & \textit{\textbf{0.900}} & 0.879 \\
\hline
std dev & 0.224 			& 0.207 	& 0.230 	& 0.185 \\
\hline
\multicolumn{5}{c}{} \\
\hline
& \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
& \multicolumn{2}{c||}{\param{prim\-a\-ry}} & \multicolumn{2}{c|}{\param{mix\-ed}} \\
\hline
 			& System 		& SBERT 				& System 			& SBERT \\
\hline
count 		& 180 			& 60 				& 180 				& 60 \\
\hline
mean 		& 0.354 	& \textit{\textbf{0.514}} 	& 0.344 	& \textbf{0.505} \\
\hline
median  	& 0.345 	& \textit{\textbf{0.532}} 	& 0.350 	& \textbf{0.518}  \\
\hline
min  		& -0.185 	& \textbf{-0.090} 		& -0.147 		& \textit{\textbf{0.049}}  \\
\hline
max 		& \textit{\textbf{0.898}} & 0.880 	& \textbf{0.894} 	& 0.881 \\
\hline
std dev 	& 0.226 	& 0.186 					& 0.226 		& 0.168 \\
\hline
\end{tabular}
\caption{\label{tab:primacy-results} Comparing Spearman rank correlation scores where \param{prim\-a\-ry} models contain only first responses from NSs and \param{mix\-ed} models contain an equal mix of first and second responses from NSs. Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{SBERT} column represents 60 rankings (2 system configurations $\times$ 30 items; SBERT operates on plain text, so the \param{term rep\-re\-sent\-a\-tion} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}


Examining this parameter across the two NS model sizes, it is clear that the 50-response model always resulted in better system performance than the 14-response model, regardless of the \param{prim\-a\-ry} or \param{mix\-ed} setting. The mean and median Spearman scores show that this increase in performance was greater for the \param{prim\-a\-ry} setting than for the \param{mix\-ed} setting. This is unsurprising; if increasing variety (to some extent) in the NS model can improve performance, the more constrained \param{prim\-a\-ry} setting should benefit more from an increased sample size than should the (already relatively varied) \param{mix\-ed} setting. More NS responses would be needed to fully explore this curve and establish statistical significance. The \param{mix\-ed} models here all contained half primary responses and half secondary responses. This default 50-50 mix is unlikely to be optimal, and with enough data, it would even be possible to determine exactly how many primary and secondary responses to include when sampling NS models for new PDT items in order to optimize system performance.

As seen in the other holistic experiments, SBERT performance here generally exceeded system performance; given access to the same NS response models, SBERT's similarity measures resulted in rankings that better approximated the weighted annotation rankings. My system did achieve higher maximum Spearman scores, but with no clear pattern to predict such cases, these higher maximums are not useful.

%
%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.6em}
%\begin{tabular}{|l|l||l|l||l|l|}
%\hline
% && \multicolumn{2}{c||}{NS model = 14} & \multicolumn{2}{c|}{NS model = 50} \\
%\hline
%	&	& Primary 	& Mixed 	& Primary 	& Mixed  	\\
%\hline
%\multirow{3}{*}{\begin{sideways}words / response~ \end{sideways}} & responses & 840 & 840 & 3000 & 3000  \\
%\cline{2-6}
%& mean 		& 5.76 	& 6.5		& 5.83 	& 6.46	 \\
%\cline{2-6}
%& median 	& 5.0 	& 6.0		& 6.0 	& 6.0	\\
%\cline{2-6}
%& min 		& 1.0 	& 1.0		& 1.0 	& 1.0	 \\
%\cline{2-6}
%& max 		& 20.0 	& 26.0		& 26.0 	& 26.0	 \\
%\cline{2-6}
%& std dev 	& 2.67 	& 3.24		& 2.74 	& 3.26	 \\
%\hline
%\hline
%\multirow{3}{*}{\begin{sideways} model TTR~ \end{sideways}} & models	& 60  & 60 & 60 & 60  \\
%\cline{2-6}
%& mean 		& 0.301 	& 0.382		& 0.180 	& 0.221	 \\
%\cline{2-6}
%& median 	& 0.297 	& 0.383		& 0.181 	& 0.225	\\
%\cline{2-6}
%& min 		& 0.166 	& 0.252		& 0.084 	& 0.134	 \\
%\cline{2-6}
%& max 		& 0.534 	& 0.543		& 0.292 	& 0.302	 \\
%\cline{2-6}
%& std dev 	& 0.082 	& 0.705		& 0.042 	& 0.043	 \\
%\hline
%\end{tabular}
%\caption{\label{tab:primacy-model-stats}Comparing the number of words per response and the model type-to-token ratio (TTR) for \param{prim\-a\-ry} and \param{mix\-ed} models, using NS models of either 14 or 50 random responses per item.
%}
%\end{center}
%\end{table}
%


\subsection{\param{Term Rep\-re\-sent\-a\-tion} Experiments}
\label{sec:exp-term-reps}
The current system allows for different \param{term rep\-re\-sent\-a\-tions}, which are variations on syntactic dependencies. A dependency consists of a \textit{head}, \textit{dependent}, and \textit{label}. In past work, I experimented with omitting one or more of these elements to allow for less restrictive matching (see Table~\ref{tab:dist-ranked-parameters}). In the current work, I compared the system performance using three formats: \textit{label-dependent-head} (\param{ldh}), \textit{dependent-head} only (\param{xdh}), and \textit{dependent} only (\param{xdx}). In other words, my system uses a ``bag of terms'' approach, where the bags contain either labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}) or words (\param{xdx}). The labeled and unlabeled dependencies were the top performers in my previous work, and the \param{xdx} format was included as a kind of baseline approximating a bag of words approach, although, as seen throughout this chapter, \param{xdx} models were among the top performers in many cases.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l|l||l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
\hline
			& \param{ldh} 	& \param{xdh} 			& \param{xdx} 		& SBERT \\
\hline
\hline
count 			& 120 		& 120 					& 120 				& 120 \\
\hline
mean 			& 0.333 	& 0.336 			& \textbf{0.351} 	& \textit{\textbf{0.487}} \\
\hline
median 			& 0.318 	& \textbf{0.344} 		& 0.330 		& \textit{\textbf{0.507}} \\
\hline
min & \textit{\textbf{-0.108}} 	& -0.181 				& -0.158 	& 	-0.138 \\
\hline
max 			& 0.871 	& 0.875 & \textit{\textbf{0.900}} 			& 0.881 \\
\hline
std dev 		& 0.223 	& 0.227 				& 0.231 			& 0.196 \\
\hline
\multicolumn{5}{c}{} \\
\hline
 & \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
& \param{ldh} & \param{xdh} & \param{xdx} & SBERT \\
\hline
\hline
count 	& 120 			& 120 				& 120 					& 120 \\
\hline
mean & \textbf{0.350} 	& 0.349 			& 0.348 			& \textit{\textbf{0.509}} \\
\hline
median 	& 0.364 		& \textbf{0.374} 	& 0.331 			& \textit{\textbf{0.523}} \\
\hline
min 	& -0.147 		& -0.185 			& \textit{\textbf{-0.062}} & -0.090 \\
\hline
max 	& 0.892 		& 0.893 			& \textit{\textbf{0.898}} 		& 0.881 \\
\hline
std dev & 0.229 		& 0.236 			& 0.213 				& 0.177 \\
\hline
\end{tabular}
\caption{\label{tab:termrep-results} Comparing Spearman rank correlation scores where system configurations use different \param{term rep\-re\-sent\-a\-tions}: \param{ldh} (labeled dependencies), \param{xdh} (unlabeled dependencies), or \param{xdx} (dependents only; i.e., \textit{words}). Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} and \textit{SBERT} column represents 120 different rankings (4 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. 
%%% 4/16/21 LK OK
}
\end{center}
\end{table}

The results in Table~\ref{tab:termrep-results} show that in nearly all cases, regardless of \param{term rep\-re\-sent\-a\-tion}, performance was best with the 50-response NS models. For the \param{xdx} setting, the mean (but not median) was slightly higher for the smaller model, and this was the only exception. This difference between model sizes was greatest for the \param{ldh} models and smallest for the \param{xdx} models. This makes sense, given that the \param{ldh} representations are the most complex and \param{xdx} representations are least complex, because we can expect the simpler \param{xdx} models to become ``saturated'' with relevant NNS response terms at smaller sample sizes as compared to \param{ldh} models. In Table~\ref{tab:sttr}, this is reflected in standardized type-to-token ratios. For the \param{crowd\-sourced} models, the greatest difference in STTR is seen when moving from the \param{ldh} 14-response models to 50-response models. For the \param{xdh} representation, the two sample sizes have nearly identical STTRs, and for the \param{xdx} representation, the 14-response model has the highest STTR. This suggests the optimal NS model size is correlated with the complexity. Considering Table~\ref{tab:sttr}, it appears that STTRs for the simpler \param{xdh} and \param{xdx}  representations likely peak somewhere between the 14-response sample and 50-response sample, but \param{ldh} samples may see increased STTR for samples larger than 50 responses.

With more NS responses, future work could form larger samples and determine the size at which STTR reaches its maximum for \param{ldh} representations. The same could be done for \param{xdh} and \param{xdx} \param{term rep\-re\-sent\-a\-tions} as well. With such a ``map'' of the relationship between STTR and sample size for NS models, one could better explore the correlations between system performance and \param{term rep\-re\-sent\-a\-tions} across sample sizes. For processing new PDT items, this would make it possible to choose the optimal \param{term rep\-re\-sent\-a\-tion} based on the number of available NS responses.



\section{Optimization Conclusion}
\label{sec:optimization-conclusion}
The experiments in this chapter uncovered a number of trends that would be informative for optimizing my system's performance on future items. I found that the smaller of the two NS model sizes worked best for ranking NNS responses in a way that corresponds with individual feature annotations, as measured with mean average precision throughout Section~\ref{sec:exp-annotations}. Moreover, complexity was key to many of the trends I observed. For example, for the features most related to content or semantics (\feat{core e\-vent}, \feat{in\-ter\-pret\-a\-bil\-ity}, and \feat{ver\-i\-fi\-a\-bil\-ity}), labeled dependency representations, which result in the highest complexity (as observed through STTR in Table~\ref{tab:sttr}) performed best, while the lowest complexity setting (the dependent-only representation) worked best for \feat{an\-swer\-hood} and \feat{gram\-mat\-i\-cal\-ity}.
%These experiments also revealed that my system generally outperforms SBERT at this task.

In the experiments discussed in Section~\ref{sec:exp-holistic}, I found the larger of the two NS model sizes generally worked best for ranking NNS responses in a way that correlates with the weighted annotation score, a holistic measure of response quality. For \param{in\-trans\-i\-tives} and for \param{xdx} terms (dependents only), however, the smaller NS model size was found to work best. This is notable because these are the settings that result in the lowest standardized type-to-token ratios (STTRs), as seen in Table~\ref{tab:sttr}. This indicates that response complexity and optimal NS model size are correlated; for more complex items, larger models are needed. Among \param{tar\-get\-ing} and \param{prim\-a\-cy} settings, all settings had stronger Spearman scores with the larger models. This preference for larger models was greater for the settings with higher STTRs---\param{un\-tar\-get\-ed} and \param{mix\-ed}---than for their less complex counterparts, \param{tar\-get\-ed} and \param{prim\-a\-ry}, further highlighting the correlation between item complexity and NS model size. Because the 50-response NS models worked best for most parameter settings here, future research should involve collecting more NS responses to see if even larger models can improve performance. This would also allow for a more thorough exploration of the impact of response variability across sample sizes.
%These results also showed that SBERT outperforms my system at holistic ranking of NNS responses.

Through comparisons with SBERT, these experiments revealed that my system can outperform a state-of-the-art language embedding tool at ranking responses in a way that captures the five annotation features, as measured with mean average precision throughout Section~\ref{sec:exp-annotations}. 
These five annotation features are not strictly defined in strict linguistic terms, but in practice they do correlate with surface forms. It appears that for predicting these features within a heavily constrained subset of the language, it may be preferable to use a constrained model. My system, which is constrained to the set of dependencies seen in the model and test response when determining similarity, brings with it limited linguistic knowledge. SBERT, however, is trained on web-scale data for far richer modeling of lexical and linguistic knowledge. This additional knowledge may dilute the correlations between the features used here and the constrained language seen in the model and test responses. In other words, SBERT may ``overthink'' this task.

For approximating the ideal rankings, however, SBERT consistently outperformed my system, as measured with the Spearman rank correlations with the weighted annotation scores throughout Section~\ref{sec:exp-holistic}. These findings would be useful to anyone implementing a system for processing NNS sentences in visual contexts. For relying on similarity with a NS model as a representation of overall NNS response quality, a powerful tool like SBERT better correlates with the desired rankings than a custom, dependency-based system, even after considerable attempts at optimizing settings. This is to be expected, because the task in this scenario is predicting holistic quality  purely through semantic textual similarity, so SBERT's much richer lexical knowledge is beneficial here. However, for focusing on specific, custom features of a response, as may be desirable in language learning contexts, a custom system can outperform such a tool. Moreover, because my system scores responses using syntactic dependencies rather than uninterpretable word embeddings, it is better suited for providing feedback. Taken together, these findings suggest that researchers and developers should consider these approaches to be complementary, using them alone or jointly to suit the task at hand. 


%
%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.6em}
%\begin{tabular}{|l|l||l|l|l||l|l|l|}
%\hline
% && \multicolumn{3}{c||}{NS model = 14} & \multicolumn{3}{c|}{NS model = 50} \\
%\hline
%	&	& \param{ldh} 	& \param{xdh} 	& \param{xdx} 	& \param{ldh} 	& \param{xdh} 	& \param{xdx} 		\\
%\hline
%\multirow{3}{*}{\begin{sideways}model TTR \end{sideways}} & model	& 120 & 120 & 120 	& 120 & 120 & 120	 \\
%\cline{2-8}
%& mean 		& 0.525 	& 0.513		& 0.279 			& 0.367		& 0.354 	& 0.157	 \\
%\cline{2-8}
%& median 	& 0.538 	& 0.525		& 0.286 			& 0.366		& 0.358 	& 0.157	 \\
%\cline{2-8}
%& min 		& 0.177 	& 0.177		& 0.136 			& 0.150		& 0.141 	& 0.063	 \\
%\cline{2-8}
%& max 		& 0.825 	& 0.818		& 0.437 			& 0.570		& 0.562 	& 0.228	 \\
%\cline{2-8}
%& std dev 	& 0.145 	& 0.144		& 0.068 			& 0.099		& 0.096 	& 0.037	 \\
%\hline
%\end{tabular}
%\caption{\label{tab:termrep-model-stats}Comparing the model-level dependency type-to-token ratios (TTRs) for all NS models when dependencies are formatted in the three different \param{term rep\-re\-sent\-a\-tions}: \param{ldh} (labeled dependencies) and \param{xdh} (unlabeled dependencies) and \param{xdx} (dependents only), using NS models of either 14 or 50 random responses per item.
%}
%\end{center}
%\end{table}
%








%\section{Combined settings experiments}
%\label{sec:exp-combos}
%\subsection{Combined settings results}
%\label{sec:combos-results}








%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.6em}
%\begin{tabular}{|l|l||l|l|l||l|l|l|}
%\hline
% && \multicolumn{3}{c||}{NS model sample size = 14} & \multicolumn{3}{c|}{NS model sample size = 50} \\
%\hline
%	&	& Intrans 	& Trans 	& Ditrans 	& Intrans 	& Trans 	& Ditrans 		\\
%\hline
%\multirow{3}{*}{\begin{sideways}words / response \end{sideways}} & responses	& ct  & ct & ct & ct & ct & ct	 \\
%\cline{2-8}
%& mean 	& mean 	& mean		& mean 	& mean		& mean 	& mean	 \\
%\cline{2-8}
%& median 	& md 	& md		& md 	& md		& md 	& md	 \\
%\cline{2-8}
%& min 	& min 	& min		& min 	& min		& min 	& min	 \\
%\cline{2-8}
%& max 	& max 	& max		& max 	& max		& max 	& max	 \\
%\cline{2-8}
%& std dev & sd 	& sd		& sd 	& sd		& sd 	& sd	 \\
%\hline
%\hline
%\multirow{3}{*}{\begin{sideways}model TTR \end{sideways}} & model	& ct & ct & ct 	& ct & ct & ct	 \\
%\cline{2-8}
%& mean 	& mean 	& mean		& mean 	& mean		& mean 	& mean	 \\
%\cline{2-8}
%& median 	& med 	& med		& med 	& med		& med 	& med	 \\
%\cline{2-8}
%& min 	& min 	& min		& min 	& min		& min 	& min	 \\
%\cline{2-8}
%& max 	& max 	& max		& max 	& max		& max 	& max	 \\
%\cline{2-8}
%& std dev & sd 	& sd		& sd 	& sd		& sd 	& sd	 \\
%\hline
%\end{tabular}
%\caption{\label{tab:transitivity-model-stats} .
%}
%\end{center}
%\end{table}
