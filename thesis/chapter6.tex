%\chapter{Method}
%\label{chap:method}
%\section{Introduction}
\chapter{Optimization}
\label{chap:optimization}
In this chapter, I discuss experiments intended to optimize my system for rating and ranking responses automatically, first introduced in Chapter~\ref{chap:pilot}. In practical terms, this means looking for correlations between the performance of my system (and its particular settings) and known features of the NS responses, such as the transitivity of the PDT item event, the size of the NS model, and whether models contain only primary responses or a mix of primary and secondary responses. I also consider correlations between system performance and observable measures of the NS data, namely type-to-token ratios and mean response lengths. Where sufficient data allows, I also look for patterns indicating correlations between pairs of parameter settings used in a given system configuration.

My earliest attempts at ranking responses were rule-based and relied on strict matching with a pre-established set of acceptable responses, described in Section~\ref{sec:pilot-study}. This found moderate success, leading to the improved approach described in Section~\ref{sec:2016work}, which is data-driven and relies on more flexible methods of comparison.
In Section~\ref{sec:current-method}, I give an overview of the updates to this approach used here to process the new, larger dataset described in Chapter~\ref{chap:data}. Ideally, for each of the five annotation features, my system rankings should maximize the separation of positively and negatively annotated NNS responses. In Section~\ref{sec:exp-annotations}, I use mean average precision to investigate which system settings and model sizes do this best for different types of items. In Section~\ref{sec:exp-holistic}, I use Spearman rank correlations between system rankings and the weighted annotation rankings to see how various system settings effect the ability to approximate holistic response rankings derived from human judgements.


\section{Updated method}
\label{sec:current-method}
The work discussed in Chapter~\ref{chap:pilot} relied on a shaky implementation of ``correctness'' or ``appropriateness'' for responses, and this needed improvement, first and foremost. Developing a better and more reliable annotation scheme was a key goal for me in expanding the work to the current dissertation, in order to give the work more meaning and context and make my corpus useful for a broad range of uses. The annotation planning discussed in Section~\ref{sec:scheme} was a direct result of the challenges of working with an inadequate annotation scheme. 

As discussed in Sections~\ref{sec:est-feat-weights} and~\ref{sec:holistic-scoring}, in the current corpus, annotations no longer encode for errors, but instead give a binary score for five different features, which are then weighted and combined to produce a score between zero and one. This expands the range of metrics available for judging system performance. Mean average precision (MAP) can now be used to judge system performance focused on individual annotation features. I can now also compare system produced rankings against the weighted annotation rankings for a holistic approach to response scores; i.e., how well does the system rank responses in comparison to an ideal ranking based on manual annotations? I use Spearman rank correlation for these scores.\lk{expand on Spearman; what does it mean? Sp vs Pearson, etc}

Because the annotations and evaluation are much different in the current work, it does not exactly follow that findings from the previous work will hold true. However, I believe that the previous work has highlighted some of the system settings that are most likely to perform well, and I chose to focus my experiments on some of the best performing settings. For example, all of the current experiments rely on the tf-idf cosine (\param{TC}) approach, as this generally outperformed the others. As discussed in Section~\ref{sec:metrics}, the \param{TC} performance suffers for items where the non-native speaker (NNS) data is noisiest (with regard to spellings) and the native speaker (NS) data is relatively homogenous (particularly with regard to verb choice). Rather than continue experimenting with the underperforming approaches, I chose to address these issues directly instead. As discussed in Chapter~\ref{chap:data}, to address the spelling noise, I made sure that data collection participants had access to spelling correction while typing their responses. To address the uniformity of the NS responses, I surveyed a much larger group of participants and instructed each of them to provide two responses per picture description task (PDT) item.

The current work retains just three of the five term representations previously used (see Section~\ref{sec:response-rep}): \param{ldh}, \param{xdh}, and \param{xdx}. The \param{ldh} and \param{xdh} forms performed best with the older dataset. Moreover, as these represent labeled and unlabeled dependencies, their use in linguistics is well established. The \param{xdx} representation is kept here as a rough equivalent of a bag-of-words model, which is useful as a baseline for comparing the other two.

Finally, as \param{Brown} overwhelmingly outperformed the \param{WSJ} as a tf-idf reference corpus, the current experiments rely exclusively on the former. The larger PDT dataset follows a similar narrative style to that described in Section~\ref{sec:pilot-data}, so I am confident \param{Brown} is again the best option here.

All experiments throughout this chapter score and rank 70 NNS responses per item; this is the maximum number of NNS responses available across all PDT items. Where more than 70 responses are available, a random sample of 70 is used. The large increase in the number of PDT items and the size of the datasets means that running an exhaustive search for the best parameters among all combinations is not feasible. The experiments in Chapter~\ref{chap:pilot} used 30 different combinations of system settings (i.e., \textbf{configurations}) for each of 10 items. By comparison, the variables and parameters used in this chapter result in up to 48 different configurations (see Table~\ref{tab:all-params}) for each of the 30 items. To make sense of this large number of results, I have chosen to focus my optimization efforts on each parameter individually.


\begin{table*}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Transitivity & Targeting & Familiarity & Primacy & Term Norm. & Term Rep. \\
\hline
\hline
Intransitive & Targeted & Familar & Primary & Normalized & \param{ldh} \\
\hline
Transitive & Untargeted & Crowdsourced & Mixed & Non-norm. & \param{xdh} \\
\hline
Ditransitive & & & & & \param{xdx} \\
\hline
\end{tabular}
\caption{All parameters or variables and their settings; a system configuration combines one setting from each column.}
\label{tab:all-params}
\end{center}
\end{table*}

%\chapter{Optimization}
%\label{chap:optimization}
%In this chapter, I detail my research applying the methods discussed in Chapter~\ref{chap:pilot} to the much larger and more richly annotated dataset described in Chapters~\ref{chap:data} and \ref{chap:annotation}. The chapter primarily consists of a series of experiments focused on isolating and optimizing a number of parameters or variables in my picture description task (PDT) response analysis pipeline. 

To help contextualize the results of these experiments, I used a state-of-the-art language modeling tool to rank responses according to their similarity to the NS model. I discuss this tool and its use as a baseline in Section~\ref{sec:bert-baseline}.

The optimization experiments are organized here according to the sequence in which the variables are relevant in my process, which begins with data collection and ends with scoring and ranking non-native speaker (NNS) responses. Thus, in Section~\ref{sec:exp-transitivity}, I begin with the variable I refer to as \textit{transitivity}, which emerged during task design for the PDT described in Chapter~\ref{chap:data}; I look at the effects of applying my dependency-based tf-idf cosine pipeline to new item types, namely intransitives and ditransitives, and compare against performance on transitive items. Next, in Section~\ref{sec:exp-targeting}, I turn to experiments regarding \textit{targeting}, which refers to whether or not the PDT item subject was referenced in the prompt (as discussed in Section~\ref{sec:pdt}). In Section~\ref{sec:exp-familiarity}, I examine a variable I call \textit{familiarity}, which refers to whether the native speakers (NSs) contributing to the model are \textit{familiar} to me personally or are \textit{crowdsourced}. Another new variable follows in Section~\ref{sec:exp-primacy}; I call this \textit{primacy}, which refers to whether the NS model contains only first (\param{primary}) responses, or an equal number of first and second (\param{mixed}) responses (also discussed in Section~\ref{sec:pdt}). I then evaluate the effects of a new innovation---normalizing the weight of each NS term in the model according to the length of the response in which it appeared, in Section~\ref{sec:exp-term-norm}; I call this variable \textit{term normalization}.\lk{Is this technically norming the terms or the responses?} For the final variable experiments, I return in Section~\ref{sec:exp-term-reps} to the best performing dependency \textit{term representations} from Section~\ref{sec:response-rep} to see how they perform with the current dataset.

After examining these variables individually, in Section~\ref{sec:exp-combos} I consider the hypothesis that particular configurations \lk{Check research Qs and sync this up} will perform better than others in particular conditions. This is a ``non-exhaustive,'' mostly future-looking set of experiments, where I report some promising trends.

%Finally, in Section~\ref{sec:exp-bert}, I present a set of experiments comparing the performance of my best system settings against BERT, a more sophisticated, state of the art language model capable of ranking NS responses according to their similarity to the collection of NNS responses. These results provide insights into the trade-offs between using my simpler, highly transparent tool and a more powerful yet highly opaque tool.

\section{Sampling NS response models}
\label{sec:sampling}

Throughout this chapter, the experiments are performed with randomly sampled NS models of two sizes in order to examine the effects of model size on system performance. The larger models contain 50 NS responses per PDT item. This is the maximum number of NS responses that are available across all PDT items using relevant system configurations. For example, item 26 in the corpus contains 290 total NS responses, but only roughly 90\% are \param{crowdsourced}, 50\% are \param{targeted}, and 50\% are first (\param{primary}) responses, and selecting for this configuration leaves exactly 50 responses from which to form a model. The smaller models throughout this chapter contain 14 NS responses per item. This is the maximum number of responses available across all items from \param{familiar} NSs, so I chose this size in order to fairly compare \param{crowdsourced} and \param{familiar} responses (Section~\ref{tab:primacy-results}) and I retained it throughout this chapter to best contextualize the familiarity experiments.
%I26: 158 Targeted, 132 Untargeted

\section{Establishing a baseline}
\label{sec:bert-baseline}
The central task of my work---ranking NNS responses using a set of NS responses---is not a standard task with established metrics in any relevant field like natural language processing or language testing. Moreover, this work relies on a custom dataset which has not been widely adopted. These facts make it challenging to assess the performance of my ranking system and its various configurations or to compare this work against similar research. In order to give some frame of reference for this work, I chose to use a BERT, a widely adopted state-of-the-art language modeling tool \cite{BertDevlin2018}. My system scores each single (NNS) test response according to its similarity with the set of (NS) model responses. Measuring sentence similarity is one of BERT's most used functions, so for each scoring and ranking experiment in this chapter, I use BERT to generate corresponding output. The resulting Spearman scores are discussed throughout to help contextualize my system's performance.
\subsection{About BERT}
\label{sec:about-bert}
XYZ: BERT / SBERT

\subsection{Integrating BERT}

For each experiment reported throughout this chapter, my system and BERT are given access to the same NS responses as the basis for their similarity measures. Naturally, however, each is trained on or makes use of very different language resources. The linguistic ``intelligence'' of my system comes largely from the Stanford Parser and its pre-trained grammar model, as discussed in Section~\ref{sec:pilot-study} \cite{klein:manning:03}. The model is trained on the standard training sections of the Penn Treebank, which contain over one million words of English text, manually part-of-speech tagged and parsed, sourced from the Wall Street Journal and the Brown Corpus \cite{marcus-et-al:93}. My approach also uses the Brown Corpus for tf-idf, meaning a word frequency model extracted from the Brown Corpus also serves as a linguistic resource \cite{kucera:francis:67}. BERT, on the other hand, is trained on vastly larger amounts of unannotated text, from a much broader range of sources.

My implementation of BERT here varies slightly from the way I implemented my own system's similarity measuring approach throughout most of this chapter. In my system, each NS response in the model is processed and dumped into a single ``bag of dependencies,'' which is then used to generate a single similarity score (via tf-idf cosine). BERT operates directly on plain text sentences, and because the use of punctuation in the PDT responses is not consistent, concatenating all NS model responses in order to generate a single similarity score is not ideal. Instead, I use BERT to do a pairwise comparison between the NNS test response and each NS response in the model and then average these similarity scores.

I did explore an implementation of my system's similarity scoring that is more consistent with this individual treatment of model responses for BERT, and this is discussed in Section~\ref{sec:exp-term-norm}.


\section{Annotation features experiments}
\label{sec:exp-annotations}
In this section, I revisit the five annotation features discussed in Chapter~\ref{chap:annotation} to see how they correlate with the performance of my system. For a given feature, I use mean average precision (MAP) to see how well the system rankings separate the positively and negatively annotated responses. In other words, when calculating MAP for the \feat{core event} feature, a ``0'' annotation for \feat{core event} is treated as an error. I isolate these MAP scores for each setting within the \param{transitivity}, \param{targeting}, and \param{primacy} variables, as well as in total. I also compare across \param{term representations}. To contextualize these scores, I include MAP scores for the weighted annotation rankings (WAR) and BERT rankings. Note that the superior WAR MAP scores seen here are to be expected, given that the annotation weights reflected in the WAR were tuned according to the judgements of the same annotators who developed the annotation scheme.

These experiments would be useful to anyone considering an approach to content analysis like mine. In an intelligent computer-assisted language learning (ICALL) game, for example, there may be times when \feat{core event} is the main concern, or others where \feat{answerhood} or another feature is most relevant. By isolating each annotation feature and examining how model sizes and term representations effect MAP---both overall and for individual variables, such as \param{intransitives} or \param{targeted} items---I observe a number of trends that would be helpful in designing an ICALL game that selects an optimal system configuration for handling each user response.

I present here two tables for each of the five annotation features. For each feature, this first table presents MAP scores for models comprised of \param{crowdsourced} responses, showing both the 14-response and 50-response models. This table serves to examine whether the task of recognizing the feature is sensitive to differences in model size, and whether parameters like targeting or primacy interact with any such differences. The second table presents MAP scores for the \param{familiar} and \param{crowdsourced} NS models, using models of 14 responses each. This second table serves to compare the effectiveness of familiar and crowdsourced NS response models at recognizing the given annotation feature in an NNS response. Note that the first table covers both \param{primary} and \param{mixed} response models, but the second table covers only \param{mixed} response models (for both \param{familiar} and \param{crowdsourced} responses), because for some items, 14 is the maximum number of familiar responses available (including first and second responses). MAP differences between the crowdsourced 14-response models in each first table and corresponding second table owe to this difference. In all 10 tables, I use \textbf{bold} to indicate the highest term representation MAP \textit{within} each of the two models presented and \textit{\textbf{bold with italics}} to indicate the highest MAP \textit{between} the two models.

Overall, a few trends stand out. First, both as a trend and a caveat, note that the differences in MAP scores observed here are small, and may not always be statistically significant.
%Within all 10 of the tables presented in this section, the maximum difference between any two MAP scores ranges between 0.08 and 0.24.
The observations here are thus not intended to guide high stakes decisions, but as an indication of promising directions for future work, ideally with much more annotated data to confirm them.  Chief among these observations is the underperformance of unlabeled dependencies (xdh). In some cases, labeled dependencies (ldh) achieve the highest MAP, and in some cases dependents only (xdx) work best, but there are no cases in which unlabeled dependencies win. This suggests that future iterations of my system could safely eliminate the use of unlabeled dependencies for the sake of simplicity. The MAP scores also show that with only one exception, BERT underperforms the system. 
%It is worth noting, however, that no attempts were made to optimize BERT for this task, beyond the use of the two different size NS models.
This shows that with regard to recognizing custom annotation features, a custom pipeline based on dependency parsing and tf-idf can outperform newer, more sophisticated machine learning approaches.  Another notable trend seen here is that transitive items are often an exception; where intransitives and ditransitives see higher performance with a given model size or term representation, transitives frequently differ. Yet another observation is that with regard to NS model size, less is overwhelmingly more. For each of the five features, the total MAP (which covers all rankings for all items, provided by all available system configurations) is highest for the smaller model. In some cases, the larger model may be better for a particular item type or parameter setting, but the total MAP is always highest for the smaller model. Finally, \param{crowdsourced} models usually outperform \param{familiar} models with regard to MAP. This may seem counterintuitive, as I expected all familiar participants to complete the PDT most faithfully. However, it may be that crowdsourced responses are more like NNS responses than are familiar responses. For one, crowdsourced participants are less motivated than familiar participants, and this manifests in lazy or bad faith responses; this noise may simply better model NNS responses. Moreover, the familiar participants were all handpicked native English speakers, whereas the crowdsourced participants are anonymous, with no way of confirming that they are in fact native English speakers. It is possible that some ``NS'' responses come from non-native speakers, which could explain why crowdsourced models achieve higher MAP.

\subsection{\feat{Core Event} experiments}
\label{sec:map-core}
\feat{Core event}, as discussed in Section~\ref{sec:scheme}, assesses whether a response captures the main action of the PDT item and requires that the event is linked to a subject (and an object or objects where necessary).

Table~\ref{tab:core-map} presents \param{Core event} MAP scores for the 14-response and 50-response crowdsourced models. These scores show that for assessing \param{core event}, the smaller model, used with labeled dependencies is superior across the board. The 50-response model comes closest for transitives, but never outperforms the 14-response model. We can also observe here that the term representation setting is more relevant in the larger model, with \param{xdx} besting \param{ldh} in the case of ditransitives and untargeted items. NS responses to ditransitives and untargeted items tend to  be the least homogenous; they have higher type-to-token ratios \lk{xyz} than their counterparts. Moving from \param{ldh} to \param{xdx} representations results in a lower type to type-to-token ratio \lk{xyz}, so it is unsurprising that it improves performance for these items.

%% CORE EVENT MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr   & \textit{\textbf{0.859}} & 0.856 & 0.854 & 0.865 & 0.835  & \textbf{0.855} & 0.854 & 0.852 & 0.865 & 0.831 \\ \hline
Tran    & \textit{\textbf{0.737}} & 0.735 & 0.728 & 0.742 & 0.703   & \textbf{0.736} & 0.733 & 0.725 & 0.742 & 0.701 \\ \hline
Ditr    & \textit{\textbf{0.665}} & 0.661 & 0.664 & 0.660 & 0.634  & 0.657 & 0.656 & \textbf{0.661} & 0.660 & 0.629 \\ \hline
\hline
Targ    & \textit{\textbf{0.739}} & 0.738 & 0.732 & 0.735 & 0.708  & \textbf{0.737} & 0.735 & 0.729 & 0.735 & 0.704 \\ \hline
Untg    & \textit{\textbf{0.768}} & 0.763 & 0.765 & 0.777 & 0.740  & 0.762 & 0.759 & \textbf{0.763} & 0.777 & 0.736 \\ \hline
\hline
Prim    & \textit{\textbf{0.754}} & 0.752 & 0.747 & 0.756 & 0.723  & \textbf{0.750} & 0.748 & 0.745 & 0.756 & 0.719 \\ \hline
Mix      & \textit{\textbf{0.753}} & 0.749 & 0.750 & 0.756 & 0.725  & \textbf{0.749} & 0.746 & 0.746 & 0.756 & 0.721 \\ \hline
\hline
Total 	 & \textit{\textbf{0.753}} & 0.751 & 0.748 & 0.756 & 0.724 	& \textbf{0.750} & 0.747 & 0.746 & 0.756 & 0.720 \\ \hline
\end{tabular}
\caption{\label{tab:core-map}Mean Average Precision (MAP) scores for the \feat{Core Event} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, primary NS models), and for the full set (Total).
}
\end{center}
\end{table}


%% CORE EVENT MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Familiar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.859                   & 0.859 & \textit{\textbf{0.865}} & 0.865 & 0.838 & \textbf{0.857}          & 0.852 & 0.848                   & 0.865 & 0.833 \\ \hline
Tran  & \textit{\textbf{0.740}} & 0.737 & 0.726                   & 0.742 & 0.703 & \textbf{0.738}          & 0.735 & 0.728                   & 0.742 & 0.702 \\ \hline
Ditr  & 0.651                   & 0.648 & \textbf{0.660}          & 0.660 & 0.625 & 0.663                   & 0.659 & \textit{\textbf{0.673}} & 0.660 & 0.641 \\ \hline
\hline
Targ  & \textbf{0.733}          & 0.732 & 0.732                   & 0.735 & 0.707 & \textit{\textbf{0.739}} & 0.736 & 0.733                   & 0.735 & 0.709 \\ \hline
Untg  & 0.767                   & 0.764 & \textit{\textbf{0.769}} & 0.777 & 0.737 & \textbf{0.767}          & 0.761 & \textbf{0.767}          & 0.777 & 0.742 \\ \hline
\hline
Total & 0.750                   & 0.748 & \textbf{0.751}          & 0.756 & 0.722 & \textit{\textbf{0.753}} & 0.749 & 0.750                   & 0.756 & 0.725 \\ \hline
\end{tabular}
\caption{\label{tab:core-fam-map}Mean Average Precision (MAP) scores for the \feat{Core Event} annotation feature, comparing \param{familiar} and crowdsourced (\param{Crowd}) responses. MAP is derived from various response rankings: the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), weighted annotation ranking (WAR), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, targeted items), and for the full set (Total). Note that all models represented here are \param{mixed} due to the small number of familiar participants.
}
\end{center}
\end{table}


\subsection{\feat{Answerhood} experiments}
\label{sec:map-answer}

%%ANSWERHOOD MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.868 & 0.871 & \textit{\textbf{0.878}} & 0.881 & 0.869 & 0.866 & 0.868 & \textbf{0.874} & 0.881 & 0.868 \\ \hline
Tran  & 0.816 & 0.819 & \textbf{0.846} & 0.845 & 0.838 & 0.818 & 0.823 & \textit{\textbf{0.851}} & 0.845 & 0.838 \\ \hline
Ditr  & 0.824 & 0.826 & \textit{\textbf{0.841}} & 0.837 & 0.833 & 0.821 & 0.822 & \textbf{0.840} & 0.837 & 0.833 \\ \hline
\hline
Targ  & 0.787 & 0.788 & \textbf{0.810} & 0.817 & 0.799 & 0.787 & 0.789 & \textit{\textbf{0.811}} & 0.817 & 0.798 \\ \hline
Untg  & 0.885 & 0.890 & \textit{\textbf{0.900}} & 0.892 & 0.894 & 0.883 & 0.886 & \textbf{0.899} & 0.892 & 0.895 \\ \hline
\hline
Prim  & 0.837 & 0.840 & \textit{\textbf{0.854}} & 0.854 & 0.845 & 0.837 & 0.840 & \textit{\textbf{0.854}} & 0.854 & 0.846 \\ \hline
Mix   & 0.835 & 0.838 & \textit{\textbf{0.857}} & 0.854 & 0.848 & 0.833 & 0.835 & \textbf{0.856} & 0.854 & 0.847 \\ \hline
\hline
Total & 0.836 & 0.839 & \textit{\textbf{0.855}} & 0.854 & 0.847 & 0.835 & 0.838 & \textit{\textbf{0.855}} & 0.854 & 0.846 \\ \hline
\end{tabular}
\caption{\label{tab:answerhood-map}Mean Average Precision (MAP) scores for the \feat{Answerhood} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, primary NS models), and for the full set (Total).
}
\end{center}
\end{table}

%% ANSWERHOOD MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Familiar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.868 & 0.871 & \textit{\textbf{0.882}} & 0.881 & 0.868 & 0.869 & 0.873 & \textbf{0.878} & 0.881 & 0.870 \\ \hline
Tran  & 0.824 & 0.826 & \textit{\textbf{0.852}} & 0.845 & 0.840 & 0.817 & 0.818 & \textbf{0.847} & 0.845 & 0.840 \\ \hline
Ditr  & 0.820 & 0.822 & \textit{\textbf{0.846}} & 0.837 & 0.832 & 0.820 & 0.822 & \textbf{0.845} & 0.837 & 0.835 \\ \hline
\hline
Targ  & 0.786 & 0.787 & \textit{\textbf{0.815}} & 0.817 & 0.798 & 0.785 & 0.787 & \textbf{0.813} & 0.817 & 0.802 \\ \hline
Untg  & 0.889 & 0.892 & \textit{\textbf{0.904}} & 0.892 & 0.896 & 0.885 & 0.889 & \textbf{0.900} & 0.892 & 0.894 \\ \hline
\hline
Total & 0.837 & 0.840 & \textit{\textbf{0.860}} & 0.854 & 0.847 & 0.835 & 0.838 & \textbf{0.857} & 0.854 & 0.848 \\ \hline
\end{tabular}
\caption{\label{tab:answer-fam-map}Mean Average Precision (MAP) scores for the \feat{Answerhood} annotation feature, comparing \param{familiar} and crowdsourced (\param{Crowd}) responses. MAP is derived from various response rankings: the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), weighted annotation ranking (WAR), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, targeted items), and for the full set (Total). Note that all models represented here are \param{mixed} due to the small number of familiar participants.
}
\end{center}
\end{table}


\subsection{\feat{Grammaticality} experiments}
\label{sec:map-gramm}

%%GRAMMATICALITY MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.868 & 0.870 & \textit{\textbf{0.872}} & 0.887 & 0.866 & 0.863 & 0.864 & \textbf{0.866} & 0.887 & 0.864 \\ \hline
Tran  & 0.753 & 0.756 & \textbf{0.757} & 0.781 & 0.757 & 0.758 & 0.760 & \textit{\textbf{0.761}} & 0.781 & 0.757 \\ \hline
Ditr  & 0.682 & 0.685 & \textit{\textbf{0.700}} & 0.695 & 0.694 & 0.679 & 0.685 & \textbf{0.697} & 0.695 & 0.693 \\ \hline
\hline
Targ  & 0.777 & 0.778 & \textit{\textbf{0.784}} & 0.800 & 0.782 & 0.776 & 0.776 & \textbf{0.783} & 0.800 & 0.781 \\ \hline
Untg  & 0.758 & 0.763 & \textit{\textbf{0.769}} & 0.776 & 0.762 & 0.757 & 0.762 & \textbf{0.766} & 0.776 & 0.761 \\ \hline
\hline
Prim  & 0.769 & 0.773 & \textit{\textbf{0.776}} & 0.788 & 0.770 & 0.768 & 0.770 & \textbf{0.774} & 0.788 & 0.770 \\ \hline
Mix   & 0.766 & 0.768 & \textit{\textbf{0.776}} & 0.788 & 0.774 & 0.765 & 0.768 & \textbf{0.775} & 0.788 & 0.772 \\ \hline
\hline
Total & 0.768 & 0.770 & \textit{\textbf{0.776}} & 0.788 & 0.772 & 0.767 & 0.769 & \textbf{0.775} & 0.788 & 0.771 \\ \hline
\end{tabular}
\caption{\label{tab:gramm-map}Mean Average Precision (MAP) scores for the \feat{Grammaticality} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, primary NS models), and for the full set (Total).
}
\end{center}
\end{table}



%% GRAMMATICALITY MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Familiar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.863 & 0.864 & \textbf{0.873}          & 0.887 & 0.863 & 0.868 & 0.869 & \textit{\textbf{0.874}} & 0.887 & 0.869 \\ \hline
Tran  & 0.760 & 0.759 & \textit{\textbf{0.762}} & 0.781 & 0.760 & 0.752 & 0.754 & \textbf{0.757}          & 0.781 & 0.758 \\ \hline
Ditr  & 0.678 & 0.685 & \textbf{0.698}          & 0.695 & 0.698 & 0.678 & 0.680 & \textit{\textbf{0.699}} & 0.695 & 0.696 \\ \hline
\hline
Targ  & 0.776 & 0.776 & \textit{\textbf{0.787}} & 0.800 & 0.783 & 0.776 & 0.777 & \textbf{0.786}          & 0.800 & 0.786 \\ \hline
Untg  & 0.757 & 0.762 & \textit{\textbf{0.768}} & 0.776 & 0.764 & 0.756 & 0.759 & \textbf{0.767}          & 0.776 & 0.763 \\ \hline
\hline
Total & 0.767 & 0.769 & \textit{\textbf{0.778}} & 0.788 & 0.773 & 0.766 & 0.768 & \textbf{0.776}          & 0.788 & 0.774 \\ \hline
\end{tabular}

\caption{\label{tab:gramm-fam-map}Mean Average Precision (MAP) scores for the \feat{Grammaticality} annotation feature, comparing \param{familiar} and crowdsourced (\param{Crowd}) responses. MAP is derived from various response rankings: the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), weighted annotation ranking (WAR), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, targeted items), and for the full set (Total). Note that all models represented here are \param{mixed} due to the small number of familiar participants.
}
\end{center}
\end{table}



\subsection{\feat{Interpretability} experiments}
\label{sec:map-interp}

%%INTERPRETABILITY MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.932                   & 0.931 & \textit{\textbf{0.933}} & 0.930 & 0.922 & 0.928          & 0.927 & \textit{\textbf{0.933}} & 0.930 & 0.923 \\
\hline
Tran  & \textit{\textbf{0.823}} & 0.821 & 0.811                   & 0.803 & 0.806 & \textbf{0.821} & 0.816 & 0.812                   & 0.803 & 0.804 \\
\hline
Ditr  & 0.789                   & 0.784 & \textit{\textbf{0.794}} & 0.721 & 0.777 & 0.786          & 0.782 & 
\textbf{0.792}          & 0.721 & 0.772 \\
\hline
\hline
Targ  & 0.835                   & 0.832 & \textit{\textbf{0.836}} & 0.804 & 0.828 & 0.833          & 0.829 & \textbf{0.834}          & 0.804 & 0.826 \\
\hline
Untg  & \textit{\textbf{0.862}} & 0.858 & 0.856                   & 0.833 & 0.842 & \textbf{0.857} & 0.855 & \textbf{0.857}          & 0.833 & 0.840 \\
\hline
\hline
Prim  & \textit{\textbf{0.847}} & 0.845 & 0.846                   & 0.818 & 0.837 & 0.845          & 0.842 & \textbf{0.846}          & 0.818 & 0.833 \\
\hline
Mix   & \textit{\textbf{0.849}} & 0.846 & 0.846                   & 0.818 & 0.833 & 0.844          & 0.841 & \textbf{0.845}          & 0.818 & 0.833 \\
\hline
\hline
Total & \textit{\textbf{0.848}} & 0.845 & 0.846                   & 0.818 & 0.835 & \textbf{0.845} & 0.842 & \textbf{0.845}          & 0.818 & 0.833 \\
\hline
\end{tabular}
\caption{\label{tab:interpretability-map}Mean Average Precision (MAP) scores for the \feat{Interpretability} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, primary NS models), and for the full set (Total).
}
\end{center}
\end{table}


%% INTERPRETABILITY MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Familiar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.930          & 0.930 & \textit{\textbf{0.934}} & 0.930 & 0.923 & \textbf{0.933}          & 0.931 & 0.932          & 0.930 & 0.922 \\ \hline
Tran  & \textbf{0.822} & 0.819 & 0.811                   & 0.803 & 0.805 & \textit{\textbf{0.826}} & 0.824 & 0.811          & 0.803 & 0.805 \\ \hline
Ditr  & 0.787          & 0.786 & \textit{\textbf{0.796}} & 0.721 & 0.782 & 0.788                   & 0.783 & \textbf{0.795} & 0.721 & 0.772 \\ \hline
Targ  & 0.835          & 0.833 & \textit{\textbf{0.836}} & 0.804 & 0.830 & \textbf{0.835}          & 0.832 & \textbf{0.835} & 0.804 & 0.825 \\ \hline
Untg  & \textbf{0.858} & 0.857 & \textbf{0.858}          & 0.833 & 0.843 & \textit{\textbf{0.863}} & 0.859 & 0.857          & 0.833 & 0.841 \\ \hline
Total & \textbf{0.847} & 0.845 & \textbf{0.847}          & 0.818 & 0.837 & \textit{\textbf{0.849}} & 0.846 & 0.846          & 0.818 & 0.833 \\ \hline
\end{tabular}
\caption{\label{tab:gramm-fam-map}Mean Average Precision (MAP) scores for the \feat{Interpretability} annotation feature, comparing \param{familiar} and crowdsourced (\param{Crowd}) responses. MAP is derived from various response rankings: the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), weighted annotation ranking (WAR), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, targeted items), and for the full set (Total). Note that all models represented here are \param{mixed} due to the small number of familiar participants.
}
\end{center}
\end{table}





\subsection{\feat{Verifiability} experiments}
\label{sec:map-verif}

%%VERIFIABILITY MAP - N14 & N50
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Crowd} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 50} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.852                   & 0.852 & \textit{\textbf{0.853}} & 0.866 & 0.840 & 0.849                   & 0.849          & \textbf{0.851} & 0.866 & 0.836 \\
\hline
Tran  & \textit{\textbf{0.809}} & 0.808 & 0.803                   & 0.798 & 0.787 & \textbf{0.807}          & 0.806 & 0.803          & 0.798 & 0.785 \\
\hline
Ditr  & 0.814                   & 0.812 & \textit{\textbf{0.815}} & 0.780 & 0.798 & 0.811                   & 0.809          & \textbf{0.812} & 0.780 & 0.796 \\
\hline
\hline
Targ  & \textit{\textbf{0.825}} & 0.824 & \textit{\textbf{0.825}} & 0.815 & 0.812 & \textit{\textbf{0.825}} & 0.824          & 0.823          & 0.815 & 0.810 \\
\hline
Untg  & \textit{\textbf{0.825}} & 0.824 & 0.822                   & 0.815 & 0.805 & \textbf{0.820}          & 0.819          & \textbf{0.820} & 0.815 & 0.802 \\
\hline
\hline
Prim  & \textit{\textbf{0.826}} & 0.824 & 0.823                   & 0.815 & 0.808 & \textbf{0.824}          & 0.823          & 0.822          & 0.815 & 0.806 \\
\hline
Mix   & \textit{\textbf{0.825}} & 0.824 & 0.824                   & 0.815 & 0.808 & \textbf{0.821}          & \textbf{0.821} & \textbf{0.821} & 0.815 & 0.805 \\
\hline
\hline
Total & \textit{\textbf{0.825}} & 0.824 & 0.824                   & 0.815 & 0.808 & \textbf{0.823}          & 0.822          & 0.822          & 0.815 & 0.806 \\
\hline
\end{tabular}
\caption{\label{tab:verifiability-map}Mean Average Precision (MAP) scores for the \feat{Verifiability} annotation feature, derived from various response rankings: weighted annotation ranking (WAR), the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, primary NS models), and for the full set (Total).
}
\end{center}
\end{table}


%% VERIFIABILITY MAP - F14 & N14
\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.35em}
\begin{tabular}{|l||l|l|l||l|l||l|l|l||l|l|}
\hline
 & \multicolumn{5}{c||}{\param{Familiar} NS model = 14} & \multicolumn{5}{c|}{\param{Crowd} NS model = 14} \\
\hline
    		& ldh	& xdh &	xdx & WAR	& BERT & ldh	& xdh &	xdx & WAR	& BERT \\ \hline
\hline
Intr  & 0.847                   & 0.847 & \textbf{0.852} & 0.866 & 0.836 & 0.852                   & 0.852 & \textit{\textbf{0.854}} & 0.866 & 0.843 \\ \hline
Tran  & \textit{\textbf{0.808}} & 0.807 & 0.803          & 0.798 & 0.787 & \textbf{0.807}          & 0.807 & 0.802                   & 0.798 & 0.786 \\ \hline
Ditr  & 0.811                   & 0.811 & \textbf{0.812} & 0.780 & 0.802 & 0.815                   & 0.812 & \textit{\textbf{0.817}} & 0.780 & 0.796 \\ \hline
Targ  & 0.821                   & 0.821 & \textbf{0.822} & 0.815 & 0.814 & 0.824                   & 0.824 & \textit{\textbf{0.826}} & 0.815 & 0.811 \\ \hline
Untg  & \textbf{0.824}          & 0.822 & 0.823          & 0.815 & 0.803 & \textit{\textbf{0.825}} & 0.824 & 0.823                   & 0.815 & 0.806 \\ \hline
Total & 0.822                   & 0.822 & \textbf{0.823} & 0.815 & 0.808 & \textit{\textbf{0.825}} & 0.824 & 0.824                   & 0.815 & 0.808 \\ \hline
\end{tabular}
\caption{\label{tab:gramm-fam-map}Mean Average Precision (MAP) scores for the \feat{Verifiability} annotation feature, comparing \param{familiar} and crowdsourced (\param{Crowd}) responses. MAP is derived from various response rankings: the three system term representation rankings (labeled dependencies (ldh), unlabeled dependencies (xdh), and dependents only (xdx)), weighted annotation ranking (WAR), and BERT rankings. MAP scores are shown for each item type or parameter setting (e.g, intransitive items, targeted items), and for the full set (Total). Note that all models represented here are \param{mixed} due to the small number of familiar participants.
}
\end{center}
\end{table}




\section{Holistic experiments}
\label{sec:exp-holistic}

\section{Transitivity experiments}
\label{sec:exp-transitivity}
Here I examine the performance of my ranking system when applied to items that are canonically either intransitive, transitive, or ditransitive. Unlike the other variables throughout this chapter, transitivity is not a parameter setting. Individual PDT items are assumed to fit predominately only one of the three transitivity types here. In other words, I cannot choose to process a given item with any of the three transitivity settings. Rather, the experiments in this section examine my system's performance across three sets of 10 items each, representing intransitive, transitive and ditransitive events.

In experimenting with this parameter (and most of the others throughout this chapter), I compare models of two different sizes in order to see the effects of sample size on performance. The smaller of the two models contains 14 NS responses per item, and the larger model contains 50 NS responses per item. My system uses these models as the basis of its tf-idf cosine similarity measure that is used to score each response and in turn rank the full set of 70 NNS responses. As a benchmark, I also use BERT with the same NS models to produce similarity scores and rank the NNS responses.

Sets of descriptive statistics for the Spearman rank correlation scores produced by my system and BERT using these models are presented in Table~\ref{tab:transitivity-results}. There are 10 items per type. Each item has a targeted and untargeted version, which are separate datasets. For each dataset, I sample both primary and mixed models. My system converts the response text to three different term representations (\param{ldh}, \param{xdh}, \param{xdx}). This results in 12 system configurations: 2 targeting settings $\times$ 2 primacy settings $\times$ 3 term representations. Using all 12 configurations results in 120 Spearman rank correlations per transitivity type. The system statistics presented in Table~\ref{tab:transitivity-results} cover these 120 scores. Because BERT operates on plain text and cannot make use of the term representation variable, it involves only four configurations, resulting in 40 Spearman scores per transitivity type.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l|}
\hline
 & \multicolumn{6}{c|}{NS model sample size = 14} \\
\hline
 & \multicolumn{2}{c||}{Intransitives} & \multicolumn{2}{c||}{Transitives} & \multicolumn{2}{c|}{Ditransitives} \\
\hline
		& System 	& BERT 		& System 	& BERT 		& System 	& BERT 		\\
\hline
\hline
count 	& 120 		& 40 		& 120 		& 40 		& 120 		& 40		 \\
\hline
mean 	& 0.439 	& \textbf{0.497} 	& 0.314 	& \textit{\textbf{0.563}}		& 0.267 	& \textbf{0.400}	 \\
\hline
median 	& 0.416 	& \textbf{0.479} 	& 0.304 	& \textit{\textbf{0.555}}		& 0.276 	& \textbf{0.444}	 \\
\hline
min 	& -0.119 	& \textit{\textbf{0.199}} 	& -0.110 	& \textit{\textbf{0.199}}	& -0.181 	& \textbf{-0.138} \\
\hline
max 	& \textit{\textbf{0.900}} 	& 0.881		& \textbf{0.777} 	& 0.772		& \textbf{0.710} 	& 0.697	 \\
\hline
std dev & 0.228 	& 0.189		& 0.218 	& 0.134		& 0.198 	& 0.222	 \\
\hline
\multicolumn{7}{c}{} \\
\hline
 & \multicolumn{6}{c|}{NS model sample size = 50} \\
\hline
 & \multicolumn{2}{c||}{Intransitives} & \multicolumn{2}{c||}{Transitives} & \multicolumn{2}{c|}{Ditransitives} \\
\hline
		& System 	& BERT 				& System 	& BERT 						& System 	& BERT \\
\hline
\hline
count 	& 120 		& 40 				& 120 		& 40 						& 120 		& 40 	\\
\hline
mean 	& 0.423 	& \textbf{0.516} 	& 0.345 	& \textit{\textbf{0.566}}	& 0.278 	& \textbf{0.446} \\
\hline
median 	& 0.426 	& \textbf{0.517}	& 0.331 	& \textit{\textbf{0.561}}	& 0.286 	& \textbf{0.471} \\
\hline
min 	& -0.076 	& \textbf{0.200}	& -0.204 	& \textit{\textbf{0.222}}	 & -0.185 	& \textbf{-0.090} \\
\hline
max & \textit{\textbf{0.898}} & 0.881	& \textbf{0.778} & 0.771	 			& 0.708 	& \textbf{0.709} \\
\hline
std dev & 0.249 	& 0.172 			& 0.207 	& 0.135 					& 0.195 	& 0.200 \\
\hline
\end{tabular}
\caption{\label{tab:transitivity-results} Comparing Spearman rank correlation scores for \param{intransitive}, \param{transitive} and \param{ditransitive} PDT items, using NS models of either 14 or 50 random responses per item. Each \textit{System} column represents 120 different rankings (12 system configurations $\times$ 10 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 40 rankings (4 system configurations $\times$ 10 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}





\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.6em}
\begin{tabular}{|l|l||l|l|l||l|l|l|}
\hline
 && \multicolumn{3}{c||}{NS model = 14} & \multicolumn{3}{c|}{NS model = 50} \\
\hline
	&	& Intrans 	& Trans 	& Ditrans 		& Intrans 	& Trans 	& Ditrans 		\\
\hline
\multirow{3}{*}{\begin{sideways}Words/Response~ \end{sideways}} & responses	& 560  & 560 & 560 & 2000 & 2000 & 2000	 \\
\cline{2-8}
& mean 		& 4.9 	& 6.2		& \textbf{7.1} 	& 4.9		& 6.2 	& \textbf{7.3}	 \\
\cline{2-8}
& median 	& 4.0 	& 6.0		& \textbf{7.0} 	& 4.0		& 6.0 	& \textbf{7.0}	 \\
\cline{2-8}
& min 		& 1.0 	& 1.0		& 1.0 			& 1.0		& 1.0 	& 1.0	 \\
\cline{2-8}
& max 		& 19.0 	& 16.0	& \textbf{26.0} & \textbf{26.0}	& 24.0 	& \textbf{26.0}	 \\
\cline{2-8}
& std dev 	& 2.6 	& 2.5		& 3.2 			& 2.6		& 2.7 	& 3.2	 \\
\hline
\hline
\multirow{3}{*}{\begin{sideways}Model TTR~~~~~ \end{sideways}} & models	& 40 & 40 & 40 	& 40 & 40 & 40	 \\
\cline{2-8}
& mean 		& 0.360 	& \textbf{0.329} 		& 0.334 	& 0.220		& 0.192 		& \textbf{0.189}	 \\
\cline{2-8}
& median 	& 0.369 	& \textbf{0.329}		& 0.337 	& 0.218		& 0.190 		& \textbf{0.184}	 \\
\cline{2-8}
& min 		& 0.169 	& \textbf{0.166}		& 0.210 	& 0.116		& 0.122 		& \textbf{0.084}	 \\
\cline{2-8}
& max 		& 0.543 	& 0.542			& \textbf{0.484} 	& 0.302		& \textbf{0.274} & 0.276	 \\
\cline{2-8}
& std dev 	& 0.101 	& 0.079					& 0.075 	& 0.046		& 0.040 		& 0.049	 \\
\hline
\end{tabular}
\caption{\label{tab:transitivity-model-stats}Comparing the number of words per response and the model word type-to-token ratio (TTR) for \param{intransitive}, \param{transitive} and \param{ditransitive} PDT items, using NS models of either 14 or 50 random responses per item.
}
\end{center}
\end{table}

Table~\ref{tab:transitivity-results} shows some notable patterns. In all cases, BERT outperforms my system on the most important metrics here: mean and median Spearman scores. This is expected, as BERT and SBERT are powerful, highly developed, state-of-the-art approaches to language modeling and sentence similarity and are trained on web-scale datasets. My similarity measure is far less sophisticated and involves tools trained on much smaller datasets, but also has the advantage of preserving a high degree of transparency and explainability. My system does, however, reach a higher maximum Spearman score than BERT in all but one case.

For both size models, my system performs best on intransitives, followed by transitives and then ditransitives. My system achieves its highest mean and median Spearman scores on the intransitives using the smaller model, even beating out its performance using the larger model. For the larger model, performance on the ditransitives increases over the smaller model in terms of the mean and median Spearman scores. Transitives, as might be expected, fall in the middle: for the larger model, the mean is higher but the median is lower.

Naturally, we can expect intransitives to be the simplest of these types, and ditransitives the most complex, because ditransitive verbs require more arguments. As a metric of complexity, I examined type-to-token ratios (TTRs) for the words in NS models, and for the 50 response models, this pattern was shown to be true, as seen in Table~\ref{tab:transitivity-model-stats}. Combined with the TTRs, the performance observations above suggest a trend: where the responses can be expected to be relatively simple and constrained, my system performs better with a smaller sample, but where the responses are relatively varied, my system performs better with a larger sample. Future research could explore this curve by sampling a wider range of model sizes, ideally including models larger than 50 responses.

Like my system, BERT achieves its highest mean and median Spearman scores with the smaller model. For both size models, BERT performs best on transitives, however, followed by intransitives and then ditransitives, which research suggests is an inherent feature of BERT. \citet{papadimitriou2021multilingual} examine BERT's accuracy in encoding nouns as either a subject or an object. BERT does not produce these labels, so the researchers used BERT embeddings corresponding to the nouns derived from sentence embeddings and used them to train classifiers to recognize these noun roles. They found that BERT is highly accurate at the task overall, but least accurate at handling intransitive subjects; the most common errors being when the classifiers label these noun embeddings as objects. This is consistent with BERT performance peaking on transitives in my study. BERT is most accurate with \textit{agent} role subjects and objects, which are most consistently present in transitive items. BERT's lower accuracy at handling \textit{experiencer} subjects, commonly seen in intransitive sentences, explains the lower performance on intransitive items, despite their relative simplicity. For ditransitives, BERT's lower performance may be influenced by the increased complexity, but research suggests this is more directly related to sentence length, and ditransitive items, naturally, are the longest in my dataset, as they require more verb arguments (see Table~\ref{tab:transitivity-model-stats}). \citet{warstadt2019} also used classifiers trained on a corpus of sentences with corresponding BERT output and human grammaticality judgments and found that performance correlates strongly with sentence length, with a sharp drop appearing for sentences of 11 words or more.

\section{Targeting experiments}
\label{sec:exp-targeting}
Here we compare the performance of my ranking system when applied to targeted vs untargeted data.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l||l|l|}
\hline
& \multicolumn{4}{c||}{NS model sample size = 14} & \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
 & \multicolumn{2}{c||}{Targeted} & \multicolumn{2}{c||}{Untargeted} & \multicolumn{2}{c||}{Targeted} & \multicolumn{2}{c|}{Untargeted} \\
\hline
	& System 		& BERT 		& System 	& BERT 								& System 		& BERT 						& System 		& BERT \\
\hline
count 	& 180 		& 60 		& 180 		& 60 								& 180 			& 60 						& 180 			& 60 \\
\hline
mean 	& 0.380 	& \textit{\textbf{0.530}} 	& 0.300 	& \textbf{0.444} 	& 0.393 		& \textit{\textbf{0.550}} 	& 0.305 		& \textbf{0.469}  \\
\hline
median 	& 0.369 	& \textit{\textbf{0.545}} 	& 0.314 	& \textbf{0.472} 	& 0.389 		& \textit{\textbf{0.564}} 	& 0.323			& \textbf{0.496} \\
\hline
min & -0.147 	& \textbf{-0.138} & -0.181 	& \textit{\textbf{-0.107}} 			& \textbf{-0.048} & -0.090					& -0.185 		& \textit{\textbf{0.132}} \\
\hline
max 	& 0.840 	& \textbf{0.879} 	& \textit{\textbf{0.900}}	& 0.881 	& 0.872 		& \textbf{0.881}			& \textit{\textbf{0.898}} 	& 0.880 \\
\hline
std dev 	& 0.241 	& 0.192 	& 0.204 	& 0.191 						& 0.234 		& 0.173						& 0.208 		& 0.172 \\
\hline
\end{tabular}
\caption{\label{tab:targeting-results} Comparing Spearman rank correlation scores for \param{Targeted} and \param{Untargeted} versions of the PDT data, using NS models of either 14 or 50 random responses per item. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 60 rankings (2 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}



\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.6em}
\begin{tabular}{|l|l||l|l||l|l|}
\hline
 && \multicolumn{2}{c||}{NS model = 14} & \multicolumn{2}{c|}{NS model = 50} \\
\hline
	&	& Targeted 	& Untarget 	& Targeted 	& Untargeted  	\\
\hline
\multirow{3}{*}{\begin{sideways}words / response \end{sideways}} & responses & 840 & 840 & 3000 & 3000  \\
\cline{2-6}
& mean 		& 5.40 	& 6.85		& 5.49 	& 6.79	 \\
\cline{2-6}
& median 	& 5.0 	& 6.0		& 5.0 	& 6.0	\\
\cline{2-6}
& min 		& 1.0 	& 1.0		& 1.0 	& 1.0	 \\
\cline{2-6}
& max 		& 18.0 	& 26.0		& 26.0 	& 26.0	 \\
\cline{2-6}
& std dev 	& 2.63 	& 3.16		& 2.84 	& 3.07	 \\
\hline
\hline
\multirow{3}{*}{\begin{sideways} model TTR \end{sideways}} & models	& 60  & 60 & 60 & 60  \\
\cline{2-6}
& mean 		& 0.316 	& 0.367		& 0.197 	& 0.205	 \\
\cline{2-6}
& median 	& 0.312 	& 0.371		& 0.196 	& 0.205	\\
\cline{2-6}
& min 		& 0.166 	& 0.180		& 0.084 	& 0.116	 \\
\cline{2-6}
& max 		& 0.543 	& 0.542		& 0.302 	& 0.292	 \\
\cline{2-6}
& std dev 	& 0.081 	& 0.085		& 0.049 	& 0.045	 \\
\hline
\end{tabular}
\caption{\label{tab:targeting-model-stats}Comparing the number of words per response and the model type-to-token ratio (TTR) for \param{Targeted} and \param{Untargeted} PDT items, using NS models of either 14 or 50 random responses per item.
}
\end{center}
\end{table}



\section{Familiarity experiments}
\label{sec:exp-familiarity}
Here we compare how well the system works when using different sources of NSs. 
\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
 \hline
 & \multicolumn{2}{c||}{\param{Familiar} NS} & \multicolumn{2}{c|}{\param{Crowd} NS} \\
\hline
		& System 			& BERT 						& System 			& BERT 				\\
\hline
\hline
count 	& 180 				& 60 						& 180 				& 60 				\\
\hline
mean 	& 0.338 		& \textit{\textbf{0.499}} 		& 0.339 			& \textbf{0.481} 	\\
\hline
median 	& 0.329 		& \textit{\textbf{0.513}} 		& 0.326 			& \textbf{0.500}   \\
\hline
min & -0.239 			& \textit{\textbf{-0.026}} 		& -0.181 			& \textbf{-0.125}  \\
\hline
max & \textit{\textbf{0.896}} & 0.880 					& 0.875 			& \textbf{0.879} 	\\
\hline
std dev & 0.217 			& 0.173 					& 0.224 			& 0.185 			\\
\hline
\end{tabular}
\caption{\label{tab:familiarity-results} Comparing Spearman rank correlation scores where \param{Familiar} NS models contain only responses from participants \textit{familiar} to the researcher and \param{Crowd} NS models contain only responses from crowdsourced participants. Results are shown using NS models of 14 responses; note the models used here are \param{mixed} (containing first and second responses; see Section~\ref{sec:exp-primacy}) due to the sparsity of \param{familiar} data. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 60 rankings (2 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}



\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
 \hline
 & \multicolumn{2}{c||}{words / response} & \multicolumn{2}{c|}{model TTR} \\
\hline
	& \param{Fam} 	& \param{Crowd} 	& \param{Fam} 			& \param{Crowd} 		\\
\hline
\hline
count 	& 840 			& 840 				& 60 				& 60 		\\
\hline
mean 	& 6.76 			& 6.50 				& 0.180 			& 0.382 		\\
\hline
median 	& 6.0 			& 6.0 				& 0.225 			& 0.383   	\\
\hline
min 	& 2.0 			& 1.0 				& 0.134 			& 0.252  		\\
\hline
max 	& 31.0 			& 26.0 				& 0.292 			& 0.543 		\\
\hline
std dev & 2.78 			& 3.24 				& 0.042 			& 0.070 		\\
\hline
\end{tabular}
\caption{\label{tab:familiarity-model-stats}Comparing the number of words per response and the model type-to-token ratio (TTR) for \param{Familiar} and \param{Crowdsourced} responses, using 14 responses per PDT item. \textit{Count} shows the total number of responses (840) and the total number of models (60) per participant group.
}
\end{center}
\end{table}



\section{Primacy experiments}
\label{sec:exp-primacy}
Here we compare how well the system works when using NSs' first responses vs a mix of first and second responses.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c||}{NS model sample size = 14} & \multicolumn{4}{c|}{NS model sample size = 50} \\
 \hline
 & \multicolumn{2}{c||}{Primary} & \multicolumn{2}{c||}{Mixed} & \multicolumn{2}{c||}{Primary} & \multicolumn{2}{c|}{Mixed} \\
\hline
			& System 		& BERT 		& System 	& BERT 						& System 		& BERT 				& System 			& BERT \\
\hline
\hline
count 		& 180 			& 60 		& 180 		& 60 						& 180 			& 60 				& 180 				& 60 \\
\hline
mean 		& 0.339 & \textit{\textbf{0.493}} 	& 0.340 	& \textbf{0.481} 	& 0.354 	& \textit{\textbf{0.514}} 	& 0.344 	& \textbf{0.505} \\
\hline
median 	& 0.326 & \textit{\textbf{0.517}} & 0.334 	& \textbf{0.500}  			& 0.345 	& \textit{\textbf{0.532}} 	& 0.350 	& \textbf{0.518}  \\
\hline
min 	& -0.181 & \textbf{-0.138} 		& -0.158 	& \textit{\textbf{-0.125}}  & -0.185 	& \textbf{-0.090} 		& -0.147 		& \textit{\textbf{0.049}}  \\
\hline
max & \textbf{0.875} & 0.881 & \textit{\textbf{0.900}} & 0.879 					& \textit{\textbf{0.898}} & 0.880 	& \textbf{0.894} 	& 0.881 \\
\hline
std dev & 0.224 			& 0.207 	& 0.230 	& 0.185 					& 0.226 	& 0.186 					& 0.226 		& 0.168 \\
\hline
\end{tabular}
\caption{\label{tab:primacy-results} Comparing Spearman rank correlation scores where \param{primary} models contain only first responses from NSs and \param{mixed} models contain an equal mix of first and second responses from NSs. Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 60 rankings (2 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}


\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.6em}
\begin{tabular}{|l|l||l|l||l|l|}
\hline
 && \multicolumn{2}{c||}{NS model = 14} & \multicolumn{2}{c|}{NS model = 50} \\
\hline
	&	& Primary 	& Mixed 	& Primary 	& Mixed  	\\
\hline
\multirow{3}{*}{\begin{sideways}words / response~ \end{sideways}} & responses & 840 & 840 & 3000 & 3000  \\
\cline{2-6}
& mean 		& 5.76 	& 6.5		& 5.83 	& 6.46	 \\
\cline{2-6}
& median 	& 5.0 	& 6.0		& 6.0 	& 6.0	\\
\cline{2-6}
& min 		& 1.0 	& 1.0		& 1.0 	& 1.0	 \\
\cline{2-6}
& max 		& 20.0 	& 26.0		& 26.0 	& 26.0	 \\
\cline{2-6}
& std dev 	& 2.67 	& 3.24		& 2.74 	& 3.26	 \\
\hline
\hline
\multirow{3}{*}{\begin{sideways} model TTR~ \end{sideways}} & models	& 60  & 60 & 60 & 60  \\
\cline{2-6}
& mean 		& 0.301 	& 0.382		& 0.180 	& 0.221	 \\
\cline{2-6}
& median 	& 0.297 	& 0.383		& 0.181 	& 0.225	\\
\cline{2-6}
& min 		& 0.166 	& 0.252		& 0.084 	& 0.134	 \\
\cline{2-6}
& max 		& 0.534 	& 0.543		& 0.292 	& 0.302	 \\
\cline{2-6}
& std dev 	& 0.082 	& 0.705		& 0.042 	& 0.043	 \\
\hline
\end{tabular}
\caption{\label{tab:primacy-model-stats}Comparing the number of words per response and the model type-to-token ratio (TTR) for \param{primary} and \param{mixed} models, using NS models of either 14 or 50 random responses per item.
}
\end{center}
\end{table}




\section{Term normalization experiments}
\label{sec:exp-term-norm}

In my pipeline, the NS model for each PDT item is comprised of some number of NS responses.  The length of these responses can vary; some valid responses contain only one or two words\footnote{Participants were instructed to provide complete sentences, but incomplete sentences were still judged valid where appropriate; see Chapter~\ref{chap:data} and the Annotation Guide in Appendix~\ref{appendix:annotation_guide}.}, while the longest perfectly annotated responses top out at around 15 words and some less-than-perfect responses exceed 30 words. These longer responses are relatively uncommon, but understanding their impact on a model is an important step in optimizing my response rating process. So far, I have treated each NS model as a flat ``bag of terms'' in which each term (cf. \textit{dependency}; see Section~\ref{sec:response-rep}) contributes equally, meaning longer responses carry more weight in the model. This has the potential to introduce noise. 

My hypothesis is that system performance should improve by using NS models where each term token is re-weighted by $1/\textit{n}$ before it is added to the model, where \textit{n} is the number of term tokens in the response containing the term token. In other words, I believe dependencies should be re-weighted to ensure that every NS \textit{response}---not \textit{term}---contributes equal weight to the model. The rationale here is simple. Every response used in the NS model is assumed to contain the information that is crucial for satisfying the PDT prompt, and the number of terms conveying this information is roughly equivalent from one response to another. Thus, as the number of terms in a response increases (above some minimum number), the likelihood that any given term in that response is crucial decreases.


\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Response A & Response B & Norm. wt. & Non-norm. wt.\\
\hline
\multirow{2}{*}{The girl is singing} & The girl in the cute purple & & \\
& dress is singing a song & & \\
\hline
\hline
det(the, girl) & det(the, girl) & 0.175 & 0.143 \\
\hline
nsubj(girl, sing) & nsubj(girl, sing) & 0.175 & 0.143 \\
\hline
%& erased(in, ERASED) & 0.050 & 0.071 \\
%& \textit{erased(in, ERASED)} & 0.000 & 0.000 \\
& \textit{erased(in, ERASED)} & --- & --- \\
\hline
& det(the, dress) & 0.050 & 0.071 \\
\hline
& amod(cute, dress) & 0.050 & 0.071 \\
\hline
& amod(purple, dress) & 0.050 & 0.071 \\
\hline
%& prep\_in(dress, girl) & 0.050 & 0.071 \\
& prep\_in(dress, girl) & 0.050 & 0.071 \\
\hline
aux(be, sing) & aux(be, sing) & 0.175 & 0.143 \\
\hline
root(sing, ROOT) & root(sing, ROOT) & 0.175 & 0.143 \\
\hline
& det(a, song) & 0.050 & 0.071 \\
\hline
& dobj(song, sing) & 0.050 & 0.071 \\
\hline
\hline
4 & 10 & 1.0 & 1.0 \\
\hline
\end{tabular}
\caption{\label{tab:normalize-responses-deps} A ``toy'' model consisting of lemmatized syntactic dependencies from only two NS responses, each with perfect annotation scores. (Note that the version of Stanford typed dependencies used in this work collapses dependencies containing prepositions and incorporates prepositions in a label, resulting in the ``prep\_in'' and ``erased'' dependencies above. See Section~\ref{sec:rule-method} for more on the parsing and lemmatization.)}
\end{center}
\end{table}

This is illustrated by the responses in Table~\ref{tab:normalize-responses-deps}. If we take these two responses to constitute one NS model, Response A contributes four dependencies, each of which is necessary to satisfy the five annotated features and contributes meaningfully to the model. Response B, however, contributes 10 dependencies, some of which, like \textit{amod(purple, dress)}, add non-critical detail. In a non-normalized setting, this dependency constitutes one out of a total 14 dependencies in the NS model, approximately 0.071. In a normalized setting, however, this dependency appears as zero of four (0.0) dependencies in Response A, and one of 10 (0.1) in Response B, making it 0.05 of the overall model (0.1 divided by two responses). This should have the effect of making extraneous information in the model less impactful on response ratings.

Taking this example further, consider the dependency \textit{nsubj(girl, sing)}, also from Table~\ref{tab:normalize-responses-deps}. In the non-normalized setting, this dependency appears as two out of a total 14 dependencies, or 0.143 of the NS model. In the normalized setting, the dependency appears as one out of four (0.25) dependencies in Response A, and one out of 10 (0.1) dependencies in Response B, equating to 0.175 of the NS model (0.35 divided by two responses). Because this dependency is critical, raising its weight from 0.143 to 0.175 should have a positive impact on system performance; NNS responses containing the dependency should rise in the rankings relative to those without it. 

To test my hypothesis, I compared the performance of normalized and non-normalized models. This followed my standard design of ranking NNS responses by their system scores, then comparing this ranking with the weighted annotation ranking using Spearman rank correlation. I isolate this variable as before: for all system configurations, I generate a Spearman score. Then I obtain an average score for all normalized configurations and an average score for all non-normalized configurations and compare the two.

Such normalization can be sensitive to the effects of size\lk{XYZ: Citation?}, so I conducted two experiments: one in which each NS model contains a sample 50 responses, and one in which each model contains 14 responses.

%This experiment was conducted with the largest XGS (\textit{all NS responses}) and the smallest XGS (\textit{all familiar NS responses}) to ensure that the effect of normalization is consistent. This process was repeated for the same data sets without the normalization; the Spearman scores for the normalized and non-normalized GSs were compared.

%
%To account for this and examine whether or not it poses a problem, I conducted experiments in which each \textit{response}, not each \textit{dependency}, contributes equally to the XGS. This meant simply normalizing for the length of the response by applying a weight to each dependency token as it was added to the XGS, where the weight is equal to 1 divided by the total number of dependencies in the response. The responses were then ranked by these scores, and Spearman's rank correlation coefficient (``Spearman'') was used to compare the ranking against the ``true'' GS (TGS), which is the result of ranking the responses using the weighted annotations (discussed in Chapter~\ref{chap:annotation}). This experiment was conducted with the largest XGS (\textit{all NS responses}) and the smallest XGS (\textit{all familiar NS responses}) to ensure that the effect of normalization is consistent. \lk{Get avg \# responses for All NS vs Familiar NS} This process was repeated for the same data sets without the normalization; the Spearman scores for the normalized and non-normalized GSs were compared.

The results of this experiment are shown in Table~\ref{tab:term-norm-results}. The comparisons show very little difference in the correlations, with only one of the 12 experiments showing a slightly stronger correlation for the normalized model. The simplest explanation for this is the fact that longer responses with extraneous information are relatively uncommon, so we can expect this normalization to have a low impact.
%\lk{XYZ. For different test responses, find some examples where the non-normalized config is rated/ranked higher than in the normalized config?}
\lk{XYZ. Add examples}

Because the non-normalized GSs outperform their normalized counterparts, and for the sake of simplicity, this parameter was not adopted in the other experiments discussed in this chapter; only non-normalized configurations were used in all other experiments.
\begin{table}[htb!]
\begin{center}
%\begin{tabular}{|l||p{0.12\textwidth}|p{0.12\textwidth}|p{0.12\textwidth}||l|l|l|}
\begin{tabular}{|l||l|l|l||l|l|l|}
\hline
 & \multicolumn{3}{c||}{NS model sample size = 14} & \multicolumn{3}{c|}{NS model sample size = 50} \\
\hline
		& Non-n. 		& Norm 			& BERT 								& Non-n. 			& Norm 				& BERT 		\\
\hline
\hline
count 	& 360 			& 360 			& 120 								& 360 				& 360 				& 120		 \\
\hline
mean 	& \textbf{0.340} & 0.335 & \textit{\textbf{0.487}}					& \textbf{0.349} 	& 0.347 		& \textit{\textbf{0.509}}		 \\
\hline
median 	& \textbf{0.332} & 0.313 & \textit{\textbf{0.507}} 					& \textbf{0.348} 	& 0.333 		& \textit{\textbf{0.523}}		 \\
\hline
min 	& \textbf{-0.181} 	& -0.219 & \textit{\textbf{-0.138}} 			& \textbf{-0.185} 	& -0.230 		& \textit{\textbf{-0.090}}		 \\
\hline
max	& \textit{\textbf{0.900}} & 0.891 	& 0.881 							& 0.898 	& \textit{\textbf{0.899}} 	& 0.881		 \\
\hline
std dev & 0.226 		& 0.227 		& 0.196 							& 0.225 		& 0.230 				& 0.177		 \\
\hline
\end{tabular}
\caption{\label{tab:term-norm-results} Comparing Spearman rank correlation scores where all dependencies (terms) in \textit{Non-n(ormalized)} NS models carry equal weight, and all dependencies in \textit{Norm(alized)} NS models have their scores normalized proportionally to the length of the parent response. Results are shown using NS models of 14 responses and 50 responses. Each \textit{Norm(alized)} and \textit{Non-n(ormalized)} column represents 360 different rankings (12 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 120 rankings (4 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).
%%% 4/16/21 LK OK
}
\end{center}
\end{table}


\section{Term representation experiments}
\label{sec:exp-term-reps}
The current system allows for different \textit{term representations}, which are variations on syntactic dependencies. A dependency consists of a \textit{head}, \textit{dependent}, and \textit{label}. In past work, I experimented with omitting one or more of these elements to allow for less restrictive matching (see Table~\ref{tab:dist-ranked-parameters}). In the current dissertation, I compare the system performance using the three formats: \textit{label-dependent-head} (\param{ldh}), \textit{dependent-head} only (\param{xdh}), and \textit{dependent} only (\param{xdx}). In other words, my system uses a ``bag of terms'' approach, where the bags contain either labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}) or words (\param{xdx}). The labeled and unlabeled dependencies were the top performers in my previous work, and the \param{xdx} format is included as a kind of baseline showing a bag of words approach.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l|l||l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
\hline
			& \param{ldh} 	& \param{xdh} 			& \param{xdx} 		& BERT \\
\hline
\hline
count 			& 120 		& 120 					& 120 				& 120 \\
\hline
mean 			& 0.333 	& 0.336 			& \textbf{0.351} 	& \textit{\textbf{0.487}} \\
\hline
median 			& 0.318 	& \textbf{0.344} 		& 0.330 		& \textit{\textbf{0.507}} \\
\hline
min & \textit{\textbf{-0.108}} 	& -0.181 				& -0.158 	& 	-0.138 \\
\hline
max 			& 0.871 	& 0.875 & \textit{\textbf{0.900}} 			& 0.881 \\
\hline
std dev 		& 0.223 	& 0.227 				& 0.231 			& 0.196 \\
\hline
\multicolumn{5}{c}{} \\
\hline
 & \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
& \param{ldh} & \param{xdh} & \param{xdx} & BERT \\
\hline
\hline
count 	& 120 			& 120 				& 120 					& 120 \\
\hline
mean & \textbf{0.350} 	& 0.349 			& 0.348 			& \textit{\textbf{0.509}} \\
\hline
median 	& 0.364 		& \textbf{0.374} 	& 0.331 			& \textit{\textbf{0.523}} \\
\hline
min 	& -0.147 		& -0.185 			& \textit{\textbf{-0.062}} & -0.090 \\
\hline
max 	& 0.892 		& 0.893 			& \textit{\textbf{0.898}} 		& 0.881 \\
\hline
std dev & 0.229 		& 0.236 			& 0.213 				& 0.177 \\
\hline
\end{tabular}
\caption{\label{tab:termrep-results} Comparing Spearman rank correlation scores where system configurations use different term representations: \param{ldh} (labeled dependencies), \param{xdh} (unlabeled dependencies), or \param{xdx} (dependents only; i.e., \textit{words}). Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} and \textit{BERT} column represents 120 different rankings (4 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. 
%%% 4/16/21 LK OK
}
\end{center}
\end{table}






\begin{table}[htb!]
\begin{center}
\setlength{\tabcolsep}{.6em}
\begin{tabular}{|l|l||l|l|l||l|l|l|}
\hline
 && \multicolumn{3}{c||}{NS model = 14} & \multicolumn{3}{c|}{NS model = 50} \\
\hline
	&	& \param{ldh} 	& \param{xdh} 	& \param{xdx} 	& \param{ldh} 	& \param{xdh} 	& \param{xdx} 		\\
\hline
\multirow{3}{*}{\begin{sideways}model TTR \end{sideways}} & model	& 120 & 120 & 120 	& 120 & 120 & 120	 \\
\cline{2-8}
& mean 		& 0.525 	& 0.513		& 0.279 			& 0.367		& 0.354 	& 0.157	 \\
\cline{2-8}
& median 	& 0.538 	& 0.525		& 0.286 			& 0.366		& 0.358 	& 0.157	 \\
\cline{2-8}
& min 		& 0.177 	& 0.177		& 0.136 			& 0.150		& 0.141 	& 0.063	 \\
\cline{2-8}
& max 		& 0.825 	& 0.818		& 0.437 			& 0.570		& 0.562 	& 0.228	 \\
\cline{2-8}
& std dev 	& 0.145 	& 0.144		& 0.068 			& 0.099		& 0.096 	& 0.037	 \\
\hline
\end{tabular}
\caption{\label{tab:termrep-model-stats}Comparing the model-level dependency type-to-token ratios (TTRs) for all NS models when dependencies are formatted in the three different \param{term representations}: \param{ldh} (labeled dependencies) and \param{xdh} (unlabeled dependencies) and \param{xdx} (dependents only), using NS models of either 14 or 50 random responses per item.
}
\end{center}
\end{table}









\section{Combined settings experiments}
\label{sec:exp-combos}
\subsection{Combined settings results}
\label{sec:combos-results}








%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{.6em}
%\begin{tabular}{|l|l||l|l|l||l|l|l|}
%\hline
% && \multicolumn{3}{c||}{NS model sample size = 14} & \multicolumn{3}{c|}{NS model sample size = 50} \\
%\hline
%	&	& Intrans 	& Trans 	& Ditrans 	& Intrans 	& Trans 	& Ditrans 		\\
%\hline
%\multirow{3}{*}{\begin{sideways}words / response \end{sideways}} & responses	& ct  & ct & ct & ct & ct & ct	 \\
%\cline{2-8}
%& mean 	& mean 	& mean		& mean 	& mean		& mean 	& mean	 \\
%\cline{2-8}
%& median 	& md 	& md		& md 	& md		& md 	& md	 \\
%\cline{2-8}
%& min 	& min 	& min		& min 	& min		& min 	& min	 \\
%\cline{2-8}
%& max 	& max 	& max		& max 	& max		& max 	& max	 \\
%\cline{2-8}
%& std dev & sd 	& sd		& sd 	& sd		& sd 	& sd	 \\
%\hline
%\hline
%\multirow{3}{*}{\begin{sideways}model TTR \end{sideways}} & model	& ct & ct & ct 	& ct & ct & ct	 \\
%\cline{2-8}
%& mean 	& mean 	& mean		& mean 	& mean		& mean 	& mean	 \\
%\cline{2-8}
%& median 	& med 	& med		& med 	& med		& med 	& med	 \\
%\cline{2-8}
%& min 	& min 	& min		& min 	& min		& min 	& min	 \\
%\cline{2-8}
%& max 	& max 	& max		& max 	& max		& max 	& max	 \\
%\cline{2-8}
%& std dev & sd 	& sd		& sd 	& sd		& sd 	& sd	 \\
%\hline
%\end{tabular}
%\caption{\label{tab:transitivity-model-stats} .
%}
%\end{center}
%\end{table}
