%\chapter{Method}
%\label{chap:method}
%\section{Introduction}
\chapter{Optimization}
\label{chap:optimization}
In this chapter, I discuss experiments intended to optimize my system for rating and ranking responses automatically, first introduced in Chapter~\ref{chap:pilot}. My earliest attempts were rule-based and relied on strict matching with a pre-established set of acceptable responses, described in Section~\ref{sec:pilot-study}. This found moderate success, leading to the improved approach described in Section~\ref{sec:2016work}, which is data-driven and relies on more flexible methods of comparison.
In Section~\ref{sec:current-method}, I give an overview of the updates to this approach used here to process the dataset described in Chapter~\ref{chap:data}, where the goal is now to approximate the benchmark rankings described in Section~\ref{sec:holistic-scoring}. In the sections beyond, I discuss the experiments isolating each parameter, followed by a look at overall trends.


\section{Updated method}
\label{sec:current-method}
The work discussed in Chapter~\ref{chap:pilot} relied on a shaky implementation of ``correctness'' or ``appropriateness'' for responses, and this needed improvement, first and foremost. Developing a better and more reliable annotation scheme was a key goal for me in expanding the work to the current dissertation, in order to give the work more meaning and context and make my corpus useful for a broad range of uses. The annotation planning discussed in Section~\ref{sec:scheme} was a direct result of the challenges of working with an inadequate annotation scheme. 

As discussed in Sections~\ref{sec:est-feat-weights} and~\ref{sec:holistic-scoring}, in the current corpus, annotations no longer encode for errors, but instead give a binary score for five different features, which are then weighted and combined to produce a score between zero and one. This means the evaluation can no longer take the form of mean average precision (MAP), but must instead compare response rankings, i.e., how well does the system rank responses in comparison to an ideal ranking based on manual annotations? Spearman rank correlation is used for these scores.\lk{expand on Spearman; what does it mean? Sp vs Pearson, etc}

Because the annotations and evaluation are much different in the current work, it does not exactly follow that findings from the previous work will hold true. However, I believe that the previous work has highlighted some of the system settings that are most likely to perform well, and I chose to focus my experiments on some of the best performing settings. For example, all of the current experiments rely on the tf-idf cosine (\param{TC}) approach, as this generally outperformed the others. As discussed in Section~\ref{sec:metrics}, the \param{TC} performance suffers for items where the non-native speaker (NNS) data is noisiest (with regard to spellings) and the native speaker (NS) data is relatively homogenous (particularly with regard to verb choice). Rather than continue experimenting with the underperforming approaches, I chose to address these issues directly instead. As discussed in Chapter~\ref{chap:data}, to address the spelling noise, I made sure that data collection participants had access to spelling correction while typing their responses. To address the uniformity of the NS responses, I surveyed a much larger group of participants and instructed each of them to provide two responses per picture description task (PDT) item.

The current work retains just three of the five term representations previously used (see Section~\ref{sec:response-rep}): \param{ldh}, \param{xdh}, and \param{xdx}. The \param{ldh} and \param{xdh} forms performed best with the older dataset. Moreover, as these represent labeled and unlabeled dependencies, their use in linguistics is well established. The \param{xdx} representation is kept here as a rough equivalent of a bag-of-words model, which is useful as a baseline for comparing the other two.

Finally, as \param{Brown} overwhelmingly outperformed the \param{WSJ} as a tf-idf reference corpus, the current experiments rely exclusively on the former. The larger PDT dataset follows a similar narrative style to that described in Section~\ref{sec:pilot-data}, so I am confident \param{Brown} is again the best option here.

Unless otherwise noted, all experiments throughout this chapter score and rank 70 NNS responses per item; this is the maximum number of NNS responses available across all PDT items. Where more than 70 responses are available, a random sample of 70 is used. The large increase in the number of PDT items and the size of the datasets means that running an exhaustive search for the best parameters among all combinations is not feasible. The experiments in Chapter~\ref{chap:pilot} used 30 different combinations of system settings (i.e., \textbf{configurations}) for each of 10 items. By comparison, the variables and parameters used in this chapter result in up to 48 different configurations (see Table~\ref{tab:all-params}) for each of the 30 items. To make sense of this large number of results, I have chosen to focus my optimization efforts on each parameter individually.


\begin{table*}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Transitivity & Targeting & Familiarity & Primacy & Term Norm. & Term Rep. \\
\hline
\hline
Intransitive & Targeted & Familar & Primary & Normalized & \param{ldh} \\
\hline
Transitive & Untargeted & Crowdsourced & Mixed & Non-norm. & \param{xdh} \\
\hline
Ditransitive & & & & & \param{xdx} \\
\hline
\end{tabular}
\caption{All parameters or variables and their settings; a system configuration combines one setting from each column.}
\label{tab:all-params}
\end{center}
\end{table*}

%\chapter{Optimization}
%\label{chap:optimization}
%In this chapter, I detail my research applying the methods discussed in Chapter~\ref{chap:pilot} to the much larger and more richly annotated dataset described in Chapters~\ref{chap:data} and \ref{chap:annotation}. The chapter primarily consists of a series of experiments focused on isolating and optimizing a number of parameters or variables in my picture description task (PDT) response analysis pipeline. 

To help contextualize the results of these experiments, I used a state-of-the-art language modeling tool to rank responses according to their similarity to the NS model. I discuss this tool and its use as a baseline in Section~\ref{sec:bert-baseline}.

The optimization experiments are organized here according to the sequence in which the variables are relevant in my process, which begins with data collection and ends with scoring and ranking non-native speaker (NNS) responses. Thus, in Section~\ref{sec:exp-transitivity}, I begin with the variable I refer to as \textit{transitivity}, which emerged during task design for the PDT described in Chapter~\ref{chap:data}; I look at the effects of applying my dependency-based tf-idf cosine pipeline to new item types, namely intransitives and ditransitives, and compare against performance on transitive items. Next, in Section~\ref{sec:exp-targeting}, I turn to experiments regarding \textit{targeting}, which refers to whether or not the PDT item subject was referenced in the prompt (as discussed in Section~\ref{sec:pdt}). In Section~\ref{sec:exp-familiarity}, I examine a variable I call \textit{familiarity}, which refers to whether the native speakers (NSs) contributing to the model are \textit{familiar} to me personally or are \textit{crowdsourced}. Another new variable follows in Section~\ref{sec:exp-primacy}; I call this \textit{primacy}, which refers to whether the NS model contains only first responses, or an equal number of first and second responses (also discussed in Section~\ref{sec:pdt}). I then evaluate the effects of a new innovation---normalizing the weight of each NS term in the model according to the length of the response in which it appeared, in Section~\ref{sec:exp-term-norm}; I call this variable \textit{term normalization}.\lk{Is this technically norming the terms or the responses?} For the final variable experiments, I return in Section~\ref{sec:exp-term-reps} to the best performing dependency \textit{term representations} from Section~\ref{sec:response-rep} to see how they perform with the current dataset.

After examining these variables individually, in Section~\ref{sec:exp-combos} I consider the hypothesis that particular configurations \lk{Check research Qs and sync this up} will perform better than others in particular conditions. This is a ``non-exhaustive,'' mostly future-looking set of experiments, where I report some promising trends.

%Finally, in Section~\ref{sec:exp-bert}, I present a set of experiments comparing the performance of my best system settings against BERT, a more sophisticated, state of the art language model capable of ranking NS responses according to their similarity to the collection of NNS responses. These results provide insights into the trade-offs between using my simpler, highly transparent tool and a more powerful yet highly opaque tool.

\section{Establishing a baseline}
\label{sec:bert-baseline}
The central task of my work---ranking NNS responses using a set of NS responses---is not a standard task with established metrics in any relevant field like natural language processing or language testing. Moreover, this work relies on a custom dataset which has not been widely adopted. These facts make it challenging to assess the performance of my ranking system and its various configurations or to compare this work against similar research. In order to give some frame of reference for this work, I chose to use a BERT, a widely adopted state-of-the-art language modeling tool \cite{BertDevlin2018}. My system scores each single (NNS) test response according to its similarity with the set of (NS) model responses. Measuring sentence similarity is one of BERT's most used functions, so for each scoring and ranking experiment in this chapter, I use BERT to generate corresponding output. The resulting Spearman scores are discussed throughout to help contextualize my system's performance.
\subsection{About BERT}
\label{sec:about-bert}
XYZ: BERT / SBERT

\subsection{Integrating BERT}

For each experiment reported throughout this chapter, my system and BERT are given access to the same NS responses as the basis for their similarity measures. Naturally, however, each is trained on or makes use of very different language resources. The linguistic ``intelligence'' of my system comes largely from the Stanford Parser and its pre-trained grammar model, as discussed in Section~\ref{sec:pilot-study} \cite{klein:manning:03}. The model is trained on the standard training sections of the Penn Treebank, which contain over one million words of English text, manually part-of-speech tagged and parsed, sourced from the Wall Street Journal and the Brown Corpus \cite{marcus-et-al:93}. My approach also uses the Brown Corpus for tf-idf, meaning a word frequency model extracted from the Brown Corpus also serves as a linguistic resource \cite{kucera:francis:67}. BERT, on the other hand, is trained on vastly larger amounts of unannotated text, from a much broader range of sources.

My implementation of BERT here varies slightly from the way I implemented my own system's similarity measuring approach throughout most of this chapter. In my system, each NS response in the model is processed and dumped into a single ''bag of dependencies,'' which is then used to generate a single similarity score (via tf-idf cosine). BERT operates directly on plain text sentences, and because the use of punctuation in the PDT responses is not consistent, concatenating all NS model responses in order to generate a single similarity score is not ideal. Instead, I use BERT to do a pairwise comparison between the NNS test response and each NS response in the model and then average these similarity scores.

I did explore an implementation of my system's similarity scoring that is more consistent with this individual treatment of model responses for BERT, and this is discussed in Section~\ref{sec:exp-term-norm}.


\section{Transitivity experiments}
\label{sec:exp-transitivity}
Here I compare the performance of my ranking system when applied to items that are canonically either intransitive, transitive, or ditransitive. Unlike the other variables throughout this chapter, transitivity is not a parameter setting. Individual PDT items are assumed to fit predominately only one of the three categories here. In other words, I cannot choose to process a given item with any of the three transitivity settings. Rather, the experiments in this section are intended to illustrate strengths, weaknesses and trends related to this variable.

\subsection{Transitivity results}
\label{sec:transitivity-results}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l|}
\hline
 & \multicolumn{6}{c|}{NS model sample size = 14} \\
\hline
 & \multicolumn{2}{c||}{Intransitives} & \multicolumn{2}{c||}{Transitives} & \multicolumn{2}{c|}{Ditransitives} \\
\hline
		& System 	& BERT 		& System 	& BERT 		& System 	& BERT 		\\
\hline
\hline
count 	& 120 		& 40 		& 120 		& 40 		& 120 		& 40		 \\
\hline
mean 	& 0.412 	& \textbf{0.508} 	& 0.342 	& \textit{\textbf{0.578}}		& 0.273 	& \textbf{0.409}	 \\
\hline
median 	& 0.416 	& \textbf{0.490} 	& 0.331 	& \textit{\textbf{0.579}}		& 0.278 	& \textbf{0.455}	 \\
\hline
min 	& \textbf{-0.076} 	& -0.210 	& \textit{\textbf{-0.030}} 	& -0.243	& -0.269 	& \textbf{-0.212}	 \\
\hline
max 	& \textit{\textbf{0.894}} 	& 0.884		& \textbf{0.778} 	& 0.773		& \textbf{0.709} 	& 0.701	 \\
\hline
std dev & 0.243 	& 0.171		& 0.204 	& 0.130		& 0.195 	& 0.248	 \\
\hline
\multicolumn{7}{c}{} \\
\hline
 & \multicolumn{6}{c|}{NS model sample size = 50} \\
\hline
 & \multicolumn{2}{c||}{Intransitives} & \multicolumn{2}{c||}{Transitives} & \multicolumn{2}{c|}{Ditransitives} \\
\hline
		& System 	& BERT 				& System 	& BERT 						& System 	& BERT \\
\hline
\hline
count 	& 120 		& 40 				& 120 		& 40 						& 120 		& 40 	\\
\hline
mean 	& 0.412 	& \textbf{0.517} 	& 0.342 	& \textit{\textbf{0.565}}	& 0.273 	& \textbf{0.445} \\
\hline
median 	& 0.416 	& \textbf{0.518}	& 0.331 	& \textit{\textbf{0.561}}	& 0.278 	& \textbf{0.463} \\
\hline
min 	& -0.076 	& \textbf{0.202}	& -0.030 	& \textit{\textbf{0.222}}	 & -0.269 	& \textbf{-0.090} \\
\hline
max & \textit{\textbf{0.894}} & 0.881	& \textbf{0.778} & 0.771	 			& \textbf{0.710} 	& 0.709 \\
\hline
std dev & 0.243 	& 0.173 			& 0.204 	& 0.135 					& 0.195 	& 0.199 \\
\hline
\end{tabular}
%% 2021-03-12: Checked all numbers; table is accurate.
\caption{\label{tab:transitivity-results} Comparing Spearman rank correlation scores for \param{intransitive}, \param{transitive} and \param{ditransitive} PDT items, using NS models of either 14 or 50 random responses per item. Each \textit{System} column represents 120 different rankings (12 system configurations $\times$ 10 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 40 rankings (4 system configurations $\times$ 10 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).}
\end{center}
\end{table}

\section{Targeting experiments}
\label{sec:exp-targeting}
Here we compare the performance of my ranking system when applied to targeted vs untargeted data.
\subsection{Targeting results}
\label{sec:targeting-results}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l||l|l|}
\hline
& \multicolumn{4}{c||}{NS model sample size = 14} & \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
 & \multicolumn{2}{c||}{Targeted} & \multicolumn{2}{c||}{Untargeted} & \multicolumn{2}{c||}{Targeted} & \multicolumn{2}{c|}{Untargeted} \\
\hline
	& System 		& BERT 		& System 	& BERT 		& System 	& BERT 		& System 	& BERT \\
\hline
count 	& 180 		& 60 		& 180 		& 60 		& 180 		& 60 		& 180 		& 60 \\
\hline
mean 	& 0.390 	& \textit{\textbf{0.546}} 	& 0.296 	& \textbf{0.451} 	& 0.385 	& \textit{\textbf{0.550}} 	& 0.300 	& \textbf{0.468}  \\
\hline
median 	& 0.384 	& \textit{\textbf{0.546}} 	& 0.307 	& \textbf{0.485} 	& 0.382 	& \textbf{\textbf{0.564}} 	& 0.313		& \textbf{0.495} \\
\hline
min 	& \textit{\textbf{-0.076}} 	& -0.179 	& -0.314 	& \textbf{-0.212} 	& \textbf{-0.048} 	& -0.090	& -0.269 	& \textit{\textbf{0.142}} \\
\hline
max 	& 0.859 	& \textbf{0.884} 	& \textit{\textbf{0.901}}		& 0.879 	& 0.872 	& \textbf{0.881}		& \textbf{0.894} 	& 0.880 \\
\hline
std dev 	& 0.238 	& 0.186 	& 0.216 	& 0.204 	& 0.232 	& 0.173		& 0.203 	& 0.172 \\
\hline
\end{tabular}
%% 2021-03-12: Checked all numbers; table is accurate.
\caption{\label{tab:targeting-results} Comparing Spearman rank correlation scores for \param{targeted} and \param{untargeted} versions of the PDT data, using NS models of either 14 or 50 random responses per item. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 60 rankings (2 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).}
\end{center}
\end{table}


\section{Familiarity experiments}
\label{sec:exp-familiarity}
Here we compare how well the system works when using different sources of NSs. 
\subsection{Familiarity results}
\label{sec:familiarity-results}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c||}{NS model sample size = 14} & \multicolumn{4}{c|}{NS model sample size = 50} \\
 \hline
 & \multicolumn{2}{c||}{Primary} & \multicolumn{2}{c||}{Mixed} & \multicolumn{2}{c||}{Primary} & \multicolumn{2}{c|}{Mixed} \\
\hline
		& System & BERT 	& System 	& BERT 						& System & BERT 	& System & BERT \\
\hline
\hline
count 	& 180 	& 60 		& 180 		& 60 						& 180 	& 60 		& 180 & 60 \\
\hline
mean 	& 0.349 & \textit{\textbf{0.512}} & 0.337 & \textbf{0.505} 	& 0.337 & 0.495 	& 0.347 & \textit{\textbf{0.502}} \\
\hline
median 	& 0.352 & \textit{\textbf{0.523}} & 0.342 & \textbf{0.520}  & 0.327 & \textit{\textbf{0.529}} 	& 0.361 & \textbf{0.516}  \\
\hline
min & -0.314 & \textbf{-0.212} & -0.199 & \textit{\textbf{-0.179}}  & -0.269 & \textbf{-0.090} 	& -0.147 & \textit{\textbf{-0.049}}  \\
\hline
max & \textit{\textbf{0.901}} & 0.884 & \textbf{0.895} & 0.878 		& \textbf{0.890} & 0.880 	& \textit{\textbf{0.894}} & 0.881 \\
\hline
std dev & 0.235 & 0.212 	& 0.229 	& 0.189 					& 0.218 & 0.187 	& 0.227 & 0.168 \\
\hline
\end{tabular}
%% 2021-03-12: Checked all numbers; table is accurate.
\caption{\label{tab:primacy-results} Comparing Spearman rank correlation scores where \param{primary} models contain only first responses from NSs and \param{mixed} models contain an equal mix of first and second responses from NSs. Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 60 rankings (2 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).}
\end{center}
\end{table}


\section{Primacy experiments}
\label{sec:exp-primacy}
Here we compare how well the system works when using NSs' first responses vs a mix of first and second responses.
\subsection{Primacy results}
\label{sec:primacy-results}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
 \hline
 & \multicolumn{2}{c||}{\param{Familiar} NS} & \multicolumn{2}{c|}{\param{Crowd} NS} \\
\hline
		& System & BERT 	& System 	& BERT 						\\
\hline
\hline
count 	& 180 	& 60 		& 180 		& 60 						\\
\hline
mean 	& 0.355 & \textbf{0.503} & 0.337 & \textit{\textbf{0.505}} 	\\
\hline
median 	& 0.341 & \textit{\textbf{0.529}} & 0.342 & \textbf{0.520}   \\
\hline
min & \textbf{-0.208} & -0.254 & -0.199 & \textit{\textbf{-0.179}}  \\
\hline
max & \textit{\textbf{0.900}} & 0.881 & \textbf{0.895} & 0.878 		 \\
\hline
std dev & 0.216 & 0.183 	& 0.229 	& 0.189 					\\
\hline
\end{tabular}
\caption{\label{tab:familiarity-results} Comparing Spearman rank correlation scores where \param{Familiar} models contain only responses from NSs \textit{familiar} to the researcher and \param{Crowd} models contain only responses from crowdsourced NSs. ... Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} column represents 180 different rankings (6 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking. Each \textit{BERT} column represents 60 rankings (2 system configurations $\times$ 30 items; BERT operates on plain text, so the \param{term representation} parameter does not apply).}
\end{center}
\end{table}


\section{Term normalization experiments}
\label{sec:exp-term-norm}

In my pipeline, the NS model for each PDT item is comprised of some number of NS responses.  The length of these responses can vary; some valid responses contain only one or two words\footnote{Participants were instructed to provide complete sentences, but incomplete sentences were still judged valid where appropriate; see Chapter~\ref{chap:data} and the Annotation Guide in Appendix~\ref{appendix:annotation_guide}.}, while the longest top out at around 15 words. These longer, well-formed responses are relatively uncommon, but understanding their impact on a model is an important step in optimizing my response rating process. So far, I have treated each NS model as a flat ``bag of terms'' in which each term (cf. \textit{dependency}; see Section~\ref{sec:response-rep}) contributes equally, meaning longer responses carry more weight in the model. This has the potential to introduce noise. 

My hypothesis is that system performance should improve by using NS models where each term token is re-weighted by $1/\textit{n}$ before it is added to the model, where \textit{n} is the number of term tokens in the response containing the term token. In other words, I believe dependencies should be re-weighted to ensure that every NS \textit{response}---not \textit{term}---contributes equal weight to the model. The rationale here is simple. Every response used in the NS model is assumed to contain the information that is crucial for satisfying the PDT prompt, and the number of terms conveying this information is roughly equivalent from one response to another. Thus, as the number of terms in a response increases (above some minimum number), the likelihood that any given term in that response is crucial decreases.


\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Response A & Response B & Norm. wt. & Non-norm. wt.\\
\hline
\multirow{2}{*}{The girl is singing} & The girl in the cute purple & & \\
& dress is singing a song & & \\
\hline
\hline
det(the, girl) & det(the, girl) & 0.175 & 0.143 \\
\hline
nsubj(girl, sing) & nsubj(girl, sing) & 0.175 & 0.143 \\
\hline
%& erased(in, ERASED) & 0.050 & 0.071 \\
%& \textit{erased(in, ERASED)} & 0.000 & 0.000 \\
& \textit{erased(in, ERASED)} & --- & --- \\
\hline
& det(the, dress) & 0.050 & 0.071 \\
\hline
& amod(cute, dress) & 0.050 & 0.071 \\
\hline
& amod(purple, dress) & 0.050 & 0.071 \\
\hline
%& prep\_in(dress, girl) & 0.050 & 0.071 \\
& prep\_in(dress, girl) & 0.050 & 0.071 \\
\hline
aux(be, sing) & aux(be, sing) & 0.175 & 0.143 \\
\hline
root(sing, ROOT) & root(sing, ROOT) & 0.175 & 0.143 \\
\hline
& det(a, song) & 0.050 & 0.071 \\
\hline
& dobj(song, sing) & 0.050 & 0.071 \\
\hline
\hline
4 & 10 & 1.0 & 1.0 \\
\hline
\end{tabular}
\caption{\label{tab:normalize-responses-deps} A ``toy'' model consisting of lemmatized syntactic dependencies from only two NS responses, each with perfect annotation scores. (Note that the version of Stanford typed dependencies used in this work collapses dependencies containing prepositions and incorporates prepositions in a label, resulting in the ``prep\_in'' and ``erased'' dependencies above. See Section~\ref{sec:rule-method} for more on the parsing and lemmatization.)}
\end{center}
\end{table}

This is illustrated by the responses in Table~\ref{tab:normalize-responses-deps}. If we take these two responses to constitute one NS model, Response A contributes four dependencies, each of which is necessary to satisfy the five annotated features and contributes meaningfully to the model. Response B, however, contributes 10 dependencies, some of which, like \textit{amod(purple, dress)}, add non-critical detail. In a non-normalized setting, this dependency constitutes one out of a total 14 dependencies in the NS model, approximately 0.071. In a normalized setting, however, this dependency appears as zero of four (0.0) dependencies in Response A, and one of 10 (0.1) in Response B, making it 0.05 of the overall model (0.1 divided by two responses). This should have the effect of making extraneous information in the model less impactful on response ratings.

Taking this example further, consider the dependency \textit{nsubj(girl, sing)}, also from Table~\ref{tab:normalize-responses-deps}. In the non-normalized setting, this dependency appears as two out of a total 14 dependencies, or 0.143 of the NS model. In the normalized setting, the dependency appears as one out of four (0.25) dependencies in Response A, and one out of 10 (0.1) dependencies in Response B, equating to 0.175 of the NS model (0.35 divided by two responses). Because this dependency is critical, raising its weight from 0.143 to 0.175 should have a positive impact on system performance; NNS responses containing the dependency should rise in the rankings relative to those without it. 

To test my hypothesis, I compared the performance of normalized and non-normalized models. This followed my standard design of ranking NNS responses by their system scores, then comparing this ranking with the weighted annotation ranking using Spearman rank correlation. I isolate this variable as before: for all system configurations, I generate a Spearman score. Then I obtain an average score for all normalized configurations and an average score for all non-normalized configurations and compare the two.

Such normalization can be sensitive to the effects of size\lk{XYZ: Citation?}, so I conducted the  experiments using models of my standard sample size of 50 NS responses, and again using a sample size of 15.

%This experiment was conducted with the largest XGS (\textit{all NS responses}) and the smallest XGS (\textit{all familiar NS responses}) to ensure that the effect of normalization is consistent. This process was repeated for the same data sets without the normalization; the Spearman scores for the normalized and non-normalized GSs were compared.

%
%To account for this and examine whether or not it poses a problem, I conducted experiments in which each \textit{response}, not each \textit{dependency}, contributes equally to the XGS. This meant simply normalizing for the length of the response by applying a weight to each dependency token as it was added to the XGS, where the weight is equal to 1 divided by the total number of dependencies in the response. The responses were then ranked by these scores, and Spearman's rank correlation coefficient (``Spearman'') was used to compare the ranking against the ``true'' GS (TGS), which is the result of ranking the responses using the weighted annotations (discussed in Chapter~\ref{chap:annotation}). This experiment was conducted with the largest XGS (\textit{all NS responses}) and the smallest XGS (\textit{all familiar NS responses}) to ensure that the effect of normalization is consistent. \lk{Get avg \# responses for All NS vs Familiar NS} This process was repeated for the same data sets without the normalization; the Spearman scores for the normalized and non-normalized GSs were compared.

\subsection{Term normalization results}
\label{sec:term-norm-results}

The results of this experiment are shown in Table~\ref{tab:normalize-responses-spearman}. The comparisons show very little difference in the correlations, with only one of the 12 experiments showing a slightly stronger correlation for the normalized model. The simplest explanation for this is the fact that longer responses with extraneous information are relatively uncommon, so we can expect this normalization to have a low impact.
%\lk{XYZ. For different test responses, find some examples where the non-normalized config is rated/ranked higher than in the normalized config?}
\lk{XYZ. Add examples}

Because the non-normalized GSs outperform their normalized counterparts, and for the sake of simplicity, this parameter was not adopted in the other experiments discussed in this chapter; only non-normalized configurations were used in all other experiments.


\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{2}{c||}{NS model (n=50)} & \multicolumn{2}{c|}{NS model (n=15)} \\
\hline
% Dep & Norm & Non-norm & Norm & Non-norm \\
& Norm & Non-norm & Norm & Non-norm \\
\hline
%\hline
%ldh & -0.474 & \textbf{-0.477} & -0.462 & \textbf{-0.471} \\
%\hline
%xdh & -0.476 & \textbf{-0.478} & -0.470 & \textbf{-0.474} \\
%\hline
%xdx & \textbf{-0.441} & -0.438 & -0.428 & \textbf{-0.431} \\
\hline
Spearman avg & -0.463 & \textbf{-0.465} & -0.449 & \textbf{-0.456} \\
\hline
\end{tabular}
%\caption{\label{tab:normalize-responses-spearman} Spearman correlation coefficient using gold standards (GSs) that are normalized for length (number of dependencies) and GSs that are non-normalized. This was conducted for various dependency representations: \textit{label, dependent, head (ldh)}; \textit{dependent, head} (xdh); \textit{dependent} only (xdx). The p-values are not indicated but range between 0.034 and 0.068 for all cases, indicating that the correlations are very unlikely to be coincidental.}
\caption{\label{tab:normalize-responses-spearman} Spearman correlation coefficient averaged for NS models with term scores normalized for response length (number of dependencies) and for NS models with non-normalized term scores. The p-values are not indicated but range between 0.034 and 0.068 for all cases, indicating that the correlations are very unlikely to be coincidental.}
\end{center}
\end{table}


\section{Term representation experiments}
\label{sec:exp-term-reps}
The current system allows for different \textit{term representations}, which are variations on syntactic dependencies. A dependency consists of a \textit{head}, \textit{dependent}, and \textit{label}. In past work, I experimented with omitting one or more of these elements to allow for less restrictive matching (see Table~\ref{tab:dist-ranked-parameters}). In the current dissertation, I compare the system performance using the three formats: \textit{label-dependent-head} (\param{ldh}), \textit{dependent-head} only (\param{xdh}), and \textit{dependent} only (\param{xdx}). In other words, my system uses a ``bag of terms'' approach, where the bags contain either labeled dependencies (\param{ldh}), unlabeled dependencies (\param{xdh}) or words (\param{xdx}). The labeled and unlabeled dependencies were the top performers in my previous work, and the \param{xdx} format is included as a kind of baseline showing a bag of words approach.

\subsection{Term representation results}
\label{sec:term-norm-results}

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l|l||l|}
\hline
 & \multicolumn{4}{c|}{NS model sample size = 14} \\
\hline
& \param{ldh} & \param{xdh} & \param{xdx} & BERT \\
\hline
\hline
count & 120 & 120 & 120 & 120 \\
\hline
mean & 0.342 & 0.346 & 0.341 & 0.498 \\
\hline
median & 0.347 & 0.367 & 0.330 & 0.521 \\
\hline
min & -0.314 & -0.199 & -0.076 & -0.212 \\
\hline
max & 0.895 & 0.901 & 0.900 & 0.884 \\
\hline
std dev & 0.243 & 0.238 & 0.216 & 0.200 \\
\hline
\multicolumn{5}{c}{} \\
\hline
 & \multicolumn{4}{c|}{NS model sample size = 50} \\
\hline
& \param{ldh} & \param{xdh} & \param{xdx} & BERT \\
\hline
\hline
count & 120 & 120 & 120 & 120 \\
\hline
mean & 0.350 & 0.342 & 0.335 & 0.509 \\
\hline
median & 0.364 & 0.368 & 0.322 & 0.523 \\
\hline
min & -0.147 & -0.269 & -0.075 & -0.090 \\
\hline
max & 0.892 & 0.891 & 0.894 & 0.881 \\
\hline
std dev & 0.229 & 0.235 & 0.202 & 0.177 \\
\hline
\end{tabular}
%% 2021-03-12: Checked all numbers; table is accurate.
\caption{\label{tab:termrep-results} Comparing Spearman rank correlation scores where system configurations use different term representations: \param{ldh} (labeled dependencies), \param{xdh} (unlabeled dependencies), or \param{xdx} (dependents only; i.e., \textit{words}). Results are shown using NS models of 14 responses and 50 responses. Each \textit{System} and \textit{BERT} column represents 120 different rankings (4 system configurations $\times$ 30 items) of 70 NNS responses, where each ranking receives a Spearman score via comparison with the weighted annotation ranking.}
\end{center}
\end{table}


\section{Combined settings experiments}
\label{sec:exp-combos}
\subsection{Combined settings results}
\label{sec:combos-results}

