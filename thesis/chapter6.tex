\chapter{Experiments}
\label{chap:experiments}
This chapter will discuss experiments in tuning the TC encoder pipeline.

\section{Normalizing for response length}
\label{section:experiment-normalizing-length}

Each experimental gold standard (XGS) is comprised of some number of native speaker (NS) responses to the picture description task (PDT). Even among native speaker (NS) data, response lengths can vary; some valid responses contain only one or two words, while the longest top out at around 15 words. These longer, well-formed responses are relatively uncommon, but understanding their impact on an XGS is an important step in optimizing the rating process. So far, the approach to each XGS has been to treat it as a ``bag of dependencies'' in which each dependency contributes equally, meaning longer responses can carry a greater weight in the GS. This has the potential to introduce noise. Table~\ref{tab:normalize-responses-deps} illustrates this. These responses are both included in a toy XGS consisting of NS responses with ``perfect'' feature annotations. The first response contributes four dependencies to the GS, each of which is necessary to fulfill the feature annotations and contributes meaningfully to the GS. The second response, however, contributes 10 dependencies, some of which, like \textit{amod(purple, dress)}, add non-critical detail, and these details could receive too much weight.

%%\begin{table}[htb!]
%%\begin{center}
%%\begin{tabular}{|l|c|}
%%\hline
%% Response & Dependencies \\
%%%\hline
%%%A boy is dancing & 4 \\
%%%\hline
%%%A boy in a blue shirt and gray pants is dancing & 11 \\
%%\hline
%%The girl is singing & 4 \\
%%\hline
%%The girl in the purple dress is singing a song & 10 \\
%%\hline
%%\end{tabular}
%%\caption{\label{tab:normalize-responses-ex} Example responses and dependency counts. Both responses have ``perfect'' annotations and are included in the various XGSs.}
%%\end{center}
%%\end{table}

0.464012874
0.463905084

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Response A & Response B & Norm. wt. & Non-norm. wt.\\
\hline
%%A: The boy carries the bag. & \multirow{2}{*}{B} & \multirow{2}{*}{B} & \multirow{2}{*}{yes} \\
%%\cline{1-1}
%%B: The boy is carrying groceries. & & & \\
\multirow{2}{*}{The girl is singing} & The girl in the purple dress & & \\
& is singing a song & & \\
\hline
\hline
det(the, girl) & det(the, girl) & 0.175 & 0.143 \\
\hline
nsubj(girl, sing) & nsubj(girl, sing) & 0.175 & 0.143 \\
\hline
& erased(in, WORDERASED) & 0.050 & 0.071 \\
\hline
& det(the, dress) & 0.050 & 0.071 \\
\hline
& amod(purple, dress) & 0.050 & 0.071 \\
\hline
& prep\_in(dress, girl) & 0.050 & 0.071 \\
\hline
aux(be, sing) & aux(be, sing) & 0.175 & 0.143 \\
\hline
root(sing, VROOT) & root(sing, VROOT) & 0.175 & 0.143 \\
\hline
& det(a, song) & 0.050 & 0.071 \\
\hline
& dobj(song, sing) & 0.050 & 0.071 \\
\hline
\hline
4 & 10 & 1.0 & 1.0 \\
\hline
\end{tabular}
\caption{\label{tab:normalize-responses-deps} A ``toy'' XGS consisting of lemmatized syntactic dependencies from only two responses, each with perfect annotation scores. (See Chapter~\ref{chap:method} for more on the parsing and lemmatization.)}
\end{center}
\end{table}
\lk{is worderased actually processed through tf-idf?} 

To illustrate with the examples in Table~\ref{tab:normalize-responses-deps}, consider \textit{det(the, girl)}. In the non-normalized setting, this dependency appears as two out of a total 14 dependencies, or 0.143 of the total XGS. In the normalized setting, the dependency appears as one out of four (0.25) dependencies in Response A, and one out of 10 (0.1) dependencies in Response B, equating to 0.175 of the total XGS (0.35 divided by two responses).

\lk{combine these Ps}

To account for this and examine whether or not it poses a problem, I conducted experiments in which each \textit{response}, not each \textit{dependency}, contributes equally to the XGS. This meant simply normalizing for the length of the response by applying a weight to each dependency token as it was added to the XGS, where the weight is equal to 1 divided by the total number of dependencies in the response. The responses were then ranked by these scores, and Spearman's rank correlation coefficient (``Spearman'') was used to compare the ranking against the ``true'' GS (TGS), which is the result of ranking the responses using the weighted annotations (discussed in Chapter~\ref{chap:annotation}). This experiment was conducted with the largest XGS (\textit{all NS responses}) and the smallest XGS (\textit{all familiar NS responses}) to ensure that the effect of normalization is consistent. \lk{Get avg \# responses for All NS vs Familiar NS} This process was repeated for the same data sets without the normalization; the Spearman scores for the normalized and non-normalized GSs were compared.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{|l||l|l||l|l|}
\hline
 & \multicolumn{2}{|c||}{\textit{All NS GS}} & \multicolumn{2}{|c|}{\textit{Familiar NS GS}} \\
\hline
 Dep & Norm & Non-norm & Norm & Non-norm \\
\hline
\hline
ldh & -0.474 & \textbf{-0.477} & -0.462 & \textbf{-0.471} \\
\hline
xdh & -0.476 & \textbf{-0.478} & -0.470 & \textbf{-0.474} \\
\hline
xdx & \textbf{-0.441} & -0.438 & -0.428 & \textbf{-0.431} \\
\hline
Avg & -0.463 & \textbf{-0.465} & -0.454 & \textbf{-0.458} \\
\hline
\end{tabular}
\caption{\label{tab:normalize-responses-spearman} Spearman correlation coefficient using gold standards (GSs) that are normalized for length (number of dependencies) and GSs that are non-normalized. This was conducted for various dependency representations: \textit{label, dependent, head (ldh)}; \textit{dependent, head} (xdh); \textit{dependent} only (xdx). The p-values are not indicated but range between 0.034 and 0.068 for all cases, indicating that the correlations are very unlikely to be coincidental.}
\end{center}
\end{table}

The results of this experiment are shown in Table~\ref{tab:normalize-responses-spearman}. The comparisons show very little difference in the correlations, with only one of the 12 experiments showing a slightly stronger correlation for the normalized GS. The simplest explanation for this is the fact that longer responses with extraneous information are relatively uncommon, so we can expect this normalization to have a low impact. \lk{But what else can I say here}

NTS: For different test responses, find some examples where the non-normalized model is rated/ranked higher than in the normalized model.

Because the non-normalized GSs outperform their normalized counterparts, and because normalization adds complexity to the process, this step was not used in the remaining optimization experiments.

\section{Dependency formats}
\label{section:experiment-dependency-formats}
One parameter I vary in my pipeline is the format with which I represent each dependency. A dependency consists of a \textit{head}, \textit{dependent}, and \textit{label}. In past work, I experimented with omitting one or more of these elements to allow for less restrictive matching; the results are shown in Table~\ref{tab:dist-ranked-parameters}. In the current dissertation, I compare the system performance using the three formats: \textit{label-dependent-head} (ldh), \textit{dependent-head} only (xdh), and \textit{dependent} only (xdx). In other words, in my ``bag of terms'' approach, the bags contain either labeled dependencies (ldh), unlabeled dependencies (xdh) or words (xdx). The labeled and unlabeled dependencies were the top performers in my previous work, and the xdx format is included as a kind of baseline showing a bag of words approach.

\subsection{Results}
\label{subsection:dependency-formats-results}
In all cases, the \texttt{ldh} format results in a higher Spearman correlation than \texttt{xdh}. As expected, \texttt{xdx} performs significantly worse than either \texttt{ldh} or \texttt{xdh}.

\section{Targeted vs Untargeted}
\label{section:experiment-targeted}
Here we compare the performance of my ranking system when applied to targeted vs untargeted data.
\subsection{Results}
\label{subsection:targeted-results}

\section{Intransitive vs Transitive vs Ditransitive}
\label{section:experiment-transitive}
Here we compare the performance of my ranking system when applied to items that are (predominantly): intransitive, transitive, ditransitive.
\subsection{Results}
\label{subsection:transitive-results}

\section{Familiar vs Crowdsourced response XGS}
\label{section:experiment-crowdsource}
Here we compare how well the system works when using different sources of NSs. (Crowdsourced informants drastically outnumbered Familiar, so this will require some cross-validation -- which in turn requires tf-idf for the XGS in each cross-val cycle; i.e., time intensive computation).
\subsection{Results}
\label{subsection:crowdsource-results}

\section{First responses XGS vs First and second responses XGS}
\label{section:experiment-first-responses}
Here we compare how well the system works when using NSs' first responses vs a mix of first and second responses. (The latter is nearly double the former, so this will require some cross-validation -- which in turn requires tf-idf for the XGS in each cross-val cycle; i.e., time intensive computation).
\subsection{Results}
\label{subsection:first-responses-results}

\section{XGS filtered by annotation}
\label{section:experiment-filtered}
Here we examine how performance changes when we filter the XGS to include only ``perfect'' annotation responses or ``core event = yes'' responses. (Response counts will vary depending on exactly how I do this, so this will require some cross-validation -- which in turn requires tf-idf for the XGS in each cross-val cycle; i.e., time intensive computation).
\subsection{Results}
\label{subsection:filtered-results}
