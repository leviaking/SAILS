\chapter{Method}
\label{chap:method}
\section{Introduction}
In this chapter, I explain my system for rating and ranking responses automatically, where the goal is to approximate the benchmark rankings described in Chapter~\ref{chap:annotation}. 
The data-driven method used to analyze picture description task (PDT) responses throughout this dissertation represents an evolution from my own previous rule-based methods. In this chapter, I summarize my earlier work and the lessons I learned there, then lay out the current approach.

In short, my first approach was heavily rule-based and relied on strict matching with a pre-established set of acceptable responses. This found moderate success, leading to the improved current approach, which is data-driven and relies on more flexible methods of comparison.
%
%%%%% BEGIN material from BEA 2016 %%%%
%\section{Recent work: Generalized, similarity-based approaches}
%\label{sec:recent-work}
%%%%% 2020/05/21. Resume here. %%%% 
%
%In subsequent work \citep{king:dickinson:16}, I began looking for a ``sweet spot'' of
%semantic analysis \citep[cf.][]{bailey:meurers:08} for image-based learner productions. I applied new methods to the same dataset, and the current dissertation research applies a refined version of these methods to the new dataset discussed in Chapters~\ref{chap:data} and~\ref{chap:annotation}.
%In particular, using available NLP tools, I moved away from discrete representations of correctness in the form of a gold standard set of semantic triples to a more continuous notion of correctness using a model comprised of smaller, overlapping pieces of information (section~\ref{sec:ranking}). This obviates the need for a rule-based extraction of semantic triples, which must be customized for a limited range of expected sentence types. It also allows for graded scoring of results, meaning that a response is not outright rejected because only one argument of a triple is not found. On the other hand, it means that the system does not provide a discrete ``thumbs up'' or ``thumbs down'' decision for each response, which means that downstream tasks like assessment, providing feedback or effecting game outcomes would require more careful consideration of what to do with the system output.
%
%I should note, in this context, that I am discussing semantic
%analysis given a reliable set of NS sentences.  Image processing
%tasks often rely on breaking images into semantic primitives
%\citep[see, e.g.,][and references therein]{ortiz:wolff:lapata:15}, but
%for NNS data, I want to ensure that I can account not just for
%correct semantics (the \emph{what} of a picture), but natural
%expressions of the semantics (the \emph{how} of expressing the
%content).  In other words, the goal is to reason about meaning based on
%specific linguistic forms.
%
%A second issue regarding content analysis, beyond correctness, stems
%from using an incomplete gold standard. The productive nature of language means that a sentence can be expressed in countless ways, and thus a gold standard can never really be ``complete.'' Examining the degree of this variability both for NSs and NNSs is necessary to determine whether a crowd-sourced gold standard can account for a sizable portion of test responses. Additionally, it can offer insights into theoretical research on variability in learner language (cf. \citet{ellis1987variability}, \citet{kanno1998consistency}). With regard to the current work, analyzing variability can also help determine the most effective parameters for an NLP system for image based responses. By applying a set of new approaches to my older data, I was able to select the best performing approaches for refinement and further study with the larger, current dataset.
% 
%
%%That is, different types of image content might require
%%different mechanisms for processing.  Additionally, knowing how
%%different pictures elicit different kinds of content can provide
%%feedback on appropriate types of new data to collect.
%%We approach this issue by clustering responses in various ways
%%(section~\ref{sec:clustering}) and seeing how the clusters connect to
%%system parameters.
%%
%%For both the experiments involving the accuracy of different system
%%parameters (section~\ref{sec:ranking}) and the clustering of different
%%responses (section~\ref{sec:clustering}), we present results within
%%those sections that show the promise of moving to abstract representations, but in
%%different ways for different kinds of data.
%%
%%We build directly from \citet{king:dickinson:13,king:dickinson:14},
%%where the method to obtain a semantic form from a NNS production is:
%%1) obtain a syntactic dependency representation from the off-the-shelf
%%Stanford Parser \citep{demarneffe:ea:06, klein:manning:03}, and 2)
%%obtain a semantic form from the parse, via a small set of hand-written
%%rules.  It is this method we attempt to generalize
%%(section~\ref{sec:ranking}).
%
%%\section{Data Collection}
%%\label{sec:data}
%%
%%Because our approach requires both NS and NNS responses and
%%necessitates constraining both the form and content of responses, we
%%previously assembled a small corpus of NS and NNS responses to a PDT
%%\citep{king:dickinson:13}.  Research in SLA often relies on the
%%ability of task design to induce particular linguistic behavior
%%\citep{skehan1998assessing}, and the PDT should induce context-focused
%%communicative behavior.  Moreover, the use of the PDT as a reliable
%%language research tool is well-established in areas of study ranging
%%from SLA to Alzheimer's disease \citep{ellis2000task,
%%forbes2005detecting}.
%
%%
%%The PDT consists of 10 items (8 line drawings and 2 photographs\footnote{We have not observed substantial differences between responses for the drawings and the photographs.}) intended to elicit a single sentence
%%each; an example is given in Figure~\ref{fig:example-picture}. Participants
%%were asked to view the image and describe the action in past or present tense.
%%The data consist of responses from 53 informants (14 NSs, 39 NNSs),
%%for a total of 530 sentences, with the NNSs being intermediate and
%%upper-level adult English learners in an intensive English as a Second
%%Language program.  The distribution of first languages (L1s) is: 14
%%English, 16 Arabic, 7 Chinese, 2 Japanese, 4 Korean, 1 Kurdish, 1
%%Polish, 2 Portuguese, and 6 Spanish.
%%
%%Responses were typed by the participants themselves, with spell checking disabled in some cases.  Even among the NNSs that used spell checking, a number of spelling errors resulted in real words. To address this, we use a spelling correction tool to obtain candidate spellings for each word, prune the candidates using word lists from the NS responses, recombine candidate spellings into candidate sentences, and evaluate these with a trigram language model (LM) to select the most likely intended response \citep{king:dickinson:14}.
%%
%%Once the responses had been collected, the NNS responses were
%%annotated for correctness, with respect to the content of the picture.
%%The lead author marked spelling and meaning errors which prevent a
%%complete mapping to correct information
%%\citep[see][]{king:dickinson:13}.  On the one hand, minor misspellings
%%are counted as incorrect (e.g., \textit{The artiest is drawing a
%%\textbf{portret}}), while, on the other hand, the annotation does
%%not require distinguishing between between spelling and meaning
%%errors.  In the future, we plan on fine-tuning the annotation
%%criteria.
%
%\subsection{Generalizing the methods}
%\label{sec:ranking}
%
%The previous work assumed that the assessment of NNS responses
%involves determining whether a NS gold standard set contains the same
%semantic triple that the NNS produced, i.e., whether a triple
%is \textit{covered} or \textit{non-covered}.  In such a situation the
%gold standard need only be comprised of \textit{types} (not \textit{tokens}) of semantic triples. Here the gold standard is comprised of the small set of NS responses---only 14. This means that exact matching is going to miss many cases,
%and indeed as discussed in Section~\ref{sec:rule-results}, coverage was only 51\%. Even with a much larger gold standard, we can expect responses to follow Zipf's law; a sample of language data will always be incomplete because it will not contain all ``long tail,'' low-frequency phenomena.
%
%Additionally, relying on matching of triples limits the utility of the method to specific semantic constraints, namely transitive sentences. By dropping the exact matching approach and instead comparing the frequencies of elements in the NNS response with those of the gold standard, I moved into a gradable, or ranking, approach to the analysis. For this reason, I shift terminology here from NS \textit{\textbf{gold standard}} to NS \textit{\textbf{model}}. A gold standard is roughly akin to an answer key, which was appropriate for my strict triple-matching approach. A model is typically a richer data structure containing statistics from observations of known correct data. This distinction and its relevance will be more apparent with the discussion of response representations in Section~\ref{sec:response-rep}.
%
%My goal is to emphasize the degree to which a NNS response conveys the same
%meaning as the set of NS responses, necessitating an approach which can automatically
%determine the importance of a piece of information among the set of NS responses.  This required two major decisions: 1) how to \textbf{represent} each response as a set of sub-elements, and 2) how exactly to \textbf{score} these sub-elements via comparison with the NS data. In Section~\ref{sec:response-rep}, I detail how I represented the information and in Section~\ref{sec:scoring}, I discuss comparing NNS information to NS information, which allowed me to rank responses from most to least similar to the NS model. In Section~\ref{sec:parameters}, I outline the system parameters that I combined to generate scores, and in Section~\ref{sec:metrics} I present and interpret the results of the various settings.
%%to rank responses from least to most similar to the NS gold standard.
%%\footnote{Although rankings often go from highest to lowest, I prioritize identifying problematic cases, so I rank accordingly.}
%%I also discuss the handling of various other system parameters (section~\ref{sec:parameters}).
%
%%This work used the same 10 item PDT dataset described in section~\ref{sec:first-approaches}. Another example is shown in Figure~\ref{fig:example-picture2}.
%%
%%\begin{figure}
%%\begin{center}
%%\begin{tabular}{|c|}
%%\hline
%%\includegraphics[width=0.7\columnwidth]{figures/exampleprompt2.jpg}\\
%%\hline
%%\textbf{Response (L1)} \\
%%\hline
%%The man killing the beard. (Arabic)\\
%%\hline
%%A man is shutting a bird. (Chinese) \\
%%\hline
%%A man is shooting a bird. (English) \\
%%\hline
%%The man shouted the bird. (Spanish)\\
%%\hline
%%\end{tabular}
%%\end{center}
%%\caption{Example item and responses}
%%\label{fig:example-picture2}
%%\end{figure}
%%
%\subsection{Representing responses}
%\label{sec:response-rep}
%
%%To overcome the limitations of an incomplete GS,
%I first represented each NNS response as a list of dependencies taken directly from the parse. 
%%\citep{demarneffe:ea:06}, the terms referring to
%%%being either 
%%individual dependencies (i.e., relations between words).
%%or individual words. 
%This eliminates the complications of extracting semantic triples from
%dependency parses, which only handled a very restricted set of
%sentence patterns and resulted in errors in 7--8\% of cases, as discussed in Section~\ref{sec:rule-results}.
%Operating directly on individual dependencies from the overall tree also means the system can allow for ``partial credit;'' it distributes the matching over smaller,
%overlapping pieces of information rather than a single, highly specific triple. The words within the dependencies were lemmatized using the pre-trained Stanford CoreNLP lemmatizer \citep{stanford-corenlp-2014}. Lemmatizing was used here to minimize data sparsity. For example, the main verb in the forms \textit{kicks}, \textit{kicked}, \textit{has kicked} and \textit{is kicking} was reduced to \textit{kick} in all cases, increasing the likelihood of finding matches.
%
%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|C{7em}||C{6em}||C{5.5em}||C{5.5em}||C{5em}|}
%\hline
%\multicolumn{5}{|c|}{The boy is kicking the ball.} \\
%\hline
%\hline
%\textbf{ldh} (labeled) & \textbf{xdh} (unlab.) & \textbf{lxh} & \textbf{ldx} & \textbf{xdx} (word)\\
%\hline
%\hline
%det(the,boy) & \textit{x}(the,boy) & det(\textit{x},boy) & det(the,\textit{x}) & \textit{x}(the,\textit{x}) \\
%\hline
%nsubj(boy,kick) & \textit{x}(boy,kick) & nsubj(\textit{x},kick) & nsubj(boy,\textit{x}) &  \textit{x}(boy,\textit{x}) \\
%\hline
%aux(is,kick) & \textit{x}(is,kick) & aux(\textit{x},kick) & aux(is,\textit{x}) & \textit{x}(is,\textit{x}) \\
%\hline
%root(kick,root) & \textit{x}(kick,root) & root(\textit{x},root) & root(kick,\textit{x}) & \textit{x}(kick,\textit{x}) \\
%\hline
%det(the,ball) & \textit{x}(the,ball) & det(\textit{x},ball) & det(the,\textit{x}) & \textit{x}(the,\textit{x}) \\
%\hline
%dobj(ball,kick) & \textit{x}(ball,kick) & dobj(\textit{x},kick) & dobj(ball,\textit{x}) & \textit{x}(ball,\textit{x}) \\
%\hline
%\end{tabular}
%\end{center}
%%\caption{The dependency parse of an example NNS response in CoNLL\footnote{Standard dependency parse format established by the Conference on Computational Natural Language Learning (CoNLL).} format and the corresponding visual representation.}
%\caption{Given the example sentence above, the updated approach represents responses in the dependency formats shown: ldh (for \textit{label}, \textit{head}, \textit{dependent}; i.e., labeled dependencies)), xdh (unlabeled dependencies), lxh (label+head), ldx(label+dependent), or xdx (word, or more technically, \textit{dependent}).}
%\label{tab:dep-rep}
%\end{table}
%
%Next, I obtained five different representations from the lemmatized dependencies, as shown in Table~\ref{tab:dep-rep}. I refer to this variable as \textbf{term representation}, in keeping with \textit{term} frequency-inverse document frequency, discussed in Section~\ref{sec:scoring}.
%%I first tokenized and lemmatized a response to a list of dependencies that represents the response.
%The five term representations are then variations on dependencies. The
%full form is the \textbf{labeled dependency} and includes the \textbf{l}abel, \textbf{d}ependent and \textbf{h}ead, so I refer to it as \textbf{ldh}. The remaining four forms abstract over either the label, dependent and/or head. I refer to these forms as \textbf{xdh} (i.e., unlabeled dependency), \textbf{lxh} (label+head), \textbf{ldx} (label+dependent), and \textbf{xdx} (dependent only, roughly equivalent to \textit{word} in a bag-of-words approach).
%
%The goal in choosing these five representations was to find the optimal combination of dependency features and the right level of detail to obtain the best system performance. The bag-of-words representation was implemented largely to provide a baseline by which to compare the others.
%
%This processing was applied to the collection of NS responses as well. For each item, the dependencies from all NS responses was pooled into a single flat list---a ``bag-of-dependencies.'' From this list, a copy in each of the five term representations was produced, allowing for comparison with the corresponding NNS data.
%%I tested the system performance using each of these term representations separately.
%
%%The \param{xdx} model is on a par with treating the sentence as a ``bag
%%of words'' (or more accurately, a bag of lemmas), except that some function words not receiving parses (e.g., prepositions) are not included (see section~\ref{sec:syntactic-form}).
%
%\subsection{Scoring responses}
%\label{sec:scoring}
%
%Taking the five term representations, my next
%step was to score them in a way which ranks responses from most to
%least appropriate.  I devised four scoring approaches, each
%using one of two methods to \textbf{weight} response terms combined
%with one of two methods to \textbf{compare} the weighted NNS terms
%with the NS data.
%
%For weighting, I used either a simple relative frequency measure
%or one based on \textit{term frequency-inverse document frequency} (\textit{tf-idf})
%\citep[][ch. 6]{manning-et-al:08}. Most commonly, a \textit{term} for tf-idf purposes would be a \textit{word}. However, \textit{term} here refers to a single syntactic dependency, and this is a central conceit of both the study in reference and the larger dissertation:  the dependency, which captures aspects of semantic and syntactic relationships, is an ideal atomic unit for evaluating meaning by comparing distributions in crowdsourced data. As discussed, the dependencies were represented in one of the five term representations---some combination of label, dependent and head. For the NNS data, each response was treated as a document. For the NS data, the entire collection of NS responses was treated as one document. The relative frequency measure was simply the token count of a given term in a document normalized by the total count of tokens in the document.
%
%I used tf-idf as a measure of a term's importance with the expectation that it would reduce the impact
%of semantically less important terms---e.g., determiners like
%\textit{the}, frequent in the responses, but mostly unnecessary for evaluating the
%semantic contents---and to upweight terms which may
%be salient but infrequent, e.g., only used in a handful of NS
%sentences. For example, for an item depicting a man shooting a bird
%(see Table~\ref{tab:i10responses-avgprec} and Figure~\ref{fig:example-picture}), of 14 NS responses, 12 described the subject as \textit{man}, one as \textit{he} and one as
%\textit{hunter}. Since \textit{hunter} is relatively infrequent in English, even
%one instance in the NS responses should get upweighted via tf-idf, and indeed
%that was the effect; in the bag-of-words approach, the term \textit{hunter} is weighted among the highest, and the same is true for dependencies containing the word \textit{hunter} among the other term representations. This is valuable, as numerous NNS responses used \textit{hunter}.
%
%Calculating tf-idf relies on both \emph{term frequency} ($tf$) and
%\emph{inverse document frequency} ($idf$).  Term frequency is simply
%the raw count of a term within a document. Inverse document frequency is derived from some reference corpus of documents, and it is based on the notion that appearing in more documents makes a term less informative with respect
%to distinguishing between documents.  The formula is in
%(\ref{ex:tfidf}) for a term $t$, where $N$ is the number of documents
%in the reference corpus, and $df_{t}$ is the number of documents
%featuring the term ($idf_{t} = \log \frac{N}{df_{t}}$).  A term
%appearing in fewer documents will thus obtain a higher $idf$ weight,
%and this should readjust frequencies based on semantic importance.
%
%\begin{exe}
%\ex\label{ex:tfidf} $tfidf(t) = tf_{GS} \log \frac{N}{df_{t}}$
%%, where $df_t = |\{d\in D, t \in d\}|$
%\end{exe}
%
%After this frequency counting or tf-idf weighting, the scores were then either
%\textbf{averaged} to yield a response score, or NNS term
%weights and NS term weights were treated as vectors and the response
%score was the \textbf{cosine distance} between them.  This
%yields the four approaches:
%
%%%former approach names: b = FA; m = IC (TC); c = FC; a = IA (TA)
%\paragraph{Frequency Average (FA).} 
%%This approach serves as our baseline. 
%Within the set of NS responses, the relative frequency of each term is calculated. The NS \textit{model} here is simply each NS term and its relative frequency. Each term in
%the NNS response is then given a score equal to its frequency in the
%NS model; terms missing from the NS model are scored zero. The response score is
%the average of the term scores, with higher scores closer to the NS model.
%
%\paragraph{Tf-idf Average (TA).} This involves the same
%averaging of term scores as with approach FA, but here the term scores are the tf-idf scores. The NS model here is thus each NS term and its tf-idf score. 
%
%\paragraph{Frequency Cosine (FC).} Each term score is taken as its relative frequency 
%calculated within its document: either the NS response set or the single NNS response. The NS model is then the set of all NS terms and their scores. The term scores are then treated as vectors---one vector of the NS term scores (i.e., the NS model here)---and one vector of the NNS term scores. Each vector is an ordered list of term scores for each term observed in either the NS document or the NNS document. In other words, each vector represents term scores for the sorted union set of NS and NNS terms. Naturally, many of the term scores are zero for the much shorter NNS document. The response score is the cosine distance between the vectors, with lower scores being closer to the NS model.
%
%\paragraph{Tf-idf Cosine (TC).} This involves the same distance comparison as with approach FC, but now the term scores in the vectors are tf-idf scores. The NS model here is thus the vector representing the union set of terms for the NS and NNS document, populated with the tf-idf scores from the NS document.
%
%\bigskip
%
%%The two cosine approaches are effectively primitive versions of sentence encoders like the currently popular BERT \citep{BertDevlin2018} and Universal Sentence Encoder \citep{UniversalSentenceEncoder}. Sentence encoders are a form of language model that learns mathematical representations of words by observing them in context, accounting for things like average distance from a given word type to another given word type. Sentence encodings are thus vectors representing these word values for a full sentence. These approaches result in very high dimensional spaces---imagine a sentence representation that consists of a vector for each word in the sentence, where each vector is a list of average distances from that word type to \textit{every other word type in the language}. Thus sentence encoders typically rely on methods of dimensionality reduction to compress these representations into vectors of manageable length.
%%I say my cosine approaches constitute ``primitive'' encoders because they omit this step.
%In many natural language processing scenarios where high dimensional vectors are involved, such as sentence encoders or word embeddings, methods for dimensionality reduction are employed \citep{BertDevlin2018,word2vec}. This improves efficiency by reducing the storage and computing power needed. In my FC and TC approaches, however, the number of word types (and in turn term types) remained small enough that the raw vectors representing dependencies' tf-idf scores can be processed easily with an ordinary PC. Not only does this simplify the task, it means that the process remains transparent. There are no transformers or attention mechanisms to produce compressed and unexplainable representations.  If desired, each sentence vector can be examined value by value, where each number maps to a real syntactic dependency. This is important because it leaves the door open for meaningful feedback on each response. For example, one might choose to identify the most salient dependencies in the NS model and use them to guide an ICALL user from a low scoring response to a better response.
%%That is to say, a full encoder could determine how similar a response is to a GS, but it could not tell us \textit{why} it made its determination.
%
%\subsection{System Parameters}
%\label{sec:parameters}
%
%I ran a total of 30 experiments, with each representing a combination of these system parameters for processing responses (see also Table~\ref{tab:dist-ranked-parameters}): 
%
%\paragraph{Term representation} As discussed in
%section~\ref{sec:response-rep}, the terms can take one of five
%representations: \param{\textbf{ldh}}, \param{\textbf{xdh}}, \param{\textbf{lxh}}, \param{\textbf{ldx}},
%or \param{\textbf{xdx}}.
%
%\paragraph{Scoring approach.} As discussed in
%section~\ref{sec:scoring}, the NNS responses can be
%compared with the NS models via approaches \param{\textbf{FA}}, \param{\textbf{TA}}, \param{\textbf{FC}}, or \param{\textbf{TC}}.
%
%\paragraph{Reference corpus.} The reference corpus for deriving tf-idf
%scores can be either the \textbf{Brown Corpus} (\param{\textbf{Brown}}) \citep{kucera:francis:67} or the
%Wall Street Journal (\param{\textbf{WSJ}}) Corpus \citep{marcus-et-al:93}. This is
%only relevant to approaches \param{TA} and
%\param{TC}, so \param{na} is used where approaches \param{FA} and \param{FC} are involved. The corpora are divided into as many documents as
%originally distributed (\param{WSJ}: 1640, \param{Brown}: 499). \param{WSJ} is
%larger, but \param{Brown} has the benefit of containing more balance in its
%genres (vs. newstext only). Considering the narrative nature of PDT
%responses, a reference corpus of narrative texts would be ideal, but as no such reliably parsed corpus is available, I chose the widely used, pre-parsed \param{Brown} and \param{WSJ} corpora. The corpora were converted from their standard dependency parse format to each of the five term representations used in order to be compatible with the NS and NNS data for tf-idf.
%
%%\paragraph{NNS source.} Each response has an original version
%%(\param{NNSO}) and the output of a language model spelling corrector
%%(\param{NNSLM}). (The current dissertation relies on a corpus for which participants used spell checking at the time of the task, so this offline spelling correction is no longer applicable. In short, it used a spelling tool to find candidate spellings for each word in a NNS sentence, pruned the lists of candidate words by comparing against words in NS responses, formed new candidate sentences by combining candidate words, and finally chose the most likely sentence by rating each candidate with a trigram word model. I omit the exact details here for brevity, but more can be found in \cite{king:dickinson:14}).
%%
%\subsection{Experiments and Results}
%\label{sec:metrics}
%\subsubsection{Evaluation metrics}
%
%%I ran 60 response experiments, each with different system settings
%I ran 30 response scoring experiments, each with different system settings
%(section~\ref{sec:parameters}). Within each experiment, I ranked the 39
%scored NNS responses from least to most similar to the NS model.
%
%For assessing these settings themselves, I relied on the past annotation, which counted unacceptable responses as errors (see section~\ref{sec:rule-results}).  As the lowest numerical rank indicates the greatest distance from the NS model, a good system setting should position the unacceptable responses among those with the lowest rankings.
%%Thus, I assigned each error-containing
%%response a score equal to its rank (or the average rank in the case of tied responses).
%To evaluate this discriminatory power, I used \textbf{(mean) average precision ((M)AP)}
%\citep[][ch. 8]{manning-et-al:08}.
%
%\begin{table}[htb!]
%\begin{center}
%\setlength{\tabcolsep}{0.3em}
%\begin{tabular}{|r|c|l|c|}
%\hline
%Rank & Score & Sentence & Error \\
%\hline
%\hline
%\multirow{2}{*}{1} & 1.000 & she is hurting. & 1 \\
%& 1.000 & man mull bird & 1 \\
%\hline
%3 & 0.996 & the man is hurting duck. & 1 \\
%4 & 0.990 & he is hurting the bird. & 1 \\
%\hline
%11 & 0.865 & the man is trying to hurt a bird & 1 \\
%12 & 0.856 & a man hunted a bird. & 0 \\
%\hline
%17 & 0.775 & the bird not shot dead.  & 1 \\
%18 & 0.706 & he shot at the bird & 0 \\
%19 & 0.669 & a bird is shot by a un & 1 \\
%20 & 0.646 & the old man shooting the birds & 0 \\
%\hline
%37 & 0.086 & the old man shot a bird. & 0 \\
%38 & 0.084 & a old man shot a bird. & 0 \\
%39 & 0.058 & a man shot a bird & 0 \\
%\hline
%\hline
%\multicolumn{4}{|c|}{Average Precision: 0.75084} \\
%\hline
%\end{tabular}
%\caption{Example item rankings for the best system setting (\param{TC}, \param{Brown} Corpus, and labeled dependencies (\param{ldh}) based on average precision scores. Note that not all 39 responses are shown.}
%\label{tab:i10responses-avgprec}
%\end{center}
%\end{table}
%
%For average precision (AP), one calculates the precision of error
%detection at every point in the ranking, lowest to highest. Consider
%Table~\ref{tab:i10responses-avgprec}, which presents an excerpt of ranked sentence
%responses for one PDT item. The precision for
%the first cut-off (1.000) is 1.0, as two responses have been
%identified, and both are errors ($\frac{2}{2}$). At the 11th- and
%12-ranked response, precision is 1.0 (=$\frac{11}{11}$) and 0.917
%(=$\frac{11}{12}$), respectively, precision dropping when the item is
%not an error.
%AP averages over the precisions for all $m$ responses ($m=39$ for the
%NNS data), as shown in (\ref{ex:ap}), with each response notated as
%$R_k$.  Averaging over all 10 items results in the Mean AP (MAP).
%
%%In Table~\ref{tab:i10responses-avgprec}, an excerpt of sentence
%%responses is shown for one item, ranked from lowest to highest.  To
%%take one example, the third-ranked sentence, \textit{the man is hurting duck}, has a score of 0.996, and it is annotated as an error (1 in
%%the \textit{E} column). 
%%Thus, the evaluation metric adds a score of 3
%%to the overall sum.  The sentence ranked 18, by contrast, is not an
%%error, and so nothing is added.  In the case of the top rank, two
%%responses with errors are tied, covering rank 1 and 2, so each adds a score of 1.5.
%%
%%The sum of these scores is taken as the \textbf{Raw} metric for that
%%experimental setting. In many cases, one version of a response
%%(\param{NNSO} or \param{NNSLM}) contains an error, but the other
%%version does not. Thus, for example, an \param{NNSO} experiment may
%%result in a higher error count than the \param{NNSLM} equivalent, and
%%in turn a higher Raw score.
%%In this sense, Raw scores emphasize error reduction and incorporate
%%item difficulty.
%%
%%However, it is possible that the \param{NNSO} experiment, even with
%%its higher error count and Raw score, does a better job ranking the
%%responses in a way that separates good and erroneous ones.
%
%\begin{exe}
%\ex\label{ex:ap} $AP(item) = \frac{1}{m} \sum\limits_{k=1}^m
%Precision(R_k)$
%\end{exe}
%
%%As mentioned, the Raw metric emphasizes error reduction, as it
%%reflects not just performance on identifying errors, but also the
%%effect of the overall number of errors.
%%In this way, it may be useful
%%for predicting future system performance, an issue we explore in the
%%evaluation of clustering items (section~\ref{sec:clusteringresults}).
%%MAP, on the other hand, emphasizes finding the optimal separation
%%between errors and non-errors and is thus more of the focus in the
%%evaluation of the best system parameters next.
%
%\subsubsection{Best system parameters} 
%
%To start the search for the best system parameters, it may help to
%continue with the example in
%Table~\ref{tab:i10responses-avgprec}. The best setting, as determined by the
%MAP metric, uses the tf-idf cosine (\param{TC}) approach with the Brown Corpus (\param{Brown}), and the full form of the labeled dependencies (\param{ldh}). It ranks highest because errors are
%well separated from non-errors; the highest ranked of 17 total errors
%is at rank 19.  Digging a bit deeper, one can see in this example how
%the verb \textit{shoot} is common in all the highest-ranked cases shown
%(\#37--39), but absent from all the lowest, showing both the effect of
%the NS model (as all NSs used \textit{shoot} to describe the action) and the
%potential importance of even simple representations like lemmas.  In
%this case, the labeled dependency (\param{ldh}) representation is best, likely because the
%word \textit{shoot} is not only important by itself, but also in terms
%of which words it relates to, and how it relates (e.g.,
%\textit{dobj(bird,shoot)}).
%
%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|r|l|c|c|c|}
%\hline
%Rank & MAP & Approach & Reference & Term rep.\\
%\hline
%\hline
%1 & 0.5168 & \param{TC} & \param{Brown} & \param{ldh} \\
%\hline
%2 & 0.5128 & \param{TC} & \param{WSJ} & \param{ldh} \\
%\hline
%3 & 0.5124 & \param{TC} & \param{Brown} & \param{xdh} \\
%\hline
%4 & 0.5109 & \param{TC} & \param{Brown} & \param{lxh} \\
%\hline
%5 & 0.5102 & \param{TC} & \param{WSJ} & \param{xdh} \\
%\hline
%\hline
%26 & 0.4826 & \param{FA} & \textit{na} & \param{ldx} \\
%\hline
%27 & 0.4816 & \param{TA} & \param{Brown} & \param{xdx} \\ 
%\hline
%28 & 0.4769 & \param{FC} & \textit{na} & \param{lxh} \\ 
%\hline
%29 & 0.4721 & \param{TA} & \param{WSJ} & \param{xdx} \\
%\hline
%30 & 0.4530 & \param{FA} & \textit{na} & \param{lxh} \\ 
%\hline
%\end{tabular}
%\caption{Based on Mean Average Precision, the five best and five worst combinations of system parameters across all 10 PDT items.}
%\label{tab:all-dist-ranked-settings}
%\end{center}
%\end{table}
%
%Table~\ref{tab:all-dist-ranked-settings} shows the five best and five
%worst combinations of system settings averaged across all 10 PDT items, as ranked by
%MAP. The table clearly indicates that the \param{TC} approach is superior, occurring in all of the top five combinations, while approaches \param{FA} and \param{FC} compete for worst. \param{Brown} appears among top scoring combinations more often than does \param{WSJ}. Finally, for the term representation, labeled (\param{ldh}) and unlabeled (\param{xdh}) dependencies score are used among the top scoring combinations.
%
%I also summarize the rankings for each isolated parameter, presented in Table~\ref{tab:dist-ranked-parameters}. For a given parameter, e.g., \param{ldh}, I averaged the scores from all settings including \param{ldh} across all 10 items. Generally, the same trends appear salient. Notably, \param{TC} outperformed the other models, with \param{FC} and \param{TA} close behind (and nearly tied). Performance fell for the simplest model, \param{FA}, which was in fact intended as a baseline. With \param{\textbf{T}C}$>$\param{\textbf{F}C} and \param{\textbf{T}A}$>$\param{\textbf{F}A}, tf-idf weighting seems preferable to basic frequencies. Likewise, with \param{T\textbf{C}}$>$\param{T\textbf{A}} and \param{F\textbf{C}}$>$\param{F\textbf{A}}, for my term based scoring, comparing score vectors outperformed simply comparing score averages.
%
%\begin{table*}
%\begin{center}
%\begin{tabular}{|l|r||l|r||l|r|}
%\hline
%\multicolumn{2}{|c||}{Approach} & \multicolumn{2}{|c||}{Term rep.} & \multicolumn{2}{|c|}{Ref. corpus} \\
%\hline
%\hline
%0.50630 & TC &0.50499 & ldh & 0.50461 & Brown \\
%\hline
%0.49609 & TA & 0.50405 & xdh & 0.49777 & WSJ \\
%\hline
%0.49471 & FC & 0.49287 & ldx & & \\
%\hline
%0.48247 & FA & 0.49190 & xdx & & \\
%\hline
% & & 0.49115 & lxh & & \\
%\hline
%\end{tabular}
%\caption{Individual system parameters ranked by Mean Average Precision for all 10 PDT items.}
%\label{tab:dist-ranked-parameters}
%\end{center}
%\end{table*}
%
%These findings largely confirmed my expectations. The \param{TC} approach was intended to evaluate responses by focusing comparison on the most salient content. The scores here prove that to be successful. Regarding the top term representations, labeled and unlabeled dependencies both capture the relationship between dependents and their heads, making them an ideal unit for analyzing ``who did what to whom'' in the context of a PDT. Finally, given the subject matter and narrative style of the task, it is unsurprising that \param{Brown} serves as a better tf-idf reference than \param{WSJ}.
%
%The trends noted in these averages were strong overall, but a closer look at individual PDT items revealed some exceptions. For example, for the item depicting a man raking leaves, the \param{ldx} and \param{xdx} term representations were the top performers. As discussed in Section~\ref{sec:rule-results}, 100\% of NSs used the verb \textit{rake}, but only 3/39 NNSs used \textit{rake}. Consider the response, ``The man rakes leaves.'' We can expect the main verb of a transitive sentence to appear in at least three dependencies: \textit{root}(rakes, ROOT), nsubj(man, rakes), and dobj(leaves, rakes). Note that in two of these three, the verb is the syntactic head. Thus, by omitting heads, the \param{ldx} and \param{xdx} representations increase the likelihood of overlap between the NS response and the NNS model. For example, ``The man rakes leaves'' and ``The man sweeps leaves'' both result in the \param{ldx} terms nsubj(man, \textit{x}) and dobj(leaves, \textit{x}).
%
%A similar pattern emerged for an item that 12/14 NSs described as some version of ``a woman is riding a bike/bicycle,'' using the same subject, verb and object. 7/39 NNSs simply framed this as an intransitive, e.g., ``The woman is biking.'' 2/39 chose another verb---\textit{pedal} or \textit{drive}. Finally, while 30 NSs used the verb \textit{ride}, six of these misspelled it---\textit{rid/ridding} or \textit{ridging}. It is also worth noting that 7/37 instances of \textit{bike} or \textit{bicycle} were misspelled. For this item, the top system performance came from combining the \param{FC} approach with the \param{xdx} (bag-of-words) representation. The combination of noisy (misspelled) NNS data and the small, homogenous set of NS data meant that the least granular term representation (\param{xdx}) worked best. The more sophisticated tf-idf based approaches suffer here due to the level of noise, allowing the \param{FC} to win out.
%
%In the next section (\ref{sec:current-method}), I discuss how I applied these findings in the design of the current study, including the data collection and the methods of content analysis used on the much larger corpus described in Chapters~\ref{chap:data} and ~\ref{chap:annotation}.

%Despite the strength of these overall trends, variability
%does exist among the best settings for different items, a point obscured
%in the averages.  In Tables~\ref{tab:i01-dist-ranked-settings} and
%\ref{tab:i05-dist-ranked-settings}, I present the best and worst
%ranked settings for two of the least similar items, 1 and 5.
%Their dissimilarity can be seen at a glance, simply from the range of
%the AP scores (0.05--0.31 for item 1 vs. 0.52--0.81 for item 5), which
%in itself reflects a differing number of erroneous responses (2 [\param{NNSO}]
%or 6 [\param{NNSLM}] for item 1 vs. 23 or 24 for item 5).
%
%For item 1, a drawing of a boy kicking a ball, there is considerable
%variability in the best scoring approach just within the top five settings:
%all four approaches (TA, TC, FA, FC) are in the top five.  Contrary to the overall
%trends, I also found the \param{ldx} form---without any head
%information---in the two best settings.  Note also that, even though
%tf-idf weighting (\param{TA}/\param{TC}) is usually among the best settings, it is
%occurs among the worst settings, too.
%
%For item 5 in Table~\ref{tab:i05-dist-ranked-settings}, a drawing of a
%man raking leaves, the most noticeable difference is that
%of \param{xdx} being among three of the top five settings.
%I believe that part of the reason for
%the higher performance of \param{xdx} (cf. lemmas), is that for this
%item, all the NSs use the verb \textit{rake}, while none of the NNSs use this word.  For item 1 (the boy kicking a ball), there is lexical variation
%for both NSs and NNSs.

%\begin{table}[htb!]
%\begin{center}
%\begin{tabular}{|r|l|c|}
%\hline
%Rank & MAP & Settings \\
%\hline
%\hline
%1 &  &  \\
%\hline
%2 &  &  \\
%\hline
%3 &  &  \\
%\hline
%4 &  &  \\
%\hline
%5 &  &  \\
%\hline
%\hline
%26 &  &  \\
%\hline
%27 &  &  \\
%\hline
%28 &  &  \\
%\hline
%29 &  &  \\
%\hline
%30 &  &  \\
%\hline
%\end{tabular}
%\caption{Based on Average Precision, the five best and five worst settings for item 10, shown in Figure~\ref{fig:example-picture} and Table~\ref{tab:i10responses-avgprec}.}
%\label{tab:i10-dist-ranked-settings}
%\end{center}
%\end{table}

\section{Current method}
\label{sec:current-method}
The work discussed in Section~\ref{sec:current-method} relied on a shaky implementation of ``correctness'' or ``appropriateness'' for responses, and this needed improvement, first and foremost. Developing a better and more reliable annotation scheme was a key goal for me in expanding the work to the current dissertation, in order to give the work more meaning and context and make my corpus useful for a broad range of uses. The annotation planning discussed in Section~\ref{sec:scheme} was a direct result of the challenges working with an inadequate annotation scheme. 

As discussed in Chapter~\ref{chap:annotation}, in the current corpus, annotations no longer encode for errors, but instead give a binary score for five different features, which are then weighted and combined to produce a score between 0.0 and 1.0. This means the evaluation cannot take the form of MAP, but must instead compare response rankings, i.e., how well does the system rank responses in comparison to an ideal ranking based on manual annotations? Spearman rank correlation is used to provide these scores.

Because the annotations and evaluation are much different in the current work, it does not exactly follow that findings from the previous work will hold true. However, I believe that the previous work has highlighted some of the system parameters that are most likely to perform well, and I chose to focus my experiments on some of the best performing settings. For example, all of the current experiments rely on the tf-idf cosine (\param{TC}) approach, as this generally outperformed the others. As discussed in Section~\ref{sec:metrics}, the \param{TC} performance suffers for items where the NNS data is noisiest (with regard to spellings) and the NS data is homogenous (particularly with regard to verb choice). Rather than continue experimenting with the underperforming approaches, I chose to address these issues directly instead. As discussed in Chapter~\ref{chap:data}, to address the spelling noise, I made sure that data collection participants had access to spelling correction while typing their responses. To address the uniformity of the NS responses, I surveyed a much larger group of participants and instructed each of them to provide two responses per PDT item.

The current work retains just three of the five term representations: \param{ldh}, \param{xdh}, and \param{xdx}. The \param{ldh} and \param{xdh} forms performed best with the older dataset. Moreover, as these represent labeled and unlabeled dependencies, their use in linguistics is standard. The \param{xdx} representation is kept here as a rough equivalent of a bag-of-words model, which is useful as a baseline for comparing the other two.

Finally, as \param{Brown} overwhelmingly outperformed the \param{WSJ} as a tf-idf reference corpus, the current work relies exclusively on the Brown Corpus. The old and new datasets follow a similar narrative style, so I expect \param{Brown} would again be the best option here.

%The frequency average (FA) approach is used \lk{Is it?} as a baseline where appropriate, as this presents a very simplistic distributional approach as opposed to the more linguistically sophisticated TC. The large increase in the size of the datasets also means that running an exhaustive search for the best parameters among all combinations is not reasonable; as discussed, the previous work used 60 different system settings in total. For that reason, I have chosen to focus on experiments that optimize for each parameter individually, then combine the best parameter settings for use on held out data to ensure they perform optimally. These experiments and their results are discussed in Chapter 6.
%
