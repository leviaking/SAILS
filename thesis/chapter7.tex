\chapter{Conclusion}
\label{chap:conclusion}

\section{Summary}
\label{sec:conclusion-summary}

%This dissertation has presented a successful approach to automatic content analysis for NNS responses to visual prompts that relies only on corresponding crowdsourced NS responses and NLP tools that are readily available and explainable. Suitable data were necessary to accomplish this, and this dissertation also presented a novel corpus of over 13,000 PDT responses from NSs and NNSs, with each response annotated for five features. A major portion of this work involved the development and application of the annotation scheme, which was intended to capture features related to response accuracy and native-likeness. With the inter-annotator reliability for two annotators sufficiently high,
%%---Cohen's kappa ranges from 0.744 to 0.936 for all five features---
%the corpus alone constitutes a significant contribution as a resource for ICALL and SLA research. By highlighting trends between the performance of my similarity-based response ranking system and aspects of the data (i.e., feature annotations and lexical density), this work has furthered the drive toward meaning-focused analysis in ICALL systems.

This dissertation presented a successful approach to automatic content analysis for NNS responses to visual prompts that relies only on corresponding crowdsourced NS responses and NLP tools that are readily available and explainable. This work is grounded in two basic premises: first, \textit{task-based language teaching} and \textit{communicative language teaching} are far more effective methods for second language learners than the grammar drills and vocabulary exercises used in the majority of ICALL applications to this day; and second, ICALL has largely failed to take advantage of the ability of existing NLP tools to produce pedagogically sound, communication-focused systems for learners. 

In order to implement an NLP pipeline capable of assessing the semantic appropriateness of NNS sentences in various contexts, one must tackle a number of significant challenges and unknowns. Many current and past ICALL projects that address user content tend to rely on processes, rules and answer keys that are painstakingly tailored to particular tasks or curricula. This kind of development is time-consuming and expensive, and it does not easily transfer to new tasks or topics. These are two of the biggest obstacles to the adoption of task-based, communicative ICAL---and, I would argue, the wider success of ICALL in general. 

Searching for ways to overcome these obstacles, I posited that models for assessing the content of NNS task responses can be obtained at relatively low cost by simply crowdsourcing responses to the same task from NSs, then using NLP tools to identify the important components and derive representations of response meaning that abstract over surface forms. For an ICALL system where users respond to prompts in order to complete tasks, such an approach could be readily transferred to new tasks and curricula. To prove this as a viable concept, I had to determine whether NS and NNS behavior in ICALL-like tasks is adequately similar. I also had to determine what kinds of linguistic representations are sufficient for comparing this behavior in a way that primarily addresses meaning, and in turn, how NLP tools can be used to produce such representations.

This dissertation determined that NS and NNS behavior \textit{is} adequately similar for this purpose, and that syntactic dependencies are well-suited for content-focused representation and comparison. Naturally, these assertions must be qualified---in both the pilot study and the main study, the NNSs were all university-level students in English as a Second Language courses, and the majority of these were first language speakers of Chinese (or, in the case of the pilot study, Arabic). It cannot be assumed that my findings would apply to learners at lower proficiency levels or from other first languages. Moreover, this work relies on a picture description task (PDT) as a proxy for an ICALL-style communication task, and thus the validity of my approach for use in other kinds of tasks is not asserted here. Nonetheless, I believe my findings are a meaningful step forward. Specifically, this work found that my system, which treats NS and NNS responses as \textit{bags-of-dependencies}, then scores a NNS response using distance metrics for these ``bags'', reaches high levels of accuracy in predicting human annotations.

This connects directly with other important questions facing this work: \textit{What kinds of annotation are needed to address nativelikeness and response appropriateness?} and \textit{Can this annotation be applied reliably?}
In both studies here, suitable data were necessary to find the answers. Despite some success with the pilot study, the simple, single binary ``correctness'' label applied to responses was inadequately robust and reliable. The main study addressed this by introducing a novel corpus totaling over 13,000 responses to intransitive, transitive and ditransitive PDT items from a total of 499 NSs and NNSs, with each response annotated for five features.

The development and application of this annotation scheme comprised a major portion of this dissertation. Through a process of trial-and-error, the initial, unreliable construct of \textit{correctness} was split into five binary annotation features: \feat{core event}, \feat{answerhood}, \feat{grammaticality}, \feat{interpretability}, and \feat{verifiability}. These features were shown to reach sufficiently high levels of inter-annotator reliability for two annotators---Cohen's kappa ranges from 0.744 to 0.936 for all five.

Moreover, through a paired-response preference task, I showed that agreement levels about the holistic quality of PDT responses are also reliable. I also showed that these preferences correlate with the feature annotations. This further validated the annotation scheme for my purposes, as it indicated that the features are predictive of human judgements regarding the appropriateness of a response in the PDT context. Additionally, this allowed me to derive weights that correspond to each feature's importance with regard to this holistic appropriateness. The weights were then applied to the annotations to arrive at a holistic score for each NNS response. Ranking NNS responses by these holistic scores resulted in a benchmark ranking suitable for evaluating the kinds of downstream processing necessary for this dissertation (and likely suitable for a range of uses beyond the current scope).
Given these considerations, the corpus itself constitutes a significant contribution as a resource for ICALL and SLA research.

The development and validation of the new corpus unlocked this dissertation's main study, which focused on the use of NLP and NS data to automatically rank each NNS response in ways that predict its feature annotations or interpolated holistic score. This work expanded on a ranking method developed in the pilot study. This method used lemmatized syntactic dependencies (\textit{label-dependent-head}) as the ``atomic unit'' of analysis, representing a NS response as one bag-of-dependencies and the collection of NNS responses as another. As a measure of importance, the dependencies were given term frequency-inverse document frequency (tf-idf) scores through comparison with a balanced corpus of English. Then these term scores were treated as vectors, with the NS vector serving as a model and the cosine distance between the vectors constituting the NNS response score. The hypothesis here was that an NNS response's manual annotations and benchmark holistic score should correlate with its distance from the NS model.

This automatic ranking system is a pipeline of NLP tools: a dependency parser, a lemmatizer and a tf-idf tool and reference corpus. Each of these tools can be run with multiple settings or trained on various datasets.
%, meaning my system can be run in many different configurations, and these decisions can impact the performance.
The settings chosen for these tools were fixed in the main study as a result of findings in the pilot study.

Also as a result of the pilot study, new variables or \textit{parameters} were introduced; the idea here was that impact on performance is unlikely to be random, and given settings may result in better performance for given types of items. Most of these parameters stem from the data collection: NS responses were collected from two groups---\param{crowd\-sourced} and \param{fam\-il\-iar}; PDT prompts were \param{tar\-get\-ed} (indicating the subject) or \param{un\-tar\-get\-ed}; and the NS models were either \param{prim\-ar\-y}, consisting of NSs' first responses to PDT items, or \param{mixed}, containing an equal mix of NSs' first and second responses to PDT items. Downstream, the parsed responses can be represented as either labeled dependencies, unlabeled dependencies, or (unlabeled and headless) dependents. As a final variable, I investigated the use of models containing either 14 or 50 NS responses.

I used all possible combinations of these settings to generate NNS response rankings for each of the PDT items. By comparing all the rankings in a \param{tar\-get\-ed} setting versus all the rankings in an \param{un\-tar\-get\-ed} setting, for example, I identified a number of trends showing correlations between parameter settings and item type (namely \textit{transitivity}). I also identified correlations between parameter settings and two easily measurable characteristics of the PDT items: NS model lexical density (type-to-token ratios) and average NS response length.

These experiments took two forms. In the first experiments, I assessed each parameter setting by treating an individual binary annotation feature (e.g., \feat{core event}) as an \textit{error} or \textit{non-error}, then calculating \textit{mean average precision} for the ranking of NNS responses produced by that setting. This indicated how well that parameter setting performs at separating NNS responses according the binary value of the annotation feature. This found a number of small but consistent correlations, showing that my system can be optimized to achieve the best possible rankings for new PDT items. This also indicated that my system can be optimized to predict any of the five features, which could be very useful in ICALL for lessons focused on particular aspects of communication. To put my system scores in some context, I also used a state-of-the-art sentence embedding model called SBERT in place of my pipeline to generate a similarity score for each NNS response and NS model. These scores were then used to rank NNS responses. I found that my system achieves a higher mean average precision than SBERT for all five features.

The second set of experiments relied on the same system-produced and SBERT-produced rankings, but compared these to the benchmark holistic response rankings based on the weighted annotations. To evaluate each ranking, I used the Spearman rank correlation score for this comparison with the benchmark. I again controlled for each system setting; in this case, I looked at mean and median Spearman scores for a given setting to determine which settings work best for different item types. This also showed some trends that make optimization for new items possible in contexts where overall response quality is the concern. SBERT showed a slight but consistent advantage over my system for this holistic ranking task. Based on these findings, I believe further development of a holistic scoring system for ICALL could successfully integrate both systems, relying on an opaque sentence encoder like SBERT for scoring and a transparent, dependency-based system like mine to generate feedback for learners.

Broadly speaking, the identification of these correlations between particular system settings and item types validate my annotation scheme and response scoring system and indicate that they can be successfully deployed in ICALL tasks involving responses to visual stimuli. Thus this work constitutes a step forward in the drive toward meaning-focused analysis in ICALL systems.


\section{Outlook}
\label{sec:conclusion-outlook}

For every question answered in this dissertation, multiple new questions emerged. In the following sections, I discuss the possible future directions for this work in the areas of corpus linguistics, annotation, SLA, and ICALL.

\section{Corpora and Annotation}
\label{sec:outlook-annotation}
The corpus of PDT responses collected for the main study of this dissertation has been published as the \textit{Semantic Analysis of Image-based Learner Sentences} Corpus or \textit{SAILS} Corpus.\footnote{https://github.com/sailscorpus/sails} This corpus is annotated and holistically scored as described in Chapter~\ref{chap:annotation}. In its current form, the SAILS Corpus has much to offer researchers interested in ICALL, SLA, language testing, task design, and linguistic annotation. With the majority of responses coming from NNSs, it can also find uses not directly related to language learners, such as image captioning or human-computer interaction. 

An obvious way to increase the utility of the corpus would be to simply collect more responses from more participants. The work described in Chapter~\ref{chap:optimization} used NS models of 50 responses and NNS test sets of 70 responses, as these were the maximum numbers of responses available for all relevant experiments. Spearman correlation scores were used in Section~\ref{sec:exp-holistic} to evaluate the performance of my system at producing holistic response rankings. While this did lead to the identification of useful trends, many of the p-values accompanying the Spearman scores failed to establish statistical significance. With much larger NNS response sets, I believe I could confirm that these trends are statistically significant, and possibly identify trends not currently evident.

Additionally, differences in system performance were reported when comparing smaller, 14-response models with the 50-response models, and there is good reason to believe that even larger models would outperform the 50-response models in some cases. With a larger pool of NS responses from which to sample models, one could plot a curve representing system performance across a wide range of model sizes. From there, one could pinpoint the optimal NS model size for handling different item types, such as intransitive, transitive and ditransitive items.

Given that 125 of the 141 NNS participants in the SAILS Corpus were native speakers of Chinese, collecting data from participants with other first languages (L1s) would be greatly beneficial. This would allow me to investigate whether the trends found in the current work generalize for all university-level English learners or whether new trends emerge for other L1 groups. Of course, this would also allow for many other kinds of studies not related to my system, such as comparisons of the English interlanguage of various L1 speaker groups in the context of PDTs.

Expanding the study to lower-level English learners would open a new avenue of investigation as well. Performing a PDT or using a related ICALL system is only feasible with some minimum level of proficiency, and collecting data from a wider range of learner levels would allow me to determine a proficiency threshold for the effective use of my response scoring approach. Moreover, it would allow for cross-sectional SLA studies not directly related to the ICALL-oriented current work.

The SAILS Corpus contains responses from 329 \param{crowd\-sourced} NSs, but only 29 \param{fam\-il\-iar} NSs. A more in-depth exploration of the differences between these two groups could yield findings relevant to data science, crowdsourcing, task and survey design, and various areas of linguistics, but a much larger pool of \param{fam\-il\-iar} responses is needed. With regard to the current study, I believe this kind of investigation might lead to better system performance; ideally, I would be able to determine the overall relative quality of \param{crowd\-sourced} versus \param{fam\-il\-iar} responses and make the best use of the available data by weighting responses in the NS models accordingly.

A relative lack of motivation was assumed to affect the quality of \param{crowd\-sourced} responses, but these NS respondents cannot be assumed to be monolithic. By comparing the respondents' demographic data with the annotation, it may be possible to identify correlations that can lead to the construction of better NS models for my scoring system. Again, collecting more data would be helpful here. Given resources to extend this work, I would explore the use of other crowdsourcing and survey platforms that offer participants different rewards or even payment in order to better understand how motivation impacts task behavior.

Like any annotation scheme, the one implemented in the SAILS Corpus represents a compromise. A number of other features were considered but not used, and expanding the annotation scheme to include more features could improve the corpus' utility. For example, a \textit{good faith} feature was considered as a way to label responses that were clearly off-target or intended to be rude or disruptive; such responses constituted a small but non-negligible portion of the crowdsourced data. Having data clearly labeled in this way could help set expectations for crowdsourcing NS models for ICALL tasks and possibly lead to methods for identifying and omitting bad faith responses automatically. I also considered more granular features related to the \feat{core event} feature used in the current annotation. These features would indicate the presence or absence of desired arguments, namely subjects and objects. Having these annotations would allow me to test more sophisticated variations of my scoring approach, particularly with regard to the NS models; responses lacking an object, for example, could still be beneficial in an NS model, but they would likely need to be down-weighted.

\section{Second Language Acquisition}
\label{sec:outlook-sla}

The research in this dissertation is focused primarily on practical issues related to the implementation of content analysis in ICALL systems. However, this work---and in particular, the SAILS Corpus---are adjacent to numerous SLA concerns. In its current form, the corpus is ripe for investigations into the interlanguage of advanced learners of English, particularly L1 Chinese speakers. This data could be informative in studies related to image descriptions or similar short answer tasks. The current corpus can allow for comparison of NS and NNS behavior beyond the relevant investigations discussed in this dissertation. For example, the 30 PDT items are categorized as intransitives, transitives and ditransitives, but a deeper investigation of how commonly NS and NNS responses actually align with these categories and in which contexts they diverge could be a dissertation its own right.

The existing annotation adds considerable value for SLA research as well. For example, exploring the relationship between \feat{in\-ter\-pret\-a\-bil\-i\-ty} and \feat{core event} could yield interesting insights into learner behavior: Are there particular contexts in which learners are more likely to produce interpretable responses that nonetheless miss key arguments of an image? Relatedly, are there contexts in which \feat{core event} does not entail \feat{in\-ter\-pret\-a\-bil\-i\-ty}? The \feat{gramm\-at\-i\-cal\-i\-ty} feature could also lead to interesting findings in the corpus; understanding, for example, how often and in what contexts learner PDT responses fail to fulfill \feat{gramm\-at\-i\-cal\-i\-ty} but still meet the standards for \feat{core event} or \feat{in\-ter\-pret\-a\-bil\-i\-ty} could be meaningful to SLA research focused on topics like ultimate attainment.

As mentioned in Section~\ref{sec:outlook-annotation}, the utility of the corpus for SLA purposes could be greatly expanded by adding learners from more L1s or from lower proficiency levels. Understanding differences between L1 groups or proficiency levels in the context of PDTs could lead to actionable findings with regard to language testing and SLA research design.

\section{ICALL}
\label{sec:outlook-icall}

In mapping a way forward for content-focused, low-cost analysis of learner sentences in visual contexts, this project makes a significant contribution to the field of ICALL. I believe this contribution can be greatly enhanced by taking this work further and in new directions. As laid out in Chapter~\ref{chap:intro}, I was motivated by the desire to move toward task-based ICALL systems capable of approximating natural communication and ignoring minor concerns with form. Specifically, I envisioned a story-based ICALL game where learners complete tasks in visual contexts; responses would then be analyzed via comparison with NS models and the game state would change accordingly. A major concern unaddressed by this dissertation, then, is establishing a suitable mechanism for determining how the game would decide to proceed based on a user response; this may be outside my wheelhouse as a linguist. Ideally, I would collaborate with a team of experts in relevant areas like SLA, language pedagogy, human-computer interaction, game design, and creative writing to sketch out a story-based game that is compatible with my approach to content analysis and grounded in linguistic research. Such a team would then need to produce a PDT or similar data collection instrument as needed for producing NS models.

The leap from my scoring approach to an ICALL system capable of sustaining interactions with learners that are long enough to have a meaningful impact on acquisition entails a number of challenges. An important step would be determining when and where to provide feedback and how this can be done automatically or at low cost. Given the current method, for a low-scoring NNS response, an ICALL system could suggest a better replacement from the NS model, which could be implemented as a kind of automatic recasting.

Fleshing out a game beyond these simple interactions would require a lot more content and sophisticated discourse, however. SBERT and similar tools may be an important part of the solution. With advances in both hardware and software, it has only very recently become possible to train sentence encoders and other machine learning language technologies on things like question and answer corpora, dialogs, and essays in ways that capture not just sentence-level language models but also a deeper ``understanding'' of content of and discourse. A drawback to these systems is that their behavior is largely unexplainable, and this is problematic because we need to understand any processing and its intermediate steps in order to properly handle learner input and generate appropriate feedback. This dissertation has begun the work of analyzing the behavior of a sentence encoder versus my dependency-based system in contexts relevant to ICALL. I believe that the future of ICALL lies in the successful integration of highly-effective but opaque machine learning technologies with simpler but transparent NLP approaches like mine. 

