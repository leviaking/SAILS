#!/usr/bin/env bash

if [ ! $# -ge 1 ]; then
  echo Usage: `basename $0` 'file(s)'
  echo
  exit
fi

scriptdir=`dirname $0`

###TOTALLY IGNORE#!/bin/bash

if [ ! $# -ge 1 ]; then
  echo Usage: `basename $0` 'file(s)'
  echo
  exit
fi

#scriptdir=`dirname $0`

###NOTE: This script requires Bash 4 or above (because it uses the readarray command)

##This script takes a file of plain text, with one sentence per line. It parses this text file with Stanford (to constituency and dependency). It also processes the text file with the stanford core-nlp lemmatizer, generating an xml file. It takes the newly parsed .conll file and the xml/lemmatized file, and replaces the conll words with their lemmatized counterparts, writing out a new file as "whatever.lemma_conll".

##HOW TO RUN THIS SCRIPT: Get this directory set up:
##dissertation/
##	SAILS/
##		scripts/
##			gscsv_to_lemmatized_conll.sh
##			gscsv_to_txt.py
##			lemmatize_conll.py
##		responses/
##			I01T_all_ns.csv [etc]
##			txt/
##				I01T_all_ns.txt [etc; these are generated by this script]
##	SAILS_annex/
##		stanford-parser-2012-01-06/
##		stanford-corenlp-full-2014-06-16/

##The first command parses raw text to PTB style constituency trees; the second takes these trees and converts them to dependencies (typed, collapsed, and with propagated conjunctions; modify the stanford command options to change these parameters) printed in CoNLL format.

scriptdir=$(pwd)
#echo $scriptdir
cd ../
rawcsvdir=$(pwd)/responses/rawcsvs
textdir=$(pwd)/responses/txt
lemmaxmldir=$(pwd)/responses/lemmaxml
penndir=$(pwd)/responses/penn
lkconlldir=$(pwd)/responses/LKconll
cd ../
parserdir=$(pwd)/SAILS_annex/stanford-parser-2012-01-06
lemmatizerdir=$(pwd)/SAILS_annex/stanford-corenlp-full-2014-06-16
#echo $scriptdir
#echo $rawcsvdir
#echo $parserdir
#echo $lemmatizerdir
cd $rawcsvdir
#allcsvs=($(ls .))
#echo ${allcsvs[@]}
allcsvs=(
#I01T_all_ns.csv
#I01U_all_ns.csv
)

#for c in ${allcsvs[@]}; do echo $c ; done

cd $scriptdir


for mycsv in ${allcsvs[@]}
	do label=${mycsv/.csv/}  ## filename minus file extension
	mytext=$textdir/$label.txt
	mylemmaxml=$lemmaxmldir/$label.xml
	python $scriptdir/gscsv_to_txt.py $rawcsvdir/$mycsv
	##echo $label
	##echo $mytext
	cd $lemmatizerdir
	####the following lines and the for loop are my hacky and less-than-ideal work around for the following:
	##this was the old command##java -mx500m -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma -file $mytext -outputDirectory $lemmaxmldir
	##LK: I cannot find a way to force the Stanford CoreNLP tool to handle each line as a sentence. For some sentence fragments, it combines the fragment with the subsequent line. For example:
	##The b.
	##Getting ready to take a bite of pizza.
	##The two responses above get combined in the lemmatized output. This is a problem because I cannot then align the lemmatized output with the (correct) parsed output. I think I am now forced to somehow load the lemmatizer anew for each line so that it has no choice but to handle each line as a single sentence.	The following lines do this.
	readarray respies < $mytext
	tempcounter=1
  ## as mentioned above, we load the lemmatizer anew for each response here; this is the only way to force the lemmatizer to interpret each line as its own utterance (and not combine lines containing sentence fragments)
	for respy in "${respies[@]}"
		do padded=$(printf %03d $tempcounter)
		echo $respy > $textdir/lemmatizer_temp.txt
		#cat $textdir/lemmatizer_input_temp.txt
		java -mx500m -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma -file $textdir/lemmatizer_temp.txt -outputDirectory $lemmaxmldir -outputExtension $padded
		cat $lemmaxmldir/lemmatizer_temp.txt$padded >> $mylemmaxml
		rm $lemmaxmldir/lemmatizer_temp.txt$padded
		rm $textdir/lemmatizer_temp.txt
		tempcounter=$((tempcounter + 1))
		#do echo $respy
		done
	####back to normal below... i.e., we load the parser model only once for the whole list of responses
	cd $parserdir
	java -mx800m -cp "stanford-parser.jar:" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -outputFormat "penn" -outputFormatOptions "CCPropagatedDependencies" $parserdir/grammar/englishPCFG.ser.gz $mytext > $penndir/$label.penn
	java -mx800m -cp "$parserdir/stanford-parser.jar:" edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile $penndir/$label.penn -conllx -CCprocessed > $lkconlldir/$label.LKconll
	python $scriptdir/lemmatize_conll.py $mytext
	done

# pseudocode
## for masterannofile in corpusdir:
  ## run python script, which does:
    ## generate "rawcsv" of NNS only, with fields: ResponseID, Response, C, A, G, I, V
  ####### BELOW: Repurpose /scripts/gscsv_to_lemmatized_conll.sh for these steps
  ## from rawcsv, get text file of responses;
  ## with stanford coreNLP, lemmatize responses;
      #### can we cache here to avoid duplicate processing? (we must load lemmatizer for each response)
  ## with stanford parser, parse text to penn, store in csv w fields: ReponseID, penn parse
  ## from penncsv, get text file of penn;
  ## with stanford parser, parse this to conll, store in csv w fields: ResponseID, conll parse
  ####### ABOVE: Repurpose /scripts/gscsv_to_lemmatized_conll.sh for these steps
  ## run lemmatize_conll.py to get lemma_conll files
  ## add lemmatized parses to rawcsv to create finalcsv; see add_parses_to_gscsv.py; repurpose (simplify) this script for test reponses
  ## run prep_conll_for_tfidf.py: this generates the *depstrings.csv, which is finalcsv + ldh, xdh, xdx
#####
## Then we run lk_tfidf.py (must create this by modifying lk_tfidf_LOO_TC.py) on each test item csv
  ## Note that this must score each test response in relation to each GS;
      #### can we cache here to avoid duplicate processing?
