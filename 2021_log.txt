2021/02/16

Here's the remaining work:

1. Split "Method" into "Pilot" and "Method"
	A. Ch 3: "Pilot": 2/16: COMPLETE
	B. Ch 6: "Method": 2/17
	C. Revise "Data", "Annotation" accordingly: 2/18
2. Write "Optimization"
	A. Transitivity: 2/22
	B. Targeting: 2/23
	C. Familiarity: 2/24
	D. Primacy: 2/25
	E. Term Norm: COMPLETE
	F. Term representation: 2/19
	G. Combinations and trends: 3/2
	H. Bert vs Me: 3/4
3. Write Lit Review: 3/12
4. Write Intro: 3/22
5. Write Conclusion: 3/31


2021/02/24:
Today I want to grind out all the stats results I need from that csv of 360 spearman scores.



2021/02/26, Friday.
Meeting with Markus in 10 minutes.
TODO:
Fix term norm results table (make parallel to others)
Add BERT (baseline) scores throughout
Add discussion to results
	dig into what the scores represent, guided by stats, deviations from BERT
	Example:
		Term Representation:
			ldh has highest mean, but xdx has highest max (and highest min)
			xdh has highest median
			What's the story here?
			
From meeting:
Investigate p values for mins and maxs and go from there
Investigate Table 6.6--these probably shouldn't be negatives!

Check in in a week with email updates;
Meet on 3/12 at 1:30pm.
################################################################################


2021/02/28, Sunday. BERT.
################################################################################

2021/03/01. Monday. BERT.
Today:
Complete BERT script;
Run BERT on my data;
Generate Spearman scores (BERT vs Weighted Annotation);
DONE;
Tomorrow: modify and rerun thesis_spearman_stats.py to include BERT scores
################################################################################

2021/03/09. Tuesday.
When BERT is finished running on all N14 samples, run get_bert_spearman_correlations.py.
update thesis_bert_spearman_stats.py (cf thesis_spearman_stats.py); run it.
DONE

##
ARGH! I've been using BERT in training mode (default?); this results in non-deterministic scores.
I'm going to run it again in "eval" mode.:
#
Run BERT N14 vs N70:
change bert_vs_sails.py: train_sample = 'N14'
change get_bert_spearman_correlations.py: train_sample = 'N14'
change thesis_bert_spearman_stats.py: train_sample = 'N14'
python bert_vs_sails.py
python get_bert_spearman_correlations.py
python thesis_bert_spearman_stats.py
DONE ALL
#
Run BERT N50 vs N70:
change bert_vs_sails.py: train_sample = 'N50'
change get_bert_spearman_correlations.py: train_sample = 'N50'
change thesis_bert_spearman_stats.py: train_sample = 'N50'
python bert_vs_sails.py
DONE
python get_bert_spearman_correlations.py
DONE
python thesis_bert_spearman_stats.py
DONE
#
###
TODO:
UPDATE ALL CHAPTER 6 TABLES WRT BERT SCORES!
###


################################################################################
2021-03-12.
Updating Chapter 6 now.
TODO:
Regenerate / check numbers for term norm table (currently 6.6); reconcile this table format with others throughout chapter 6.
Write discussion of Spearman correlation; why use Spearman (vs Pearson, etc)? What is the range of Spearman scores and what do they mean? (1.0, 0, -0.5, -1.0)
Write discussion of BERT; what it is/does, how it works; training data; model and implementation used;


Questions for Markus:
How to best highlight information in the Tables?
Does it make sense to highlight the highest (or lowest) standard deviation?

################################################################################
2021-03-26.
TODO:
Complete Familiarity experiments and tables;
################################################################################
Discussion of BERT / SBERT;
inputs, outputs;
mention and cite architecture, but not in detail;
what do we learn from using BERT w.r.t. my system? BERT wins, so we should abandon my system? (NO, but elaborate)
################################################################################
Email Markus on 4/2;
meet again on 4/9;

################################################################################
################################################################################
TODO:
###
CHANGE TO N14 and run:
python get_all_tfidf_cosines-weighted_deps.py DONE

CHANGE TO N14 and run:
python get_all_spearman_correlations.py DONE

CHANGE TO N14 and run:
python thesis_spearman_stats.py DONE

###
CHANGE TO F14 and run:
python get_all_tfidf_cosines-weighted_deps.py DONE

CHANGE TO F14 and run:
python get_all_spearman_correlations.py DONE

CHANGE TO F14 and run:
python thesis_spearman_stats.py DONE

###
CHANGE TO N50 and run:
python get_all_tfidf_cosines-weighted_deps.py DONE

CHANGE TO N50 and run:
python get_all_spearman_correlations.py DONE

CHANGE TO N50 and run:
python thesis_spearman_stats.py DONE

## For UNWEIGHTED F14 and N14 "term_vectors" folders -- restore these from Github DONE
################################################################################

2021-04-01. Thursday.
Today:
Grind out the last of the table work in Ch. 6:
	Confirm which configurations should be covered by each;
		Confirm that the correct file (exists and) is used;
		Confirm that the numbers in the tables are correct;
Dig into the table and trends for Transitivity (currently Table 6.2);
	"Observe and report" -- any explanations for trends?
Same for remaining parameter stats tables in Ch. 6....


Starting from term-norm table because that might involve some more coding to get the numbers...
Yep. Adapting get_bert_spearman_correlations.py to get_bert_spearman_correlations-weighted_deps.py.
Run this for F14-W, N14-W, N50-W. DONE
Then...
Adapting thesis_spearman_stats.py to combined_thesis_spearman_stats.py to operate on the combined_spearman_Nxx-VS-N70.csv... DONE
it generates the appropriate spearman_stats csvs in the stats/* folders.
Run this for Nxx =
weighted N14, N50, F14 DONE
unweighted N14, N50, F14 DONE
I have confirmed that the descriptive stats produced are correct -- I verified it with formulas in the spearman csvs the descriptive stats are based on.

I can now complete the term-norm table. DONE
Double-check these tables:
term-rep:
familiarity:
primacy:
targeting:
transitivity:
ALL DONE

################################################################################

2021-04-02. Friday.
Still working on the list from yesterday.
Why do some rankings have such high p-values?
What is going on with the negative Spearman correlations?

In each section:
What is the story behind the min and max values? we won't have annotation to identify these in the wild.


Observations:

I've gone over all the tables, noted some observations and now I'm going from there. The good news is that I'm fairly confident there are some trends I can latch on to. The most salient to me has been the effect of the size of the NS models. Throughout I compare "N14" models (14 NS responses) and N50 models; the N14 models typically outperform the N50 models when the conditions are relatively constrained: Targeted vs Untargeted, Primary vs Mixed (first responses vs first + second responses), and Intransitives vs Transitives/Ditransitives. "Constrained" might not be the right word, but these conditions tend to produce lower type-to-token ratios, which I'm also working to incorporate into the discussion.

Right now I'm focusing on the discussion of transitivity and the related Spearman stats. I'm aiming to finish that section over the weekend, because I think whenever you next get a chance to provide some feedback, it would be helpful to know if I'm on the right track with the similar sections.


Table 6.2 Transitivity:
	For both samples (N14 and N50):
		System scores: intrans > trans > ditrans
		BERT scores: trans > intrans > ditrans
	System rarely beats BERT: Intransitive max (N14, N50); Transitive min (N14);
	For System mean and median, N50 beats N14 for trans and ditrans, but is worse for intrans;
	For BERT mean and median, N50 beats N14 for intrans and ditrans, but is worse for trans;
	Suggests that choices relating to sample size and the use of BERT vs system could be optimized for items according to transitivity.

Table 6.3 Targeting:
	Targ > Untarg (always--system & BERT, both sample sizes)
	For System mean and median: N14 narrowly beats N50 for Targeted; N14 narrowly loses to N50 for Untargeted;
		Suggests that smaller samples work better than larger samples under more constrained conditions (i.e., likely higher type-to-token ratio? cf Transitivity Table 6.2 and similar pattern for intransitives)
	For BERT mean and median: N50 is always better than N14 (Targ and Untarg)
	For Untargeted, System achieves a higher max than BERT (N14 and N50)

Table 6.4 Primacy:
	System mean and median: For Primary, N14 > N50; for Mixed, N14 < N50;
	BERT mean and median: For Primary, N14 < N50;
		Mixed mean: N14 < N50; Mixed median: N14 > N50;
	System: For N14, Primary > Mixed; For N50, Primary < Mixed;
	BERT: For N14, there is very little difference between Primary and Mixed; Primary has slightly lower mean but slightly higher median than Mixed. But for N50, Primary mean and median are both slightly higher than Mixed.
		This suggests something about BERT's ability to zero in on the meaning common to all responses as it obvserves more examples. For System, the pattern is kind of intuitive--if you have a small model, you want those responses to be as on-target as possible, which happens with Primary responses; when you ask for second responses, you expand coverage but also get some lower-quality responses. Relatedly, if you do use second responses, it's probably best to do so in models that are large enough that a small number of bad responses cannot ruin the performance overall.
		Cf. Transitivity and Targeting tables; For more constrained contexts like intrans, targeted, or primary response models only, we see that System performs best with the N14 model (vs N50); this likely correlates with type-to-token ratios;

Table 6.5 Familiarity:
	System and BERT show slightly better performance for Familiar > Crowd. This is expected, but there's only N14 and not a lot to go on here.
	
Table 6.7 Term normalization:
	System: very little difference between Normalized and Non-Normalized models; slightly better performance from Non-normalized, more noticeable for N14 than N50.
	Term-normalization isn't worth doing.

Table 6.8 Term Representation:
	For System mean & median: ldh clearly benefits from more training data (N14 < N50); xdx clearly suffers from more training data (N14 > N50); xdh is roughly unchanged (i.e., it's between ldh and xdx in terms of this effect, as we'd expect).
	For N14, xdh is clearly best; xdh > ldh > xdx;
	For N50, ldh has the highest mean, but xdh has the highest median;
	xdx always has the highest minimum (including BERT);
	xdx seems best at these sample sizes; ldh is much more competetive at N50, so maybe there's a sample size where ldh > xdh. With larger datasets, I suspect the labels would become more meaningful, but it's possible that they introduce a bit of noise at these sample sizes.
	
################################################################################
################################################################################
4/9/2021
Still need:
Why do some rankings have such high p-values?
What is going on with the negative Spearman correlations?

Working on Transitivity section.
Question: Why does SBERT perform best on transitives?

"we use the dependency parse information to determine whether each noun is an A or an O, and if it is either we pass the whole sentence through mBERT and take the contextual embedding corresponding to the noun." --papadimitriou2021multilingual
How does this work? how do you get the embedding corresponding to the noun from the sentence embedding?
################################################################################
4/14/2021.
OK, a few changes. I'd been working on getting two metrics: [see training_stats.py] response length and TTR for all NS models. In doing so, I noticed that some of the stats were distorted by extreme outliers -- namely, one response containing 49 words. There were also null/invalid responses coming through as empty strings. I believe some part of my system (lemmatizer?) was filtering them out. That also distorts the picture. My solution was to fix this first and re-run everything. My fix was this:
Return to the training pool files from which the final training files were sampled. In the pool files, I removed any extreme outliers for length--I believe I *could* automate this, so I justify it: a simple rule like "delete from the pool any response which is more than 12 standard deviations away from the mean response length". In practice here, I simply deleted any response which was not just long, but overly long in bad faith. I also deleted any response which was gibberish (this can easily be done with a lexicon, language model, etc.), as well as a few single word, bad faith answers: I believe this could also be automated with a simple list-- most are interjections, "yes", "no", "hey", "ok", "none", "stupid", etc.

From there, I just redid everything:

Sample training sets from NS pools: 
sample_training_sets.py
sample_training_sets-familiar.py

Score NNS responses:
get_all_tfidf_cosines.py
get_all_tfidf_cosines-familiar.py
get_all_tfidf_cosines-weighted_deps.py
bert_vs_sails.py
bert_vs_sails-familiar.py


################################################################################

As of 4/15 (Thursday) morning, I've completed the above and need to do the below:

Get all Spearman scores:
get_all_spearman_correlations.py
get_bert_spearman_correlations.py
get_bert_spearman_correlations-weighted_deps.py (F14, N14, N50)
DONE

Get all Spearman descriptive stats:
thesis_spearman_stats-familiar.py (F14 weighted, F14 unweighted)
DONE
thesis_spearman_stats.py (N14 weighted, N50 weighted, N14 unweighted, N50 unweighted)
DONE

combined_thesis_spearman_stats.py (F14, N14, N50 x weighted, unweighted)
DONE


Now I'm returning to training_stats.py.
This script should produce descriptive stats for TTRs and response lengths that correspond to the various spearman stats files generated in previous steps. For example, the spearman stats files show me how my system (and BERT) performs on intransitives vs transitives vs ditransitives; training_stats.py generates TTR stats and response length stats for the NS models corresponding to these 3 categories. Thus I can see if performance correlates with complexity (as seen through TTR) etc.
The script is mostly complete but I need to fix the get_termrep_ttrs parts.
DONE -- I got this working and it generates all the right stats files now.

################################################################################
OOPS. The termrep ttr stats file revealed that there were 5 fewer responses than should be present in the N50 models. That's 5/3000 total, so not a big deal, but I want to get this right. The issue is that I removed too many responses from the pool files before sampling. I tracked the problem down to just 4 N50 models:
I26U-gNSC-r1-Di-N50.csv
I28U-gNSC-r1-Di-N50.csv
I29U-gNSC-r1-Di-N50.csv
I30U-gNSC-r1-Di-N50.csv

NOTE that NO FAMILIAR files are effected.


I scrounged up enough previously unused responses (leftover from SurveyMonkey) and added them to the 4 corresponding pool files.
I deleted ALL the stats files, because these will all be effected. I deleted the 4 scored files.

I now need to regenerate the samples for these 4 files.
DONE.
Now, rescore them. Note that these all operate by walking the directory, so I need to set up a dummy folder or something for the desired files...
get_all_tfidf_cosines.py
get_all_tfidf_cosines-weighted_deps.py
bert_vs_sails.py
DONE

Then, move everything back into place (2 renamed folders)
DONE

Get all Spearman scores:
get_all_spearman_correlations.py N14, N50, F14 (weighted and unweighted -- uncomment script accordingly)
get_bert_spearman_correlations.py N14, N50, F14
get_bert_spearman_correlations-weighted_deps.py N14, N50, F14
DONE
Get all Spearman descriptive stats:
thesis_spearman_stats.py (N14, N50 x weighted, unweighted)
thesis_spearman_stats-familiar.py (F14 x weighted, unweighted)
combined_thesis_spearman_stats.py (N14, N50, F14 x weighted, unweighted)
DONE

I think that brings me back to:
training_stats.py
DONE

I think this means I end the day roughly where I expected. Tomorrow I need to repopulate all the numbers in the Ch 6 Tables, and add the word length and TTR tables.

################################################################################

Oops, missed at least one script:
thesis_bert_spearman_stats.py
thesis_bert_spearman_stats-familiar.py
DONE

Currently updating tables:

{tab:transitivity-results}
update N14 system numbers: DONE
update N14 BERT numbers: DONE
update N14 highlighting: DONE
#
update N50 system numbers: DONE
update N50 BERT numbers: DONE
update N50 highlighting: DONE


{tab:targeting-results}
update N14 system numbers: DONE
update N14 BERT numbers: DONE
update N14 highlighting: DONE
#
update N50 system numbers: DONE
update N50 BERT numbers: DONE
update N50 highlighting: DONE


{tab:primacy-results}
update N14 system numbers: DONE
update N14 BERT numbers: DONE
update N14 highlighting: DONE
#
update N50 system numbers: DONE
update N50 BERT numbers: DONE
update N50 highlighting: DONE


{tab:familiarity-results}
update F14 system numbers: DONE
update F14 BERT numbers: DONE
update F14 highlighting: DONE
#
update N14 system numbers: DONE
update N14 BERT numbers: DONE
update N14 highlighting: DONE


{tab:term-norm-results}
update N14 unweighted system numbers: DONE
update N14 weighted system numbers: DONE
update N14 BERT numbers: DONE
update N14 highlighting: DONE
#
update N50 unweighted system numbers: DONE
update N50 weighted system numbers: DONE
update N50 BERT numbers: DONE
update N50 highlighting: DONE


{tab:termrep-results}
update N14 system numbers: DONE
update N14 BERT numbers: DONE
update N14 highlighting: DONE
#
update N50 system numbers: DONE
update N50 BERT numbers: DONE
update N50 highlighting: DONE
################################################################################

4/17/2021. Saturday.
Today I'm putting ALL the TTR and word length tables together in Ch 6.
DONE

################################################################################
4/19/2021. Monday.
Still need:
Why do some rankings have such high p-values?
What is going on with the negative Spearman correlations?

For now, I'm working through the experiment sections, discussing trends, sentence lengths and TTRs.

I'm getting frustrated because I'm not finding obvious trends.
The TTRs for intransitives, transitives and ditransitives are opposite what I'd expect: the intransitives have the highest TTR and the ditransitives have the lowest TTR, meaning the intransitives have the highest complexity or lexical density, and the ditransitives have the lowest.
For each of the experiments laid out in Ch 6, I made a scatterplot graph where the TTR is the x axis and the Spearman is the y axis. This didn't reveal a strong pattern.
I wrote a script (ttr_vs_spearman.py) where I generated a csv of the TTR and Spearman for all 120 models. Then I made a scatterplot of this. It's ugly, revealing no clear trends.

So now, in frustration, I'm trying this:
**sample_test_sets_unique_annotation.py**
Instead of ranking a test set of 70 NNS responses, I've sampled test sets of 4 NNS responses. I ensured that each of the 4 responses has a different annotation score (*actually, this wasn't always possible; see I13U--only 2 annotation scores exist because all responses were perfect or nearly perfect (ceiling effect)). My hope is that this will make any patterns more obvious.
Now I have the samples, so I need to complete the pipeline...
For now, I'm going to start with N50 vs N04:
get_all_tfidf_cosines.py
DONE
get_all_spearman_correlations.py
DONE
thesis_spearman_stats.py
DONE
bert_vs_sails.py
DONE

get_bert_spearman_correlations.py
DONE

combined_thesis_spearman_stats.py
DONE

SKIP: training_stats.py No need to repeat this

ttr_vs_spearman.py
DONE


OK, I still don't quite grasp why a given ranking gets a negative Spearman score, but granted I'm not at 100%.
I'm going to repeat the above for test samples of N02, which should make this blaringly obvious.
sample_test_sets_unique_annotation.py
DONE
get_all_tfidf_cosines.py
DONE

get_all_spearman_correlations.py
DONE

thesis_spearman_stats.py
DONE

ttr_vs_spearman.py
DONE

Maybe later:
	bert_vs_sails.py
	get_bert_spearman_correlations.py
	combined_thesis_spearman_stats.py



################################################################################
Still frustrated and searching for trends here... I have a few other ideas to try.

BIG IDEAS:
1. Plot NS avg response length vs Spearman: YES! trend: lower words per response trends with higher (better) Spearman!
	(I recently did NS average TTR vs Spearman but it's not promising)
2. Get NNS avg response length & NNS TTR;
	3. Plot NNS avg response length vs Spearman;
	4. Plot NNS TTR vs Spearman
5. Calculate (mean) average precision for Core Event (instead of Spearman based on weighted annotation ranking);
	(This shows how well my system can separate Core=1 responses from Core=0 responses)
6. More granular analysis: For example, compare: Targ+r1, Targ+r2, Untarg+r1, Untarg+r2;


TO DO:
1. Get a grasp of the p values (for Spearman); if possible, explain why some are so high (that's bad!); regardless, address the concern in a Ch. 6 section.
2. Negative Spearman correlations: Address this phenomenon in a Ch. 6 section; use examples or toy examples;


Meeting with Markus 4/23:
Shorter sentences may tend toward higher TTR, so consider normalizing TTR for sentence length (especially wrt to transitivity); "Standardized TTR; wordsmith tools--corpus ling"

Look at annotation features to see how my ranking separates 1s and 0s.

Why are my max scores sometimes better than BERT?

################################################################################
04/26. Revisiting the above questions...


NTS: TODO: compare NNS STTRs vs Spearman; also NS STTRs vs NNS STTRs;

################################################################################
05/05. I've made a lot of progress since above...
I'm currently reorganizing Ch 6 and tidying it up. I don't plan to add any more content there.

Revisit parameter experiment sections and switch tables/discussions to STTR (not TTR).

Super tired and ineffective today... Tomorrow, or later today:
Do I have a table in Ch 5 that presents the percentage of responses that are positively annotated for each feature? I'd like to see this for Crowd, Familiar, and NNS. Also broken down by Targeting, Transitivity, Primacy...
I think I'd rather move this to Ch 6...
I think this necessitates 5 tables, one per feature; these can each go in the respective Ch 6 section.
Each has:
columns:
	FNS14, CNS14, CNS50, NNS70, using the relevant sample sets.
rows (positive annotation rates):
	intrans, trans, ditrans; targ, untarg; primary, mixed;

I think we could also get some mileage out of another with the same layout, where the values are instead average response length (words/response), and another where the values are STTR.

EOD. Tomorrow, return to the table:
\caption{\label{tab:sttr}Comparing average standardized type-to-token ratio (STTR) for the samples used throughout this chapter as NS models and NNS test sets, in total and by parameter setting.}
I'll need to populate the rest of the table... this will likely require some scripting/calculating.

################################################################################
5/6. Working on getting the STTRs to complete the table (tab:sttr).


################################################################################
5/10. Much progress. Chapter 6 is almost entirely written. I'm finishing it up today.
Here's what's up:
1. Revise transitivity section: 20 minutes. DONE
2. Write Familiarity section: 45 minutes. DONE
3. Write Primacy section: 1 hour. DONE
4. Write Termrep section: 45 minutes. DONE
(?) 5. Write Chapter conclusion. 30 minutes. ...
6. Revise termnorm section: 30 minutes. DONE

################################################################################

5/12.
All the words that appear in special contexts (\feat{}, \param{}) need to include "optional" hyphens, because they will not wrap otherwise---they may extend into the margins if they appear last on the line.
DONE

################################################################################
5/13. Thursday.
Pedal to the metal, eyes on the prize... We're almost there!

To do:
Conclusion
Intro
Lit Review

Edit Chapters 3-5 for style and terminology to be consistent with Ch 6. Pay attention to the use of special typefaces (\feat{}, \param{}, etc.)

I probably need to re-latex the appendices so that they fit the format requirements for the dissertation.

NOTES FOR:

LIT REVIEW:
See Pilot Study: \label{sec:pilot-data} for useful citations to discuss.
Also Pilot Study: \label{sec:rule-method}


CONCLUSION/INTRO:
Re: Pilot study:
Inspired by content analysis, ICALL.
Collected PDT responses, small corpus, annotated for "appropriateness"
Rule-based extraction study
	Found some promising results, but this approach was overly complicated, required too much manual curation of datasets, cannot easily be scaled up or applied to new items.
	The extraction layer of processing introduces errors which can be avoided by other approaches.
	

The gardener is cleaning the street. Portuguese
A man is cleaning the street from leaves. Arabic
The man is sweeping the floor. Chinese
A man is gathering lots of leafs. Korean

################################################################################

From Markus:

Summarize at the end...
Explicitly list some recommendations ~"implications"?
Frame this as "proof of concept", the word "trends" is useful here...
	"Is it helpful to look at targeted vs untargeted? Etc.? Are these parameters helpful? Why bother looking at these parameters? Should we just eliminate 'targeted vs untargeted', etc.?"
Framing:
	What does processing (results) tell us about how data should be collected?
Another question we answer here:
	How does one evaluate systems? What is the right evaluation?
If I didn't have S6.5, I'd just use BERT, right? But 6.5 shows us that my system is useful for individual features.

The multiple ways of evaluation are a contribution in itself.
MD: this chapter shows me why i should 'buy' your annotation...

Move term-norm just before Transitivity.


**MD to be gone June 25-July 5ish.


From meeting with Markus. 5/28.
Insert RQs somewhere... maybe between 1.1 Motivation and 1.2 Overview.




################################################################################

2021/06/04. Meeting with Markus.
Pre-meeting:
# Let's nail down defense date. Thursday 8/5 or Friday 8/6. (Any preference?)
FRIDAY 8/6 it is.
Email committee. ask time of day... maybe just propose 2pm unless objections.
Email Stuart.

"Roadmap"-- frame Overview this way: DONE

ABSTRACT! (Need this to initiate e-doc for defense) DONE


2021/06/11. Meeting with Markus.
In Outlook section:
Contextualize a little more in the big picture... Citations will help.
Currently Outlook reads as how *my* next steps would be;
Add more/reframe slightly about how *other* researchers can extend my work in new ways.
Applicability to other kinds of research... ~"with a little more work, this corpus can be very useful for projects like x,y,z..."~

Spell it out for ICALL researchers... tell them how my work impacts system development, how it is relevant even if they might not see it.


Meurers & Dickinson (2017) on how annotation adds value: http://www.sfs.uni-tuebingen.de/~dm/papers/Meurers.Dickinson-17.html
--Page 9 references King and Dickinson 2013!


################################################################################

30 minute presentation (slides); roughly 60-80 minutes of questions; 15-20 minutes per prof;
start from outside members, work toward main advisor;
Sandra will ask: "if you had to do it again, what would you do differently?"

30-minute presentation: for "anyone in the world" "starting from zero";
30 minutes +/- five-ish; enough slides for this, plus backup slides anticipated;
"what did you do? why is it important? (lesser: what does it lead to?)"
tell the story of your dissertation;
structuring it around research questions is a good idea;

################################################################################
TODO:

Main slides:
SBERT
TTR/STTR


Backup slides:
MAP toy example (how avg precision works)
tf-idf toy example
TTR/STTR toy example
MAP slides for 4 remaining features, a la Core Event slides;
Spearman slides for remaining parameters, a la Transitivity slides;
